<!doctype html>
<html lang="en">

<head>
  <!-- Required meta tags -->
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- Bootstrap CSS -->
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0/dist/css/bootstrap.min.css" rel="stylesheet"
    integrity="sha384-wEmeIV1mKuiNpC+IOBjI7aAzPcEZeedi5yW5f2yOq55WWLwNGmvvx4Um1vskeMj0" crossorigin="anonymous">
  <link rel="stylesheet" href="assets/css/style.css">
  <title>Deciphering Big Data - Individual e-Portfolio</title>
  <style>
    body {
      background-color: #000; /* Black background */
      color: #fff; /* White text */
    }

    .sidebar {
      height: 100%;
      width: 250px;
      position: fixed;
      top: 0;
      left: 0;
      background-color: #262626;
      padding-top: 20px;
    }

    .sidebar a {
      padding: 10px 15px;
      text-decoration: none;
      font-size: 18px;
      color: #fff;
      display: block;
    }

    .sidebar a:hover {
      background-color: #575757;
    }

    .content {
      margin-left: 270px;
      padding: 20px;
    }

    .unit {
      background-color: #fff; /* White background for units */
      color: #000; /* Black text for units */
      margin-bottom: 20px;
      padding: 20px;
      border-radius: 10px;
      box-shadow: 0 0 10px rgba(0, 0, 0, 0.5);
    }

    .btn-custom {
      margin-right: 10px;
    }
  </style>
</head>

<body>
  <div class="sidebar">
    <a href="#unit1"><h1>Collaborative Discussion Forum Summaries</a>
    <a href="#unit2">Outcomes from the Team Exercises</a>
    <a href="#unit3">Unit 3</a>
    <a href="#unit4">Unit 4</a>
    <a href="#unit5">Unit 5</a>
    <a href="#unit6">Unit 6</a>
    <a href="#unit7">Unit 7</a>
    <a href="#unit8">Unit 8</a>
    <a href="#unit9">Unit 9</a>
    <a href="#unit10">Unit 10</a>
    <a href="#unit11">Unit 11</a>
    <a href="#unit12">Unit 12</a>
  </div>

  <div class="content">
    <h1 class="mt-5">Deciphering Big Data - Individual e-Portfolio</h1>
    <div class="unit" id="unit1">
      <h1>Collaborative Discussion Forum Summaries</h1>
    
<p>In our collaborative discussions, we explored the transformative potential of the Internet of Things (IoT) and the stringent data protection regulations under GDPR and the ICO.
The Internet of Things (IoT) has the potential to transform a number of sectors by improving operations, decision-making, and solutions through device connectivity and real-time data analysis (Sundmaeker et al., 2010). In smart cities, for instance, it optimises energy consumption and traffic management; in healthcare, it facilitates early diagnosis and individualised therapy through continuous patient monitoring (Lee & Lee, 2015). However, obstacles including the absence of defined protocols, problems with data integration, and the requirement for sophisticated big data infrastructures stand in the way of IoT's broad adoption. Significant security and privacy problems necessitate strong measures like encryption and frequent software upgrades (Weber, 2010). In order to reduce IoT risks, Panagiotis Mourtas emphasised the significance of robust security measures, regular evaluations, and reasonably priced cloud-based platforms (Fortinet, 2024; FasterCapital, 2024; Medium, 2023).
We also talked about the tough rules enacted by the GDPR and ICO to protect personal data. While the ICO prioritises encryption and ongoing assessments, the GDPR requires suitable safeguards depending on threats to people' rights (Voigt & von dem Bussche, 2017; ICO, 2018). The two frameworks follow a risk-based methodology, with the ICO providing flexibility for compliance with UK laws after Brexit (Bradford, 2020; Edwards, 2018). The consequences of the UK's independent data agreements after Brexit were examined by Robert Draper and Panagiotis Mourtas, together with the ICO's capacity to offer customised compliance guidance (Burgess, 2020; DDCMS, 2024).
</p>



      
      <a href="assets/pdf/bigdata-initial-post-1.pdf" target="_blank" class="btn btn-primary btn-custom">Collaboration Discussion 1 - Initial Post</a>
      <a href="assets/pdf/bigdata-peer-responses-1.pdf" target="_blank" class="btn btn-primary">Collaboration Discussion 1 - Peer Responses</a>
      <a href="assets/pdf/bigdata-summary-post-1.pdf" target="_blank" class="btn btn-primary">Collaboration Discussion 1 - Summary Post</a>
      <a href="assets/pdf/big-data-initial-post-2.pdf" target="_blank" class="btn btn-primary btn-custom">Collaboration Discussion 2 - Initial Post</a>
      <a href="assets/pdf/big-data-peer-responses-2.pdf" target="_blank" class="btn btn-primary btn-custom">Collaboration Discussion 2 - Peer Responses</a>
      <a href="assets/pdf/big-data-summary-post-2.pdf" target="_blank" class="btn btn-primary btn-custom">Collaboration Discussion 2 - Summary Post</a>

    </div>








    
        <div class="unit" id="unit2">


<h2>Outcomes from Team Exercises</h2>

<p>Our team engaged in exercises to critically analyze data wrangling problems and apply appropriate methodologies, tools, and techniques. We developed solutions for processing datasets and solving complex problems in various environments, honing the skills required to be effective team members in a virtual professional environment.</p>

<h2>Team Exercise 1: Database Design for ABC Electronics</h2>

<h3>Summary:</h3>
<p>In the first exercise, we designed a logical database for ABC Electronics to enhance data management and streamline processes. The project involved defining entities, attributes, relationships, data types, and formats, choosing a suitable DBMS, and implementing a data management pipeline.</p>

<h3>Table: Key Aspects of Exercise 1</h3>
<table border="1" cellpadding="10" cellspacing="0">
    <thead>
        <tr>
            <th>Aspect</th>
            <th>Details</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>Logical Design</td>
            <td>Defined entities, attributes, and relationships in a relational database model.</td>
        </tr>
        <tr>
            <td>Database Management System</td>
            <td>Chose MySQL for its scalability, performance, security, and cost-effectiveness.</td>
        </tr>
        <tr>
            <td>Data Management Pipeline</td>
            <td>Captured data from various sources, used ETL processes, and implemented data cleaning techniques.</td>
        </tr>
        <tr>
            <td>Data Cleaning Techniques</td>
            <td>Data validation, managing missing values, eliminating duplicates, standardization, and normalization.</td>
        </tr>
        <tr>
            <td>Tools and Methodologies</td>
            <td>Utilized Python libraries (Pandas, NumPy), SQL queries, Tableau, and ETL tools (Talend, Apache NiFi).</td>
        </tr>
    </tbody>
</table>



<h2>Team Exercise 2: DreamHome Property Management Case Study</h2>
<h3>Summary:</h3>
echo "<p>The second exercise focused on the initial phases of the Database Development Lifecycle (DSDLC) for DreamHome Property Management. We applied fact-finding techniques, established user needs, and documented the lifecycle stages to ensure a clear and comprehensive approach for developing an effective database system.</p>";

echo "<table border='1'>
echo "<tr><th>Aspect</th><th>Details</th></tr>";

$data = [
    [
        'aspect' => 'Database Development Lifecycle (DSDLC)',
        'details' => 'Followed structured steps: planning, system definition, requirements collection, design, application design, DBMS selection, implementation, data conversion, testing, and maintenance.'
    ],
    [
        'aspect' => 'Fact-Finding Techniques',
        'details' => 'Employed techniques like documentation analysis, interviews, observations, research, and questionnaires to gather precise and comprehensive information.'
    ],
    [
        'aspect' => 'Database Requirements',
        'details' => 'Defined data and transaction requirements for Branch and Staff user views, ensuring support for DreamHome\'s operations.'
    ],
    [
        'aspect' => 'Documentation',
        'details' => 'Produced mission statements, system definition documents, and requirements specifications to guide the development process and ensure user needs are met.'
    ]
];

foreach ($data as $row) {
    echo "<tr><td>{$row['aspect']}</td><td>{$row['details']}</td></tr>";
}

echo "</table>";
?>


<h2>Reflection on Team Dynamics and Skills Development</h2>
<p>Initially, I was under a lot of strain because I was the one doing the most of the draughting during the project due to the team members' minimal involvement. Early and regular participation is essential, as demonstrated by Robert and Funmilayo's superior end product despite their tardy contributions. Proactive communication and fair responsibility sharing are crucial, as this example has shown. It focused on the importance of cooperation and leadership in a remote work environment, while also improving my abilities in time management, critical thinking, communication, and problem-solving. In light of this, I've discovered that in order to guarantee equitable contributions, I need promote early participation.
</p>

    
    </li>
    <li>
        <span class="section-title">Application of Data Formats and Types:</span>
        <ul>
            <li>Implementing practical scenarios where different data formats and types are used.</li>
            <li>Enabling APIs to parse data from one storage or process to another efficiently.</li>
        </ul>
    </li>
    <li>
        <span class="section-title">Python Routines and Applications:</span>
        <ul>
            <li>Developing Python routines to handle and manipulate different data formats.</li>
            <li>Using Python libraries (NumPy, Pandas, Matplotlib) for advanced data operations and visualization.</li>
        </ul>
    </li>
</ol>

           <h2>Reading and Study Hours</h2>

<h3>Primary Reading:</h3>
<ul>
    <li>
        <span class="book-title">Sarkar, T. & Roychowdhury, S. (2019) Data Wrangling with Python. 1st ed. Packt.</span>
        <ul>
            <li>Chapters 2 and 3: Covered advanced data structures, file handling, and introduction to NumPy, Pandas, and Matplotlib.</li>
            <li class="time">Time Spent: Approximately 6 hours.</li>
        </ul>
    </li>
    <li>
        <span class="book-title">Kazil, J & Jarmul, K. (2016) Data Wrangling with Python. O'Reilly Media Inc.</span>
        <ul>
            <li>Chapters 2, 3, and 4: Discussed basic data types, importing data from CSV, JSON, and XML files, and working with Excel files.</li>
            <li class="time">Time Spent: Approximately 5 hours.</li>
        </ul>
    </li>
</ul>

<h3>Additional Reading:</h3>
<ul>
    <li>
        <span class="book-title">Supplementary articles and documentation on Python libraries and data handling techniques.</span>
        <ul>
            <li class="time">Time Spent: Approximately 2 hours.</li>
        </ul>
    </li>
</ul>


           <h2>Skills Gained from Unit 2</h2>

<p>Unit 2 has enhanced my skills in various areas crucial for data management and analysis. Effective <strong>time management</strong> helped complete tasks efficiently, improving <strong>commercial awareness</strong> of data formats in industry applications. <strong>Critical thinking and analysis</strong> were honed by evaluating data formats and best practices. My <strong>communication and literacy skills</strong> improved through clear articulation of technical concepts.</p>

<p>I advanced my <strong>IT and digital skills</strong> by using Python for data handling and visualization, enhancing <strong>numeracy</strong> through exercises with NumPy and Pandas. <strong>Research skills</strong> were deepened by literature reviews, while collaboration fostered <strong>interpersonal skills</strong> and teamwork. Practical challenges were addressed through <strong>problem-solving</strong> and critical reflection. <strong>Ethical awareness</strong> focused on data security and privacy. Participation in discussions demonstrated <strong>teamwork and leadership abilities</strong>, and ongoing reflection improved my understanding and application of data formats.</p>

           
      <h1>Collaborative Discussion 1 (WEEK 2) - The Data Collection Process</h1>
      <h2>Discussion Topic</h2>
           
      <p>Critically evaluate the rationale behind the Internet of Things (IOT), in the context of the article by Huxley et al (2020), highlighting the opportunities, limitations, risks and challenges associated with such a large-scale process of data collection.</p>

      <h4>Learning Outcomes</h4>
           
        <ul>
        <li>Identify and manage challenges, security issues and risks, limitations, and opportunities in data wrangling.</li>
        <li>Critically analyse data wrangling problems and determine appropriate methodologies, tools, and techniques (involving preparing, cleaning, exploring, creating, optimising and evaluating big data) to solve them.</li>
        </ul>
      </ul>
      <a href="assets/pdf/bigdata-initial-post-1.pdf" target="_blank" class="btn btn-primary btn-custom">Initial Post</a>
      <a href="assets/pdf/bigdata-peer-responses-1.pdf" target="_blank" class="btn btn-primary">Peer Responses</a>
      </div>
    













           
    <div class="unit" id="unit3">
      
      <h1>Unit 3</h1>
       <h2>Data Collection and Storage</h2>


         <p>In Unit 3, "Data Collection and Storage," we explored various sources of data, methods for data collection, and the challenges associated with these processes. The unit emphasized the importance of data integrity, fact-finding, and the storage requirements for different data formats. We also critically evaluated web services and crowd-sourced data.</p>

<h2>Key Learning Outcomes</h2>

<ol>
    <li>
        <span class="section-title">Data Sources and Integrity:</span>
        <ul>
            <li>Analyze and verify the integrity of data sources.</li>
            <li>Conduct fact-finding and fact-checking for data sources.</li>
        </ul>
    </li>
    <li>
        <span class="section-title">Data Collection Methods:</span>
        <ul>
            <li>Explore various methods of data collection.</li>
            <li>Understand memory and storage requirements for different data formats.</li>
        </ul>
    </li>
    <li>
        <span class="section-title">Web Services and Crowd-Sourced Data:</span>
        <ul>
            <li>Critically evaluate the applicability and reliability of web services and crowd-sourced data.</li>
        </ul>
    </li>
    <li>
        <span class="section-title">Practical Application:</span>
        <ul>
            <li>Apply data collection methods to problem scenarios.</li>
            <li>Understand the role of web scraping in data collection.</li>
        </ul>
    </li>
</ol>

         <li>
    <span class="section-title">Practical Exercises:</span>
    <ul>
        <li>Hands-on experience with web scraping using BeautifulSoup and Requests modules in Python.</li>
        <li>Exercises on subsetting, filtering, grouping data, detecting outliers, handling missing values, and merging datasets.</li>
    </ul>
</li>


         <h2>Reading and Study Hours</h2>

<h3>Primary Reading:</h3>
<ul>
    <li>
        <span class="book-title">Sarkar, T. & Roychowdhury, S. (2019) Data Wrangling with Python. 1st ed. Packt.</span>
        <ul>
            <li>Chapters 3, 4, and 5: Covered deep dive into data wrangling, data sources, and web page parsing using BeautifulSoup.</li>
            <li class="time">Time Spent: Approximately 6 hours.</li>
        </ul>
    </li>
</ul>

         <h2>Skills Gained from Unit 3</h2>

<p>Through the completion of Unit 3, I have developed critical skills for data management and analysis, including efficient time management, insights into industry applications of data collection (commercial awareness), and enhanced critical thinking and analysis of data sources and storage requirements. My communication and literacy skills improved through the effective articulation of technical concepts. I gained advanced IT and digital skills in Python for web scraping and data parsing, conducted thorough research on web scraping techniques, and collaborated with peers to foster teamwork and shared learning. Addressing practical challenges enhanced my problem-solving abilities, while considering ethical implications improved my ethical awareness. Active participation in discussions demonstrated my teamwork and leadership capabilities, and continuous reflection on learning experiences enhanced my understanding and application of data collection methods.</p>

<h2>Collaborative Discussion 1 (WEEK 3) - The Data Collection Process</h2>
      <h2>Summary Post</h2>
<p> In this unit, we also completed a collaborative discussion where we integrated feedback from peers and content from Units 1, 2, and 3. This discussion focused on critically evaluating data collection methods, their challenges, and applications in various scenarios. The summary of this discussion, which includes insights and contributions from all participants, is available in the following link:</p> 

      <a href="assets/pdf/bigdata-summary-post-1.pdf" target="_blank" class="btn btn-primary">Summary Post</a>

         
  <h2>Web Scraping Project</h2>
      <p> In Unit 3, we undertook a web scraping task to gather job listings for "Data Scientist" using Python. This involved using the BeautifulSoup and Requests libraries to extract relevant job data from a website and save it into a JSON file. The objective was to gain practical experience with data collection methods and understand the memory and storage requirements for different data formats. The Python code and detailed report for this web scraping project are provided below.</p>  
        
   
      <h3>Learning OUTCOMES</h3>

      <ul>
      <li>Identify and manage challenges, security issues and risks, limitations, and opportunities in data wrangling.</li>
      <li>Critically analyse data wrangling problems and determine appropriate methodologies, tools, and techniques (involving preparing, cleaning, exploring, creating, optimising and evaluating big data) to solve them.</li>
      <li>Systematically develop and implement the skills required to be effective member of a development team in a virtual professional environment, adopting real life perspectives on team roles and organisation.</li>
      <ul>

      </ul>  
      <a href="assets/code/bigdata-webscraping-1.jpg" target="_blank" class="btn btn-primary btn-custom">Python code for Web Scraping </a>
      <a href="assets/pdf/web-scraping-report.pdf" target="_blank" class="btn btn-primary">Report: Web Scraping</a>
      
    
    </div>

























        
    <div class="unit" id="unit4">
      <h1>Unit 4</h1>
      <h2> Data Cleaning and Transformation</h2>

     <p>In Unit 4, "Data Cleaning and Transformation," we explored various concepts, techniques, and methods essential for cleaning and transforming data. This unit emphasized the importance of understanding the data management pipeline, evaluating factors affecting data cleaning, and the requirements critical for data design and process automation.</p>

<h2>Key Learning Outcomes</h2>

<ol>
    <li>
        <span class="section-title">Data Cleaning and Transformation:</span>
        <ul>
            <li>Examine and apply data cleaning and transformation concepts.</li>
            <li>Understand and implement the data management pipeline.</li>
            <li>Evaluate factors affecting data cleaning.</li>
        </ul>
    </li>
    <li>
        <span class="section-title">Design and Process Automation:</span>
        <ul>
            <li>Understand the requirements for design automation.</li>
            <li>Compile and document Python scripts for data cleaning and transformation.</li>
        </ul>
    </li>
    <li>
        <span class="section-title">Practical Application:</span>
        <ul>
            <li>Compile data sets and convert data into different formats.</li>
            <li>Perform practical data cleaning tasks using Python.</li>
        </ul>
    </li>
</ol>

      <h2>Reading and Study Hours</h2>

<h3>Primary Reading:</h3>
<ul>
    <li>
        <span class="book-title">Sarkar, T. & Roychowdhury, S. (2019) Data Wrangling with Python. 1st ed. Packt.</span>
        <ul>
            <li>Chapters 6 and 7: Covered data wrangling secrets, handling outliers, missing data, and advanced web scraping.</li>
            <li class="time">Time Spent: Approximately 4 hours.</li>
        </ul>
    </li>
    <li>
        <span class="book-title">Kazil, J & Jarmul, K. (2016) Data Wrangling with Python. O'Reilly Media Inc.</span>
        <ul>
            <li>Chapters 6, 7, and 14: Discussed data acquisition, cleanup, and automation.</li>
            <li class="time">Time Spent: Approximately 5 hours.</li>
        </ul>
    </li>
    <li>
        <span class="book-title">Huxley et al. (2020) Data Cleaning. Sage Foundation.</span>
        <ul>
            <li>Provided insights into best practices for data cleaning.</li>
            <li class="time">Time Spent: Approximately 1 hours.</li>
        </ul>
    </li>
</ul>


      
          
        <h2>Data Management Pipeline Test.</h2>
      
<p>The Data Management Pipeline Test was completed on Sunday, 26 May 2024, in 28 minutes and 5 seconds, with a perfect score of 2.00 out of 2.00, resulting in a grade of 10.00 out of 10.00 (100%).</p>

<p>The test focused on matching Python concepts/libraries with their purposes and best practices for Python development. This exercise reinforced key concepts and best practices, emphasizing efficiency, clarity, and proper coding standards.</p>

<p>For detailed questions and answers, please refer to the following link:</p>



      <a href="assets/pdf/bigdata-pipeline-test-1.pdf" target="_blank" class="btn btn-primary btn-custom">Test</a>


      
        <h2>UNIT EXERCISES</h2>

        <h3>Exercise 1</h3>
        <p>Follow the instructions on page 150-151 of the Data Wrangling with Python textbook to manually produce data files mn.csv and mn_headers.csv. 
        Perhaps the simplest approach to saving the cleaned data is to export it to a simple file, and the listed code saved to a new CSV file.</p>
        <a href="assets/code/big-data-data-cleaning-1.jpg" target="_blank" class="btn btn-primary btn-custom">Python code for Ex. 1</a>
        <a href="assets/pdf/Report%20on%20Data%20Cleaning%20and%20Processing%20Task.pdf" target="_blank" class="btn btn-primary btn-custom">Report: Data Cleaning</a>
        
        <h3>Exercise 2</h3>
        <p>The textbook continues to apply the rules for writing good code and produces a complete script with documentation. 
          Study the complete process listed on pages 199-209 of the Data Wrangling with Python textbook.</p>
        <a href="assets/code/big-data-data-cleaning-2.jpg" target="_blank" class="btn btn-primary btn-custom">Python code for Ex. 2</a>
        <a href="assets/pdf/Report%20on%20Data%20Cleaning%20and%20Processing%20Task%20-%20Documenting%20the%20Example.pdf" target="_blank" class="btn btn-primary btn-custom">Report: Documenting the Example</a>

    </div>














    

    
    <div class="unit" id="unit5">
      <h1>Unit 5</h1>

       <h2> Data Cleaning and Automating Data Collections</h2>

<p>In Unit 5, "Data Cleaning and Automating Data Collections," we explored practical aspects of data cleaning using Python, focused on automating data collections, and discussed the considerations for cleaning data for organizational use.</p>

<h2>Key Learning Outcomes</h2>

<ol>
    <li>
        <span class="section-title">Data Cleaning with Python:</span>
        <ul>
            <li>Examine and apply data cleaning techniques using Python examples.</li>
            <li>Create and convert data files to CSV format.</li>
            <li>Scrape appropriate web pages for data collection.</li>
        </ul>
    </li>
    <li>
        <span class="section-title">Automation of Data Cleaning:</span>
        <ul>
            <li>Evaluate how Python scripts can automate the data cleaning process.</li>
            <li>Understand how automation integrates machine learning strategies.</li>
        </ul>
    </li>
    <li>
        <span class="section-title">Database Representation and Architecture:</span>
        <ul>
            <li>Understand the use of database representation and architecture.</li>
            <li>Explain data models as conceptual representations of data relationships.</li>
        </ul>
    </li>
</ol>


      <h2>Artifacts and Their Relation to Learning Outcomes</h2>

<h3>Lecturecast and Reading Materials:</h3>
<ul>
    <li>Provided insights into data cleaning techniques and the importance of automation.</li>
    <li>Emphasized critical outcomes for data design and the use of machine learning strategies.</li>
</ul>

<h3>Practical Exercises:</h3>
<ul>
    <li>Hands-on experience with data cleaning and web scraping using Python.</li>
    <li>Exercises on automating data collection and cleaning processes.</li>
</ul>

<h2>Reading and Study Hours</h2>

<h3>Primary Reading:</h3>
<ul>
    <li>
        <span class="book-title">IBM Intellas (2016) Qradar and KAIF Integration Report</span>
    </li>
    <li>
        <span class="book-title">Datanami (2019) Data Pipeline Automation: The Next Step Forward in DataOps</span>
        <ul>
            <li class="time">Time Spent: Approximately 4 hours for both articles.</li>
        </ul>
    </li>
</ul>


      <h2>Summary of Articles</h2>

<h3>Data Pipeline Automation: The Next Step Forward in DataOps</h3>
<p>From the Datanami article, I learned about the importance of automating data pipelines to streamline data engineering tasks. The article highlighted how tools like Kubeflow and Airflow help automate machine learning workflows, while new DataOps tools manage data pipelines. This automation increases agility, reduces errors, and enhances the productivity of data engineers, allowing them to handle large-scale data more effectively.</p>

<h3>IBM Intellas: Qradar and KAIF Integration Report</h3>
<p>The IBM Intellas report described the integration of KAIF and QRADAR for advanced data analysis and cyber security. The report detailed how KAIF's machine learning strategies enhance QRADAR's capabilities, providing real-time threat analysis and intelligence. The integration ensures a robust system for digital security and forensic analysis, essential for handling complex data sets and cyber threats.</p>





      
    
    
    </div>
    <div class="unit" id="unit6">
      <h1>Unit 6</h1>
       <h2> Database Design and Normalisation</h2>

<p>In Unit 6, "Database Design and Normalisation," we focused on the principles of database design and the importance of normalization in relational databases. This unit emphasized the creation and management of databases, ensuring data integrity and efficient storage.</p>

<h2>Key Learning Outcomes</h2>

<ol>
    <li>
        <span class="section-title">Database Construction and Terminology:</span>
        <ul>
            <li>Discuss the construction and terminology associated with creating a database.</li>
            <li>Identify technical terminology used in the construction of a relational database.</li>
        </ul>
    </li>
    <li>
        <span class="section-title">Logical and Physical Database Design:</span>
        <ul>
            <li>Be familiar with the process involved in creating a logical database design.</li>
            <li>Evaluate the requirements for creating a physical architecture database design.</li>
            <li>Analyze and evaluate the requirements and limitations for creating a physical design.</li>
        </ul>
    </li>
    <li>
        <span class="section-title">Normalization:</span>
        <ul>
            <li>Understand the fundamentals of normalization.</li>
            <li>Look at the reasoning behind the use of different normal forms.</li>
        </ul>
    </li>
</ol>

<h2>Artifacts and Their Relation to Learning Outcomes</h2>

<h3>Lecturecast and Reading Materials:</h3>
<ul>
    <li>Provided insights into how data cleaning methods help with the storage of usable datasets.</li>
    <li>Explained the creation of databases and the use of key fields to link data.</li>
    <li>Analyzed anomalies that affect database integrity and the importance of normalization.</li>
</ul>

<h3>Practical Exercises:</h3>
<ul>
    <li>Hands-on activities involving real-life data wrangling tasks, such as fixing UN data, cleaning GDP data, and merging datasets.</li>
    <li>Exercises on connecting cleaned data to a database and extending data wrangling techniques.</li>
</ul>

<h2>Reading and Study Hours</h2>

<h3>Primary Reading:</h3>
<ul>
    <li>
        <span class="book-title">Sarkar, T. & Roychowdhury, S. (2019) Data Wrangling with Python. 1st ed. Packt.</span>
        <ul>
            <li>Chapter 9: Covered real-life applications of data wrangling and database connections.</li>
            <li class="time">Time Spent: Approximately 3 hours.</li>
        </ul>
    </li>
</ul>


      
          
        <h2>Development Team Project: Project Report</h2>

<p>In Unit 6, our team undertook a project to design and build a logical database for a chosen application. We acted as software consultants and developers, focusing on the following tasks:</p>

<ul>
    <li><strong>Logical Design:</strong> Identified data items/entities, their attributes, relationships, and associations.</li>
    <li><strong>Database Model Proposal:</strong> Proposed a database management system considering the client's requirements for storage, user access, and data manipulation.</li>
    <li><strong>Data Management Pipeline Evaluation:</strong> Discussed the data capture process, applied data cleaning techniques, and documented the implementation stages.</li>
</ul>

    

<p>The detailed team report is available in the following link:</p>

      

      <a href="assets/pdf/team-project-report.pdf" target="_blank" class="btn btn-primary btn-custom">Project Report</a>
   


<h2>Tutor Feedback</h2>

<ul>
    <li><strong>Knowledge and Understanding:</strong> The report shows very good knowledge and understanding of module topics, incorporated within a practical application as required.</li>
    <li><strong>Application Description:</strong> The application is a hybrid inventory system, which is described thoroughly.</li>
    <li><strong>Content and Structure:</strong> The content and structure of the report are good, the word count is within limit, and the English language is sound.</li>
    <li><strong>References:</strong> There are seven references in total listed and cited in the Harvard style.</li>
    <li><strong>Figures and Diagrams:</strong> The figure in the appendix is core material and should have been included within the main body of the report. Including a diagram showing tables' relationships and associations would have given more clarity.</li>
    <li><strong>Critical Discussions:</strong> The critical discussions are limited and need to focus more on the application. Discussing the data management pipeline should better relate to the application data rather than merely stating theoretical stages and steps.</li>
    <li><strong>Critical Evaluation:</strong> The critical evaluation section is limited to data wrangling and lists theoretical concepts without relating them to the practical application.</li>
    <li><strong>References for Critical Discussions:</strong> Having more references relevant to these topics would have assisted with better critical discussions relating theory to practice.</li>
    <li><strong>Theory and Practical Implementation:</strong> Overall, the report is theory-focused and requires better attention to linking theory to practical implementation relevant to the case at hand.</li>
</ul>



<h2>Feedback on Team Dynamics</h2>

<p>The team performed well, especially in the final stages of the project. Initial engagement was low due to other obligations, but Robert and Funmilayo significantly contributed in the last week, improving overall performance.</p>

<h3>Valuable Behaviors:</h3>
<ul>
    <li>Robert's insightful commentary on stock orders, financial transactions, and firm processes made the report more polished and effective.</li>
    <li>Funmilayo's attention to detail in organizing the introduction and critical assessment section improved the report's clarity and thoroughness.</li>
</ul>

<h3>Detrimental Behaviors:</h3>
<ul>
    <li>I initially felt pressured to oversee the project because Robert and Funmilayo were delayed in their contributions. Their efforts in the last week helped alleviate this issue. Early and consistent engagement is crucial for team cohesiveness.</li>
</ul>

<p>From this project, I learned the importance of early and frequent communication to ensure agreement and fair assignment of responsibilities. Including diverse viewpoints improves the project's outcome. In future projects, I will encourage proactive participation from all team members from the beginning to avoid last-minute rushes and ensure equal effort.</p>




 </div>










    





    
    <div class="unit" id="unit7">
      <h1>Unit 7</h1>
 <h2> Constructing Normalised Tables and Database Build</h2>

<p>In Unit 7, "Constructing Normalised Tables and Database Build," we focused on breaking down an un-normalised table into 1NF, 2NF, and 3NF, and using this data to create a relational database. This unit emphasized the importance of normalization in database design and provided practical experience in constructing and testing a relational database.</p>

<h2>Key Learning Outcomes</h2>

<ol>
    <li>
        <span class="section-title">Understanding Normalisation:</span>
        <ul>
            <li>Transform a flat file database model into a normalized model.</li>
            <li>Understand and apply the concepts of 1NF, 2NF, and 3NF.</li>
        </ul>
    </li>
    <li>
        <span class="section-title">Building and Testing a Relational Database:</span>
        <ul>
            <li>Construct a relational database model based on normalized data.</li>
            <li>Test the database for errors or anomalies to ensure referential integrity.</li>
        </ul>
    </li>
    <li>
        <span class="section-title">Identifying Functional Dependencies:</span>
        <ul>
            <li>Identify functional dependencies of data items in the dataset.</li>
            <li>Evaluate constraints and limitations associated with the dataset.</li>
        </ul>
    </li>
    <li>
        <span class="section-title">Specifying Data Attributes:</span>
        <ul>
            <li>Specify data attributes relevant and critical to using a given dataset.</li>
        </ul>
    </li>
</ol>

<h2>Artifacts and Their Relation to Learning Outcomes</h2>

<h3>Lecturecast and Reading Materials:</h3>
<ul>
    <li>Provided insights into data attributes, associations, operations, and relationships.</li>
    <li>Explained the process of normalizing data and the reasoning behind using different normal forms.</li>
</ul>

<h3>Practical Exercises:</h3>
<ul>
    <li>Hands-on experience with normalizing a dataset to 3NF and constructing a relational database model.</li>
</ul>

      <h2>Reading and Study Hours</h2>

<h3>Primary Reading:</h3>
<ul>
    <li>Letkowski, J. (2015) Doing Database Design with MySQL.</li>
    <li>IBM Intellas. (2010) What is a Database Management System?</li>
    <li>Taipalus, T. (2024) Database management system performance comparisons: A systematic literature review.</li>
    <li>Chen, W. (2023) Database Design and Implementation.
        <ul>
            <li class="time">Time Spent: Approximately 4 hours for all articles.</li>
        </ul>
    </li>
</ul>



      <h2>Tasks</h2>
<h2>Normalisation Task</h2>
<p>For the normalization task, we took an un-normalised table and transformed it into 1NF, 2NF, and 3NF, demonstrating each step. The Python code and detailed report explaining the process are available in the following link:</p>


           <a href="assets/excel/DBD_PCOM7E_Table.xlsx" target="_blank" class="btn btn-primary btn-custom">Table</a>
           <a href="assets/code/Normalisation-task-python.jpg" target="_blank" class="btn btn-primary btn-custom">Python code</a>
           <a href="assets/pdf/data-normalization.pdf" target="_blank" class="btn btn-primary btn-custom">Report: Data Normalization</a>
      <p>     </p>
      
<h2>Data Build Task</h2>
<p>Following the normalization task, we built a relational database system with linked tables, demonstrating knowledge of primary and foreign keys. We tested the database to ensure referential integrity. The Python code and report explaining this task are available in the following link:</p>

  <a href="assets/code/data-build-task-python.jpg" target="_blank" class="btn btn-primary btn-custom">Python code</a>
           <a href="assets/pdf/Report-on Data-Build-Task.pdf" target="_blank" class="btn btn-primary btn-custom">Report: Data Build Task</a>

           <p>     </p>
      
<h2>SQL and Normalisation Activities</h2>
<p>During this week's seminar, we reviewed the SQL website and discussed SQL features to incorporate into our database builds. We also discussed the database design and normalization tasks completed this week.</p>
<p>For the SQL quiz, please refer to the following link:</p>

  <a href="assets/pdf/sql-quiz.pdf" target="_blank" class="btn btn-primary btn-custom">SQL QUIZ & Results</a>

    
          
    </div>

























    
    <div class="unit" id="unit8">
      <h1>Unit 8</h1>
      <h2> Compliance and Regulatory Framework for Managing Data</h2>

<p>In Unit 8, "Compliance and Regulatory Framework for Managing Data," we explored the compliance frameworks regarding data management, focusing on the rights of individuals and the obligations of organizations to stakeholders.</p>

<h2>Key Learning Outcomes</h2>

<ol>
    <li>
        <span class="section-title">Compliance Obligations:</span>
        <ul>
            <li>Analyse compliance obligations for data stakeholders.</li>
            <li>Understand and apply applicable standards.</li>
            <li>Examine existing regulatory frameworks and evaluate activities that require regulation.</li>
        </ul>
    </li>
    <li>
        <span class="section-title">Individual Rights:</span>
        <ul>
            <li>Examine rights accorded to individuals with respect to data usage.</li>
            <li>Exercise rights with respect to data held about individuals, organizations, and stakeholders.</li>
        </ul>
    </li>
    <li>
        <span class="section-title">Regulatory Requirements:</span>
        <ul>
            <li>Understand regulatory requirements and obligations that come with data storage.</li>
            <li>Introduce standards applicable to various organizations.</li>
            <li>Evaluate best practices related to managing a database system.</li>
        </ul>
    </li>
</ol>

<h2>Artifacts and Their Relation to Learning Outcomes</h2>

<h3>Lecturecast and Reading Materials:</h3>
<ul>
    <li>Provided insights into compliance obligations and standards ensuring compliance.</li>
    <li>Explained how businesses must understand the implications arising from the use and storage of big data and the requirement for compliance.</li>
</ul>

<h3>Practical Exercises:</h3>
<ul>
    <li>Hands-on activities to apply compliant frameworks in different scenarios and understand regulatory requirements.</li>
</ul>

<h2>Reading and Study Hours</h2>

<h3>Primary Reading:</h3>
<ul>
    <li>Williams, G. (2017) The Cybercitizen and Homeland Security.</li>
    <li>Wired. (2020) What is GDPR? The summary guide to GDPR compliance in the UK.
        <ul>
            <li class="time">Time Spent: Approximately 3 hours for both articles.</li>
        </ul>
    </li>
</ul>





      
          
        <h2>Collaborative Discussion 2 (WEEK 1)- Comparing Compliance Laws</h2>
      <h3>Discussion Topic</h3>
      <p>In this discussion, we compared the GDPR's rules, particularly regarding the securing of personal data, with similar compliance laws in other countries or with the ICO in the UK. Only two students joined this discussion. More detailed information is available in the following link:</p>
         
      <a href="assets/pdf/big-data-initial-post-2.pdf" target="_blank" class="btn btn-primary btn-custom">Initial Post</a>
      <a href="assets/pdf/big-data-peer-responses-2.pdf" target="_blank" class="btn btn-primary btn-custom">Peer Responses</a>


 <h2>DreamHome Property Management Case Study</h2>
      <p>The seminar covered the DreamHome Property Management case study to illustrate how to establish a database project. We explored fact-finding techniques and documentation produced in the early stages of database development, including database planning, system definition, and requirements collection and analysis.</p>

<h3>Key Points from the Seminar:</h3>
<ul>
    <li><strong>Business Information System Concepts:</strong> Importance of database design and how it can be decomposed into conceptual, logical, and physical phases.</li>
    <li><strong>Fact-Finding Techniques:</strong> Commonly used techniques, their advantages, and disadvantages.</li>
    <li><strong>DreamHome Database Example:</strong> Overview of the current system, planning phase contributions, system definition, and requirements capturing.</li>
</ul>

<p>More detailed information about the DreamHome case is provided in the following report:</p>


      <a href="assets/pdf/big-data-dreamHome.pdf" target="_blank" class="btn btn-primary btn-custom">DreamHome Case</a>

          
    </div>





















    
        
          
    <div class="unit" id="unit9">
      <h1>Unit 9</h1>
      <h2> Database Management Systems (DBMS) and Models</h2>

<p>In Unit 9, "Database Management Systems (DBMS) and Models," we explored the foundational concepts and theories underlying various DBMS types, including flat files, relational databases, non-relational databases, data warehouses, clouds, Hadoop, and data lakes.</p>

<h2>Key Learning Outcomes</h2>

<ol>
    <li>
        <span class="section-title">DBMS Concepts and Theories:</span>
        <ul>
            <li>Evaluate design concepts and theories underpinning databases.</li>
            <li>Understand the principles of database design and development.</li>
        </ul>
    </li>
    <li>
        <span class="section-title">DBMS Strengths, Limitations, and Applications:</span>
        <ul>
            <li>Analyze the strengths and limitations of different DBMS.</li>
            <li>Examine the relevance of database designs to various programming paradigms.</li>
        </ul>
    </li>
    <li>
        <span class="section-title">Practical Application:</span>
        <ul>
            <li>Develop the ability to implement a database management system.</li>
            <li>Apply these concepts in designing applications that require large datasets.</li>
            <li>Understand database security issues.</li>
        </ul>
    </li>
</ol>

<h2>Artifacts and Their Relation to Learning Outcomes</h2>

<h3>Lecturecast and Reading Materials:</h3>
<ul>
    <li>Provided insights into the concepts and principles of database design.</li>
    <li>Discussed the implementation of DBMS and the importance of database security.</li>
</ul>

<h3>Practical Exercises:</h3>
<ul>
    <li>Hands-on experience with storing data in a relational database using SQLite.</li>
    <li>Exercises on setting up a local database with Python and saving cleaned datasets into the SQLite database.</li>
</ul>

<h2>Reading and Study Hours</h2>

<h3>Primary Reading:</h3>
<ul>
    <li>Zoiner, T. (2024) Big Data Architectures.
        <ul>
            <li class="time">Time Spent: Approximately 1 hours.</li>
        </ul>
    </li>
</ul>



      
          
        <h3>Collaborative Discussion 2 (WEEK 2) - Comparing Compliance Laws</h3>
      <h4>Discussion Topic</h4>
      <p>Compare the rules of the GDPR - in particular, with relation to the securing of personal data rule, with either similar compliance laws within your country of residence, or with the ICO in the UK.

The ICO refers to this rule as 'Security' and you should discuss your findings in relation to the standards set out and the exemptions that exist:</p>
          <ul>
          <li>'The securing personal data principle of the GDPR: Personal data shall be processed in a manner that ensures appropriate security of the personal data...' (ICO.org.uk).</li>
          <ul>
      <h4>Learning Outcomes</h4>
      
        <ul>
        <li>Identify and manage challenges, security issues and risks, limitations, and opportunities in data wrangling.</li>
        <ul>
          
      </ul>

      <a href="assets/pdf/big-data-initial-post-2.pdf" target="_blank" class="btn btn-primary btn-custom">Initial Post</a>
      <a href="assets/pdf/big-data-peer-responses-2.pdf" target="_blank" class="btn btn-primary btn-custom">Peer Responses</a>


           <h3>Building a DBMS</h3>
     <p>The seminar focused on building a DBMS using Python and SQLite. Key activities included:</p>

<ul>
    <li><strong>Installing SQLite and Setting Up a Relational Database with Python:</strong> Following instructions from the Kazil textbook.</li>
    <li><strong>Saving the Cleaned UNICEF Dataset into the SQLite Database:</strong> Practical application of storing and managing data within SQLite.</li>
</ul>

<h3>Key Points from the Seminar:</h3>
<ul>
    <li><strong>Practical Application:</strong> Emphasis on using Python for data analysis and database management.</li>
    <li><strong>Team Project Discussion:</strong> Opportunity to discuss progress on the team project and integrate feedback.</li>
</ul>

<p>More detailed information and the SQL code are provided in the following link:</p>

      <a href="assets/code/dreamhome.sql" target="_blank" class="btn btn-primary btn-custom">DreamHome SQL</a>
    </div>






























          
    <div class="unit" id="unit10">
      <h1>Unit 10</h1>

      
    <h2> More on APIs (Application Programming Interfaces) for Data Parsing</h2>

<p>In Unit 10, "More on APIs (Application Programming Interfaces) for Data Parsing," we analyzed and evaluated APIs and their role in data parsing and inter-process communication. We also examined the security requirements for robust API functionality and discussed the challenges and issues associated with API implementation.</p>

<h2>Key Learning Outcomes</h2>

<ol>
    <li>
        <span class="section-title">Evaluation of APIs:</span>
        <ul>
            <li>Understand and evaluate APIs for data parsing and inter-process communication.</li>
            <li>Examine different API protocols, types, and formats.</li>
        </ul>
    </li>
    <li>
        <span class="section-title">Security Requirements:</span>
        <ul>
            <li>Specify security requirements for ensuring the resilience and robustness of APIs.</li>
            <li>Understand API authentication methods, including Basic vs. key and OAuth.</li>
        </ul>
    </li>
    <li>
        <span class="section-title">Challenges and Implementation:</span>
        <ul>
            <li>Examine the challenges and issues associated with API implementation.</li>
            <li>Configure APIs for various platforms requiring data parsing and connectivity.</li>
        </ul>
    </li>
</ol>

<h2>Artifacts and Their Relation to Learning Outcomes</h2>

<h3>Lecturecast and Reading Materials:</h3>
<ul>
    <li>Provided insights into the concepts and principles of API design and implementation.</li>
    <li>Discussed the importance of security requirements and how to ensure API robustness.</li>
</ul>

<h3>Practical Exercises:</h3>
<ul>
    <li>Hands-on experience with configuring APIs and understanding their security requirements.</li>
    <li>Exercises on implementing APIs for data sharing, scraping, and connectivity between Python code and different file formats/management systems (XML, JSON, SQL).</li>
</ul>

<h2>Reading and Study Hours</h2>

<h3>Primary Reading:</h3>
<ul>
    <li>Cooksey, B. (2014) Real-Time Communication - An Introduction to APIs.</li>
    <li>Connolly, T. M. & Begg, C. E. (2015) Database Systems: A Practical Approach to Design, Implementation and Management.
        <ul>
            <li class="time">Time Spent: Approximately 3 hours for both chapters.</li>
        </ul>
    </li>
</ul>



          
        <h2>Collaborative Discussion 2 (WEEK 3) - Comparing Compliance Laws</h2>
      <h4>Discussion Topic</h4>
      <p>In the final week of Collaborative Discussion 2, we provided a summary post based on initial posts, peer feedback, and content from Units 8, 9, and 10. The summary post is available in the following link:</p>
      <a href="assets/pdf/big-data-initial-post-2.pdf" target="_blank" class="btn btn-primary btn-custom">Initial Post</a>
      <a href="assets/pdf/big-data-peer-responses-2.pdf" target="_blank" class="btn btn-primary btn-custom">Peer Responses</a>
      <a href="assets/pdf/big-data-summary-post-2.pdf" target="_blank" class="btn btn-primary btn-custom">Summary Post</a>

           <h3>API Security Requirements</h3>
      <p>I evaluated the security requirements of an API of our choice and wrote a brief security requirements specification to mitigate risks associated with the API for enabling data sharing, scraping, and connectivity. The report detailing this task is available in the following link:</p>
          
  
      <a href="assets/pdf/big-data-api.pdf" target="_blank" class="btn btn-primary btn-custom">Report </a>
    
    
        
          </div>




































          
    <div class="unit" id="unit11">

      
      <h1>Unit 11</h1>

      
    <h2> DBMS Transaction and Recovery</h2>


<p>In Unit 11, "DBMS Transaction and Recovery," we explored how database systems handle failures and ensure data integrity through transaction processing. The unit focused on achieving a consistent database state, where transactions are fully committed or not committed at all, preventing errors within the database.</p>

<h2>Key Learning Outcomes</h2>

<ol>
    <li>
        <span class="section-title">Transaction Processing:</span>
        <ul>
            <li>Understand the importance of transaction consistency in database systems.</li>
            <li>Identify that data transactions are fast-moving and interleaved.</li>
        </ul>
    </li>
    <li>
        <span class="section-title">ACID Transaction State:</span>
        <ul>
            <li>Introduce and understand the ACID properties (Atomicity, Consistency, Isolation, Durability) and their effects on the transaction cycle.</li>
            <li>Examine scheduled transactions, system failures, and checkpoints.</li>
        </ul>
    </li>
    <li>
        <span class="section-title">Role of Transaction Manager:</span>
        <ul>
            <li>Understand the purpose and function of a transaction manager in maintaining data integrity and handling system failures.</li>
        </ul>
    </li>
</ol>

<h2>Artifacts and Their Relation to Learning Outcomes</h2>

<h3>Lecturecast and Reading Materials:</h3>
<ul>
    <li>Provided insights into the concepts and principles of transaction processing and the ACID properties.</li>
    <li>Discussed the importance of transaction consistency and the role of a transaction manager.</li>
</ul>

<h3>Practical Exercises:</h3>
<ul>
    <li>Hands-on experience with transaction processing and recovery techniques.</li>
    <li>Exercises on implementing transaction management and ensuring data integrity.</li>
</ul>

<h2>Reading and Study Hours</h2>

<h3>Primary Reading:</h3>
<ul>
    <li>Sarkar, T. & Roychowdhury, S. (2019) Data Wrangling with Python.</li>
    <li>Cove, V. (2020) Visualize COVID-19 Trends in ArcGIS Insights.
        <ul>
            <li class="time">Time Spent: Approximately 3 hours for both chapters.</li>
        </ul>
    </li>
</ul>

      
        

           <h3>Back Up Procedure</h3>
      <p>In this task, we critically evaluated the Grandfather-Father-Son (GFS) backup procedure, highlighting its effectiveness in making large database backups less resource-heavy compared to other methods. Detailed findings are available in the following link:</p>
          
  

      <a href="assets/pdf/big-data-backup.pdf" target="_blank" class="btn btn-primary btn-custom">Report </a>
    
    
<h2>Individual Project: Executive Summary</h2>

<p>The individual project involved writing an executive summary of the completed design and build of a logical database, based on the team project report from Unit 6. This summary included:</p>

<ul>
    <li><strong>Summary of Work:</strong> Presented in an easy-to-understand, non-technical manner, supported by graphics and charts.</li>
    <li><strong>Database Modelling Concepts:</strong> Review and critical evaluation of the strengths and weaknesses of the data models used.</li>
    <li><strong>DBMS Analysis:</strong> Outcomes of the analysis of the chosen DBMS, highlighting SQL and No-SQL options.</li>
    <li><strong>Compliance and Legal Requirements:</strong> Meeting current standards (including GDPR) and outlining legal and compliance requirements.</li>
    <li><strong>Conclusions and Recommendations:</strong> Organized in order of priority to the business requirements.</li>
</ul>

<p>The detailed report is available in the following link:</p>
 <a href="assets/pdf/big-data-executive-summary.pdf" target="_blank" class="btn btn-primary btn-custom">Executive Summary </a>

      
    
    </div>


















          
    <div class="unit" id="unit12">
      <h1>Unit 12</h1>

      
         
    <h2> Future of Big Data Analytics</h2>



      <p>In Unit 12, "Future of Big Data Analytics," we explored the role of machine learning in driving advances in big data analysis and examined compliance frameworks to ensure data privacy. We also reflected on the applicability of these emerging trends and technologies to organizational contexts.</p>

<h2>Key Learning Outcomes</h2>

<ol>
    <li>
        <span class="section-title">Future Trends in Big Data Analytics:</span>
        <ul>
            <li>Understand emerging and future trends in big data analytics.</li>
            <li>Explore how machine learning strategies are applied to model large and complex data.</li>
        </ul>
    </li>
    <li>
        <span class="section-title">Machine Learning Applications:</span>
        <ul>
            <li>Gain insights into machine learning applications and technologies in database development.</li>
            <li>Reflect on the applicability of these topics to an organization.</li>
        </ul>
    </li>
    <li>
        <span class="section-title">Compliance Frameworks:</span>
        <ul>
            <li>Examine compliance frameworks for ensuring data privacy.</li>
            <li>Understand existing regulations, laws, rules, and standards related to data management.</li>
        </ul>
    </li>
</ol>

<h2>Artifacts and Their Relation to Learning Outcomes</h2>

<h3>Lecturecast and Reading Materials:</h3>
<ul>
    <li>Provided insights into emerging trends and technologies in big data analytics and machine learning.</li>
    <li>Discussed compliance frameworks and the importance of data privacy regulations.</li>
</ul>

<h3>Practical Exercises:</h3>
<ul>
    <li>Hands-on experience with machine learning applications and data privacy compliance.</li>
    <li>Exercises on understanding and applying compliance frameworks to ensure data security.</li>
</ul>

<h2>Reading and Study Hours</h2>

<h3>Primary Reading:</h3>
<ul>
    <li>Ethem, A. (2010) Introduction to Machine Learning.</li>
    <li>Williams, G. (2017) The Cybercitizen and Homeland Security.</li>
    <li>Bhatnagar, A. & Gajjar, D. (2024) Policy Implications of Artificial Intelligence.
        <ul>
            <li class="time">Time Spent: Approximately 4 hours for all chapters and articles.</li>
        </ul>
    </li>
</ul>

        

           <h2>Seminar: Content Challenge</h2>

<p>The seminar provided a discursive overview of the module, helping with final reflection and e-portfolio development. Key discussion points included:</p>

<ul>
    <li><strong>Disadvantages of File-Based Systems:</strong> Addressed by the DBMS approach.</li>
    <li><strong>ACID Properties:</strong> Discussed consistency, reliability, and concurrency control in transactions.</li>
    <li><strong>Database User Privileges:</strong> Commonly granted privileges.</li>
    <li><strong>View Updatability:</strong> Necessary restrictions for maintaining updatable views.</li>
    <li><strong>Materialized Views:</strong> Maintaining views without accessing the underlying base table.</li>
</ul>

<p>The detailed report with questions and answers is available in the following link:</p>


      <a href="assets/pdf/big-data-challenge.pdf" target="_blank" class="btn btn-primary btn-custom">Questions & Answers </a>
    
    
    </div>
  </div>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0/dist/js/bootstrap.bundle.min.js"
    integrity="sha384-p34f1UUtsS3wqzfto5wAAmdvj+osOnFyQFpp4Ua3gs/ZVWx6oOypYoCJhGGScy+8"
    crossorigin="anonymous"></script>
  <script src="https://kit.fontawesome.com/4ad03fe072.js" crossorigin="anonymous"></script>
</body>

</html>
