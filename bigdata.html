<!doctype html>
<html lang="en">

<head>
  <!-- Required meta tags -->
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- Bootstrap CSS -->
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0/dist/css/bootstrap.min.css" rel="stylesheet"
    integrity="sha384-wEmeIV1mKuiNpC+IOBjI7aAzPcEZeedi5yW5f2yOq55WWLwNGmvvx4Um1vskeMj0" crossorigin="anonymous">
  <link rel="stylesheet" href="assets/css/style.css">
  <title>Deciphering Big Data - Individual e-Portfolio</title>
  <style>
    body {
      background-color: #000; /* Black background */
      color: #fff; /* White text */
    }

    .sidebar {
      height: 100%;
      width: 250px;
      position: fixed;
      top: 0;
      left: 0;
      background-color: #262626;
      padding-top: 20px;
    }

    .sidebar a {
      padding: 10px 15px;
      text-decoration: none;
      font-size: 18px;
      color: #fff;
      display: block;
    }

    .sidebar a:hover {
      background-color: #575757;
    }

    .content {
      margin-left: 270px;
      padding: 20px;
    }

    .unit {
      background-color: #fff; /* White background for units */
      color: #000; /* Black text for units */
      margin-bottom: 20px;
      padding: 20px;
      border-radius: 10px;
      box-shadow: 0 0 10px rgba(0, 0, 0, 0.5);
    }

    .btn-custom {
      margin-right: 10px;
    }
  </style>
</head>

<body>
  <div class="sidebar">
    <a href="#unit1"><h1>Collaborative Discussion Forum Summaries</a>
    <a href="#unit2">Outcomes from the Team Exercises</a>
    <a href="#unit3">Formative, Wiki, and e-Portfolio Activities</a>
    <a href="#unit4">Unit 4</a>
    <a href="#unit5">Unit 5</a>
    <a href="#unit6">Unit 6</a>
    <a href="#unit7">Unit 7</a>
    <a href="#unit8">Unit 8</a>
    <a href="#unit9">Unit 9</a>
    <a href="#unit10">Unit 10</a>
    <a href="#unit11">Unit 11</a>
    <a href="#unit12">Unit 12</a>
  </div>

  <div class="content">
    <h1 class="mt-5">Deciphering Big Data - Individual e-Portfolio</h1>
    <div class="unit" id="unit1">
 

      <h2>Collaborative Discussion Forum Summaries - Unit 1-3 & 8-10</h2>

<h3>Summary:</h3>
<p>We examined the revolutionary possibilities of the Internet of Things (IoT) and the strict GDPR and ICO data protection laws in our collaborative discussion sessions. These talks brought to light the ways that IoT is affecting different industries as well as how crucial data protection is.</p>

<p>By improving operations and decision-making through device connectivity and real-time data analysis, the Internet of Things (IoT) has the potential to completely transform industries including smart cities and healthcare (Sundmaeker et al., 2010). However, obstacles including unclear protocols, problems integrating data, and the requirement for sophisticated big data infrastructures impede the adoption of IoT (Weber, 2010). Due to serious security and privacy issues, strong measures like encryption and frequent software upgrades are required (Panagiotis Mourtzas, 2023). While the ICO concentrates on encryption and ongoing inspections to maintain data privacy, the GDPR mandates suitable protections based on threats to people' rights (Voigt & von dem Bussche, 2017). The flexibility of the ICO in complying with regulations and the UK's independent data agreements following Brexit were also deliberated (Bradford, 2020; Edwards, 2018).<p>

  
<h3>Table 1: Key Aspects of Discussions</h3>
<table border="1" cellpadding="10" cellspacing="0">
    <thead>
        <tr>
            <th>Aspect</th>
            <th>Details</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>Internet of Things (IoT)</td>
            <td>IoT improves operations and decision-making through device connectivity and real-time data analysis in sectors like smart cities and healthcare.</td>
        </tr>
        <tr>
            <td>Challenges in IoT</td>
            <td>Issues include lack of defined protocols, data integration problems, and need for advanced big data infrastructures. Security and privacy are major concerns.</td>
        </tr>
        <tr>
            <td>Security Measures</td>
            <td>Emphasized the importance of robust security measures, regular evaluations, and affordable cloud-based platforms to mitigate IoT risks.</td>
        </tr>
        <tr>
            <td>GDPR and ICO Regulations</td>
            <td>GDPR mandates safeguards based on threat levels, while ICO focuses on encryption and ongoing assessments, providing compliance flexibility post-Brexit.</td>
        </tr>
        <tr>
            <td>Impact of Brexit</td>
            <td>Examined UK's independent data agreements and ICO's ability to offer customized compliance guidance post-Brexit.</td>
        </tr>
    </tbody>
</table>


<p></p>
<p></p>
<p></p>
<p></p>

      
      <a href="assets/pdf/bigdata-initial-post-1.pdf" target="_blank" class="btn btn-primary btn-custom">Collaboration Discussion 1 - Initial Post</a>
      <a href="assets/pdf/bigdata-peer-responses-1.pdf" target="_blank" class="btn btn-primary">Collaboration Discussion 1 - Peer Responses</a>
      <a href="assets/pdf/bigdata-summary-post-1.pdf" target="_blank" class="btn btn-primary">Collaboration Discussion 1 - Summary Post</a>
      <a href="assets/pdf/big-data-initial-post-2.pdf" target="_blank" class="btn btn-primary btn-custom">Collaboration Discussion 2 - Initial Post</a>
      <a href="assets/pdf/big-data-peer-responses-2.pdf" target="_blank" class="btn btn-primary btn-custom">Collaboration Discussion 2 - Peer Responses</a>
      <a href="assets/pdf/big-data-summary-post-2.pdf" target="_blank" class="btn btn-primary btn-custom">Collaboration Discussion 2 - Summary Post</a>

    </div>








    
        <div class="unit" id="unit2">


<h2>Outcomes from Team Exercises</h2>

<p>Through exercises, our team developed solutions for processing datasets and solving complex problems in various environments, honing the skills necessary to be productive team members in a virtual professional environment. We critically analysed data wrangling problems and applied appropriate methodologies, tools, and techniques.</p>

<h2>Team Exercise 1: Database Design for ABC Electronics - Unit 6</h2>

<h3>Summary:</h3>
<p>For ABC Electronics, we created a logical database in the first exercise to improve data management and expedite procedures. The project required selecting an appropriate database management system (DBMS), putting in place a data management pipeline, and defining entities, characteristics, relationships, data types, and formats.</p>

<h2>Team Meeting Notes:</h2>
<ul>
    <li><strong>Initial Phase:</strong> Discussion about entity identification, database structure, and project scope. low level of early member involvement.</li>
    <li><strong>Mid-Phase:</strong> Assignment of duties, beginning data integration, and preliminary design work. stressing the need of continuing contact.</li>
    <li><strong>Final Phase:</strong> Members' someÂ contributions, particularly those from Robert and Funmilayo. completion of the report authoring, data cleansing, and database design.</li>
</ul>

          
<h3>Table 2: Key Aspects of Exercise 1</h3>
<table border="1" cellpadding="10" cellspacing="0">
    <thead>
        <tr>
            <th>Aspect</th>
            <th>Details</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>Logical Design</td>
            <td>Defined entities, attributes, and relationships in a relational database model.</td>
        </tr>
        <tr>
            <td>Database Management System</td>
            <td>Chose MySQL for its scalability, performance, security, and cost-effectiveness.  (Mirayala, 2024 and Ma 
& Wang, 2024)</td>
        </tr>
        <tr>
            <td>Data Management Pipeline</td>
            <td>Captured data from various sources, used ETL processes, and implemented data cleaning techniques.</td>
        </tr>
        <tr>
            <td>Data Cleaning Techniques</td>
            <td>Data validation, managing missing values, eliminating duplicates, standardization, and normalization.  (Cuzzocrea et al., 2011)</td>
        </tr>
        <tr>
            <td>Tools and Methodologies</td>
            <td>Utilized Python libraries (Pandas, NumPy), SQL queries, and ETL tools. (Panda & Patra, 2015)</td>
        </tr>
    </tbody>
</table>

<h2>Feedback from Peers and Tutors:</h2>

<h3>Table 3: Feedback Summary</h3>
<table border="1" cellpadding="10" cellspacing="0">
    <thead>
        <tr>
            <th>Feedback Source</th>
            <th>Positive Points</th>
            <th>Areas for Improvement</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>Peer Feedback</td>
            <td>
                <ul>
                    <li>Robert's insightful commentary on stock orders and financial transactions.</li>
                    <li>Funmilayo's attention to detail in organizing the introduction and critical assessment sections.</li>
                    <li>Improved overall performance in the final stages of the project.</li>
                </ul>
            </td>
            <td>
                <ul>
                    <li>Initial low engagement from some members.</li>
                    <li>Need for early and consistent engagement to avoid last-minute rushes.</li>
                    <li>Encourage proactive participation from all team members from the beginning.</li>
                </ul>
            </td>
        </tr>
        <tr>
            <td>Tutor Feedback</td>
            <td>
                <ul>
                    <li>Demonstrated very good knowledge and understanding of module topics.</li>
                    <li>Thoroughly described hybrid inventory system application.</li>
                    <li>Good content and structure, with sound English language.</li>
                    <li>Proper referencing in Harvard style.</li>
                </ul>
            </td>
            <td>
                <ul>
                    <li>Include figures and diagrams within the main body of the report for clarity.</li>
                    <li>Focus more on relating the data management pipeline to the application data.</li>
                    <li>Enhance critical discussions and evaluations by linking theory to practical implementation.</li>
                    <li>Increase the number of relevant references to support critical discussions.</li>
                    <li>Address critical evaluation section to connect theoretical concepts with practical application.</li>
                </ul>
            </td>
        </tr>
    </tbody>
</table>

          <p></p>
          <p></p>
                <a href="assets/pdf/team-project-report.pdf" target="_blank" class="btn btn-primary btn-custom">Project Report</a>
          <p></p>
          <p></p>

<h2>Team Exercise 2: DreamHome Property Management Case Study - Unit 8</h2>

<h3>Summary:</h3>
<p>The first stages of DreamHome Property Management's Database Development Lifecycle (DSDLC) were the subject of the second exercise. We used fact-finding methods, determined user requirements, and recorded the lifecycle phases to guarantee a methodical and thorough approach to creating a successful database system.</p>

<h3>Table 4: Key Aspects of Exercise 2</h3>
<table border="1" cellpadding="10" cellspacing="0">
    <thead>
        <tr>
            <th>Aspect</th>
            <th>Details</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>Database Development Lifecycle (DSDLC)</td>
            <td>Followed structured steps: planning, system definition, requirements collection, design, application design, DBMS selection, implementation, data conversion, testing, and maintenance. (Coronel and Morris, 2019),</td>
        </tr>
        <tr>
            <td>Fact-Finding Techniques</td>
            <td>Employed techniques like documentation analysis, interviews, observations, research, and questionnaires to gather precise and comprehensive informatio. (Hoffer, Ramesh, & Topi, 2016).</td>
        </tr>
        <tr>
            <td>Database Requirements</td>
            <td>Defined data and transaction requirements for Branch and Staff user views, ensuring support for DreamHome's operations.</td>
        </tr>
        <tr>
            <td>Documentation</td>
            <td>Produced mission statements, system definition documents, and requirements specifications to guide the development process and ensure user needs are met. (Rob & Coronel, 2017)</td>
        </tr>
    </tbody>
</table>

          <h2>Feedback from Peers:</h2>

<h3>Table 5: Feedback Summary</h3>
<table border="1" cellpadding="10" cellspacing="0">
    <thead>
        <tr>
            <th>Feedback Source</th>
            <th>Positive Points</th>
            <th>Areas for Improvement</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>Peer Feedback</td>
            <td>
                <ul>
                    <li>The initial plan and scope were well-defined.</li>
                    <li>Effective use of fact-finding techniques despite low engagement.</li>
                </ul>
            </td>
            <td>
                <ul>
                    <li>Team members were not active and did not contribute much.</li>
                    <li>Need for early and consistent engagement from all members.</li>
                    <li>Encourage more collaboration and communication throughout the project.</li>
                </ul>
            </td>
        </tr>
    </tbody>
</table>

<p></p>
<p></p>

                <a href="assets/pdf/big-data-dreamHome.pdf" target="_blank" class="btn btn-primary btn-custom">DreamHome Case</a>
<p></p>
<p></p>












          

<div class="unit" id="unit3">


  <h2>Formative, Wiki, and e-Portfolio Activities</h2>

<h3>Task 1: Web Scraping - Unit 3</h3>

<h4>Objective:</h4>
<p>Writing a Python web scraping script to collect Indeed.com job listings for the term "Data Scientist" and parse the resulting data into an XML file using the requests and beautifulsoup4 libraries was the task's goal.</p>


<h4>Table 6: Key Steps and Outcomes</h4>
<table border="1" cellpadding="10" cellspacing="0">
    <thead>
        <tr>
            <th>Step</th>
            <th>Description</th>
            <th>Outcome</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>Environment Setup</td>
            <td>Installed requests and beautifulsoup4 libraries</td>
            <td>Ready to perform web scraping</td>
        </tr>
        <tr>
            <td>Webpage Identification</td>
            <td>Identified target URL and data elements</td>
            <td>Clear understanding of data to be extracted</td>
        </tr>
        <tr>
            <td>Initial Coding</td>
            <td>Sent HTTP request, parsed HTML, located job elements</td>
            <td>Successfully retrieved HTML content and located required elements</td>
        </tr>
        <tr>
            <td>Error Handling</td>
            <td>Added headers to mimic browser request, resolved HTTP 403 error</td>
            <td>Able to access and scrape the webpage</td>
        </tr>
        <tr>
            <td>Data Parsing and Storing</td>
            <td>Parsed job data, stored in XML format</td>
            <td>Structured and saved job listings in XML</td>
        </tr>
    </tbody>
</table>

<h4>Conclusion:</h4>
<p>The project brought together HTML parsing, data organisation in XML, online scraping, and managing HTTP failures. Methodical troubleshooting helped overcome obstacles and produce a script that effectively retrieves and stores Indeed.com job listings.</p>


    <p></p>
    <p></p>

    <a href="assets/code/bigdata-webscraping-1.jpg" target="_blank" class="btn btn-primary btn-custom">Python code for Web Scraping </a>
      <a href="assets/pdf/web-scraping-report.pdf" target="_blank" class="btn btn-primary">Report: Web Scraping</a>

    <p></p>
    <p></p>



<h3>Task 2: Data Cleaning and Processing - Unit 4</h3>

<h4>Objective:</h4>
<p>Using the human-readable headers from the mn_headers.csv file, the task's objective was to clean and preprocess the raw data in the mn.csv file in order to create a clean dataset for further analysis. This included renaming columns, handling missing values, and removing duplicate rows.<p>

<h4>Table 7: Key Steps and Outcomes</h4>
<table border="1" cellpadding="10" cellspacing="0">
    <thead>
        <tr>
            <th>Step</th>
            <th>Description</th>
            <th>Outcome</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>Data Loading</td>
            <td>Loaded data from mn.csv and mn_headers.csv using pandas</td>
            <td>Data successfully loaded into DataFrames</td>
        </tr>
        <tr>
            <td>Headers Dictionary</td>
            <td>Created a mapping dictionary from mn_headers.csv</td>
            <td>Facilitated renaming columns</td>
        </tr>
        <tr>
            <td>Renaming Columns</td>
            <td>Renamed columns using the headers dictionary</td>
            <td>Improved readability and interpretability</td>
        </tr>
        <tr>
            <td>Removing Duplicates</td>
            <td>Used drop_duplicates method to remove duplicate rows</td>
            <td>Ensured data integrity by eliminating redundant information</td>
        </tr>
        <tr>
            <td>Checking for Missing Values</td>
            <td>Checked for missing values using isnull().sum() method</td>
            <td>Identified columns requiring further cleaning or imputation</td>
        </tr>
        <tr>
            <td>Saving Cleaned Data</td>
            <td>Saved cleaned data to a new CSV file</td>
            <td>Cleaned dataset ready for further analysis</td>
        </tr>
    </tbody>
</table>

<h4>Conclusion:</h4>
<p>After the preprocessing and data cleaning were finished successfully, the dataset was cleaned and prepared for additional analysis. Accurate processing was secured by the methodical approach, and problems were handled with suitable technological fixes. The significance of thorough data cleaning procedures in data analysis is highlighted by this activity, laying the groundwork for dependable and perceptive outcomes.<p>

  <p></p>
  <p></p>
       <a href="assets/code/big-data-data-cleaning-1.jpg" target="_blank" class="btn btn-primary btn-custom">Python code for Data Cleaning</a>
        <a href="assets/pdf/Report%20on%20Data%20Cleaning%20and%20Processing%20Task.pdf" target="_blank" class="btn btn-primary btn-custom">Report: Data Cleaning</a>

  <p></p>
  <p></p>

  <h3>Task 3: Writing Good Code and Documentation - Unit 4</h3>

<h4>Objective:</h4>
<p>This task's goal was to use the best practices for writing code to create a comprehensive script that included documentation for preprocessing and data cleaning on a dataset that was derived from UNICEF survey data.</p>

<table border="1" cellpadding="10" cellspacing="0">
  <h4>Table 7: Steps Taken for Data Cleaning and Documentation</h4>

    <thead>
        <tr>
            <th>Step</th>
            <th>Description</th>
            <th>Code Snippet / Outcome</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>Data Loading</td>
            <td>Loaded raw data from mn.csv and headers from mn_headers.csv into pandas DataFrames.</td>
            <td><code>data = pd.read_csv('mn.csv', low_memory=False)</code><br>
                <code>headers = pd.read_csv('mn_headers.csv')</code></td>
        </tr>
        <tr>
            <td>Headers Dictionary</td>
            <td>Created a dictionary mapping acronyms to their human-readable labels using pandas.</td>
            <td><code>headers_dict = pd.Series(headers.Name.values, index=headers.Acronym).to_dict()</code></td>
        </tr>
        <tr>
            <td>Renaming Columns</td>
            <td>Renamed columns using the headers dictionary to transform cryptic headers into meaningful names.</td>
            <td><code>data.rename(columns=headers_dict, inplace=True)</code></td>
        </tr>
        <tr>
            <td>Removing Duplicates</td>
            <td>Identified and removed duplicate rows using pandas' drop_duplicates method.</td>
            <td><code>data.drop_duplicates(inplace=True)</code></td>
        </tr>
        <tr>
            <td>Checking for Missing Values</td>
            <td>Checked for missing values using the isnull().sum() method, identifying columns requiring further cleaning or imputation.</td>
            <td><code>missing_values = data.isnull().sum()</code></td>
        </tr>
        <tr>
            <td>Saving Cleaned Data</td>
            <td>Saved the cleaned dataset to a SQLite database using the sqlite3 module, creating a suitable table structure and inserting the data.</td>
            <td><code>conn = sqlite3.connect('cleaned_data.db')</code><br>
                <code>data.to_sql('cleaned_data', conn, if_exists='replace', index=False)</code></td>
        </tr>
    </tbody>
</table>

<h4>Conclusion:</h4>
<p>The data cleaning and preprocessing task was successfully completed, resulting in a cleaned dataset ready for further analysis. The systematic approach ensured accurate processing, with challenges effectively managed through appropriate technical solutions. This task highlights the importance of meticulous data cleaning practices in data analysis, laying a strong foundation for reliable and insightful results.</p>

<p></p>
<p></p>
     <a href="assets/code/big-data-data-cleaning-2.jpg" target="_blank" class="btn btn-primary btn-custom">Python code for Task 3</a>
        <a href="assets/pdf/Report%20on%20Data%20Cleaning%20and%20Processing%20Task%20-%20Documenting%20the%20Example.pdf" target="_blank" class="btn btn-primary btn-custom">Report: Documenting the Example</a>
<p></p>
<p></p>

  
<h3>Task 4: Data Normalization - UNIT 7</h3>

<h4>Objective:</h4>
<p>This work aimed to ensure data integrity and remove duplication by normalising  (DiÃ¨ne et al., 2020) a dataset from an unnormalized form to the Third Normal Form (3NF).</p>

<table border="1" cellpadding="10" cellspacing="0">
    <h4>Table 8: Steps and Outcomes of Data Normalization</h4>

    <thead>
        <tr>
            <th>Step</th>
            <th>Description</th>
            <th>Process</th>
            <th>Outcome</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>Unnormalized Data (UNF)</td>
            <td>Initial dataset with anomalies</td>
            <td>Data included student details, exam scores, course names, exam boards, and teacher names</td>
            <td>Identified missing values and duplicate rows</td>
        </tr>
        <tr>
            <td>First Normal Form (1NF)</td>
            <td>Ensure atomic values and unique records</td>
            <td>- Remove empty rows <br> - Forward-fill missing values</td>
            <td>Structured data with atomic values and unique records</td>
        </tr>
        <tr>
            <td>Second Normal Form (2NF)</td>
            <td>Remove partial dependencies</td>
            <td>- Identify primary entities: Students, Courses, Teachers <br> - Create separate tables for each entity <br> - Merge tables to form a composite table</td>
            <td>Removed partial dependencies, ensuring non-key attributes are fully dependent on the primary key</td>
        </tr>
        <tr>
            <td>Third Normal Form (3NF)</td>
            <td>Remove transitive dependencies</td>
            <td>- Identify transitive dependencies <br> - Create Exam Scores Table</td>
            <td>Eliminated transitive dependencies, resulting in well-structured tables with non-key attributes dependent only on the primary key</td>
        </tr>
    </tbody>
</table>

<h4>Critical Analysis:</h4>
<ol>
    <li><strong>Data Cleaning:</strong>
        <ul>
            <li>Essential to eliminate duplicated and incomplete records, forming the foundation for further normalization.</li>
        </ul>
    </li>
    <li><strong>Entity Identification:</strong>
        <ul>
            <li>Crucial for structuring the database around logical entities, a core principle of database normalization.</li>
        </ul>
    </li>
    <li><strong>Dependency Removal:</strong>
        <ul>
            <li>Vital for efficient querying and updates, reducing data anomalies and redundancy.</li>
        </ul>
    </li>
    <li><strong>Practical Considerations:</strong>
        <ul>
            <li>Balance between normalization and query performance to manage complexity in database queries due to joining multiple tables.</li>
        </ul>
    </li>
</ol>

<h4>Conclusion:</h4>
<p>The dataset was effectively normalised, removing duplication and guaranteeing data integrity, resulting in a collection of well-structured tables in 3NF. Notwithstanding difficulties, the methodical approach made clear how crucial it is to remove dependencies, identify entities, and clean up data. This procedure improves the accuracy and efficiency of data retrieval while also increasing the efficiency of data storage.</p>

  
<p></p>
<p></p>

           <a href="assets/excel/DBD_PCOM7E_Table.xlsx" target="_blank" class="btn btn-primary btn-custom">Table</a>
           <a href="assets/code/Normalisation-task-python.jpg" target="_blank" class="btn btn-primary btn-custom">Python code</a>
           <a href="assets/pdf/data-normalization.pdf" target="_blank" class="btn btn-primary btn-custom">Report: Data Normalization</a>


<p></p>
<p></p>


<h3>Task 5: Data Build Task</h3>

<h4>Objective:</h4>
<p>Building a relational database system with linked tables, proving that you understood primary and secondary keys, and guaranteeing referential integrity were the goals of this work.</p>


<table border="1" cellpadding="10" cellspacing="0">
    <caption>Table 9: Steps and Outcomes of Data Build Task</caption>
    <thead>
        <tr>
            <th>Step</th>
            <th>Description</th>
            <th>Process</th>
            <th>Outcome</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>Designing the Database Schema</td>
            <td>Analyzed the provided data to identify necessary tables and relationships.</td>
            <td>Identified primary tables (Students, Courses, ExamBoards, Teachers) and junction tables for many-to-many relationships.</td>
            <td>Created a detailed schema plan with primary and foreign keys for each table.</td>
        </tr>
        <tr>
            <td>Creating the Database</td>
            <td>Wrote a comprehensive SQL script to create the tables with appropriate primary and foreign keys.</td>
            <td>Included the creation of junction tables to manage many-to-many relationships and ensure referential integrity.</td>
            <td>Successfully created database schema with tables and relationships defined.</td>
        </tr>
        <tr>
            <td>Populating the Database</td>
            <td>Used SQL INSERT statements to populate the tables with the provided data.</td>
            <td>Added records for students, courses, exam boards, and teachers, establishing relationships in junction tables.</td>
            <td>Database populated with accurate data reflecting the original unnormalized dataset.</td>
        </tr>
        <tr>
            <td>Testing the Database</td>
            <td>Used SQL SELECT queries to verify relationships and data integrity.</td>
            <td>Joined tables to verify relationships between students, their courses, exam boards, and teachers.</td>
            <td>Ensured data was correctly inserted and referential integrity was maintained.</td>
        </tr>
    </tbody>
</table>


<h4>Critical Analysis:</h4>

<table border="1" cellpadding="10" cellspacing="0">
    <caption>Table 10: Critical Analysis of Data Build Task</caption>
    <thead>
        <tr>
            <th>Aspect</th>
            <th>Analysis</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>Methodology and Tools</td>
            <td>SQLite was suitable due to its simplicity and ease of use for small to medium-sized databases. Breaking down the task into schema design, data insertion, and testing proved effective. (Gaffney et al., 2022)</td>
        </tr>
        <tr>
            <td>Normalization and Integrity</td>
            <td>Fully normalized schema helped avoid data redundancy and ensured data integrity. Foreign key constraints reinforced the relationships between tables.  (Amato, xxxx)</td>
        </tr>
        <tr>
            <td>Efficiency and Error Handling</td>
            <td>The process was efficient, though initial environment issues highlighted the importance of correct tool selection. Adding scripts to drop existing tables before creation handled errors and ensured a clean slate.</td>
        </tr>
        <tr>
            <td>Scalability Considerations</td>
            <td>The current system is adequate for the provided data but might need migration to more robust systems like PostgreSQL or MySQL for larger datasets.</td>
        </tr>
    </tbody>
</table>

  <p></p>
  <p></p>

<a href="assets/code/data-build-task-python.jpg" target="_blank" class="btn btn-primary btn-custom">Python code</a>
           <a href="assets/pdf/Report-on Data-Build-Task.pdf" target="_blank" class="btn btn-primary btn-custom">Report: Data Build Task</a>

  <p></p>
  <p></p>























        
    <div class="unit" id="unit4">
      <h1>Unit 4</h1>
      
          
        <h2>Data Management Pipeline Test.</h2>
      
<p>The Data Management Pipeline Test was completed on Sunday, 26 May 2024, in 28 minutes and 5 seconds, with a perfect score of 2.00 out of 2.00, resulting in a grade of 10.00 out of 10.00 (100%).</p>

<p>The test focused on matching Python concepts/libraries with their purposes and best practices for Python development. This exercise reinforced key concepts and best practices, emphasizing efficiency, clarity, and proper coding standards.</p>

<p>For detailed questions and answers, please refer to the following link:</p>



      <a href="assets/pdf/bigdata-pipeline-test-1.pdf" target="_blank" class="btn btn-primary btn-custom">Test</a>


      
        <h2>UNIT EXERCISES</h2>












    

    
    <div class="unit" id="unit5">
      <h1>Unit 5</h1>

       <h2> Data Cleaning and Automating Data Collections</h2>

<p>In Unit 5, "Data Cleaning and Automating Data Collections," we explored practical aspects of data cleaning using Python, focused on automating data collections, and discussed the considerations for cleaning data for organizational use.</p>

<h2>Key Learning Outcomes</h2>

<ol>
    <li>
        <span class="section-title">Data Cleaning with Python:</span>
        <ul>
            <li>Examine and apply data cleaning techniques using Python examples.</li>
            <li>Create and convert data files to CSV format.</li>
            <li>Scrape appropriate web pages for data collection.</li>
        </ul>
    </li>
    <li>
        <span class="section-title">Automation of Data Cleaning:</span>
        <ul>
            <li>Evaluate how Python scripts can automate the data cleaning process.</li>
            <li>Understand how automation integrates machine learning strategies.</li>
        </ul>
    </li>
    <li>
        <span class="section-title">Database Representation and Architecture:</span>
        <ul>
            <li>Understand the use of database representation and architecture.</li>
            <li>Explain data models as conceptual representations of data relationships.</li>
        </ul>
    </li>
</ol>


      <h2>Artifacts and Their Relation to Learning Outcomes</h2>

<h3>Lecturecast and Reading Materials:</h3>
<ul>
    <li>Provided insights into data cleaning techniques and the importance of automation.</li>
    <li>Emphasized critical outcomes for data design and the use of machine learning strategies.</li>
</ul>

<h3>Practical Exercises:</h3>
<ul>
    <li>Hands-on experience with data cleaning and web scraping using Python.</li>
    <li>Exercises on automating data collection and cleaning processes.</li>
</ul>

<h2>Reading and Study Hours</h2>

<h3>Primary Reading:</h3>
<ul>
    <li>
        <span class="book-title">IBM Intellas (2016) Qradar and KAIF Integration Report</span>
    </li>
    <li>
        <span class="book-title">Datanami (2019) Data Pipeline Automation: The Next Step Forward in DataOps</span>
        <ul>
            <li class="time">Time Spent: Approximately 4 hours for both articles.</li>
        </ul>
    </li>
</ul>


      <h2>Summary of Articles</h2>

<h3>Data Pipeline Automation: The Next Step Forward in DataOps</h3>
<p>From the Datanami article, I learned about the importance of automating data pipelines to streamline data engineering tasks. The article highlighted how tools like Kubeflow and Airflow help automate machine learning workflows, while new DataOps tools manage data pipelines. This automation increases agility, reduces errors, and enhances the productivity of data engineers, allowing them to handle large-scale data more effectively.</p>

<h3>IBM Intellas: Qradar and KAIF Integration Report</h3>
<p>The IBM Intellas report described the integration of KAIF and QRADAR for advanced data analysis and cyber security. The report detailed how KAIF's machine learning strategies enhance QRADAR's capabilities, providing real-time threat analysis and intelligence. The integration ensures a robust system for digital security and forensic analysis, essential for handling complex data sets and cyber threats.</p>





      
    
    
    </div>
    <div class="unit" id="unit6">
      




    





    
    <div class="unit" id="unit7">




      <h2>Tasks</h2>

      
<h2>SQL and Normalisation Activities</h2>
<p>During this week's seminar, we reviewed the SQL website and discussed SQL features to incorporate into our database builds. We also discussed the database design and normalization tasks completed this week.</p>
<p>For the SQL quiz, please refer to the following link:</p>

  <a href="assets/pdf/sql-quiz.pdf" target="_blank" class="btn btn-primary btn-custom">SQL QUIZ & Results</a>

    
          
    </div>

























    
    <div class="unit" id="unit8">
      <h1>Unit 8</h1>
      <h2> Compliance and Regulatory Framework for Managing Data</h2>

<p>In Unit 8, "Compliance and Regulatory Framework for Managing Data," we explored the compliance frameworks regarding data management, focusing on the rights of individuals and the obligations of organizations to stakeholders.</p>

<h2>Key Learning Outcomes</h2>

<ol>
    <li>
        <span class="section-title">Compliance Obligations:</span>
        <ul>
            <li>Analyse compliance obligations for data stakeholders.</li>
            <li>Understand and apply applicable standards.</li>
            <li>Examine existing regulatory frameworks and evaluate activities that require regulation.</li>
        </ul>
    </li>
    <li>
        <span class="section-title">Individual Rights:</span>
        <ul>
            <li>Examine rights accorded to individuals with respect to data usage.</li>
            <li>Exercise rights with respect to data held about individuals, organizations, and stakeholders.</li>
        </ul>
    </li>
    <li>
        <span class="section-title">Regulatory Requirements:</span>
        <ul>
            <li>Understand regulatory requirements and obligations that come with data storage.</li>
            <li>Introduce standards applicable to various organizations.</li>
            <li>Evaluate best practices related to managing a database system.</li>
        </ul>
    </li>
</ol>

<h2>Artifacts and Their Relation to Learning Outcomes</h2>

<h3>Lecturecast and Reading Materials:</h3>
<ul>
    <li>Provided insights into compliance obligations and standards ensuring compliance.</li>
    <li>Explained how businesses must understand the implications arising from the use and storage of big data and the requirement for compliance.</li>
</ul>

<h3>Practical Exercises:</h3>
<ul>
    <li>Hands-on activities to apply compliant frameworks in different scenarios and understand regulatory requirements.</li>
</ul>

<h2>Reading and Study Hours</h2>

<h3>Primary Reading:</h3>
<ul>
    <li>Williams, G. (2017) The Cybercitizen and Homeland Security.</li>
    <li>Wired. (2020) What is GDPR? The summary guide to GDPR compliance in the UK.
        <ul>
            <li class="time">Time Spent: Approximately 3 hours for both articles.</li>
        </ul>
    </li>
</ul>





      
          
        <h2>Collaborative Discussion 2 (WEEK 1)- Comparing Compliance Laws</h2>
      <h3>Discussion Topic</h3>
      <p>In this discussion, we compared the GDPR's rules, particularly regarding the securing of personal data, with similar compliance laws in other countries or with the ICO in the UK. Only two students joined this discussion. More detailed information is available in the following link:</p>
         
      <a href="assets/pdf/big-data-initial-post-2.pdf" target="_blank" class="btn btn-primary btn-custom">Initial Post</a>
      <a href="assets/pdf/big-data-peer-responses-2.pdf" target="_blank" class="btn btn-primary btn-custom">Peer Responses</a>


 <h2>DreamHome Property Management Case Study</h2>
      <p>The seminar covered the DreamHome Property Management case study to illustrate how to establish a database project. We explored fact-finding techniques and documentation produced in the early stages of database development, including database planning, system definition, and requirements collection and analysis.</p>

<h3>Key Points from the Seminar:</h3>
<ul>
    <li><strong>Business Information System Concepts:</strong> Importance of database design and how it can be decomposed into conceptual, logical, and physical phases.</li>
    <li><strong>Fact-Finding Techniques:</strong> Commonly used techniques, their advantages, and disadvantages.</li>
    <li><strong>DreamHome Database Example:</strong> Overview of the current system, planning phase contributions, system definition, and requirements capturing.</li>
</ul>

<p>More detailed information about the DreamHome case is provided in the following report:</p>


      <a href="assets/pdf/big-data-dreamHome.pdf" target="_blank" class="btn btn-primary btn-custom">DreamHome Case</a>

          
    </div>





















    
        
          
    <div class="unit" id="unit9">
      <h1>Unit 9</h1>
      <h2> Database Management Systems (DBMS) and Models</h2>

<p>In Unit 9, "Database Management Systems (DBMS) and Models," we explored the foundational concepts and theories underlying various DBMS types, including flat files, relational databases, non-relational databases, data warehouses, clouds, Hadoop, and data lakes.</p>

<h2>Key Learning Outcomes</h2>

<ol>
    <li>
        <span class="section-title">DBMS Concepts and Theories:</span>
        <ul>
            <li>Evaluate design concepts and theories underpinning databases.</li>
            <li>Understand the principles of database design and development.</li>
        </ul>
    </li>
    <li>
        <span class="section-title">DBMS Strengths, Limitations, and Applications:</span>
        <ul>
            <li>Analyze the strengths and limitations of different DBMS.</li>
            <li>Examine the relevance of database designs to various programming paradigms.</li>
        </ul>
    </li>
    <li>
        <span class="section-title">Practical Application:</span>
        <ul>
            <li>Develop the ability to implement a database management system.</li>
            <li>Apply these concepts in designing applications that require large datasets.</li>
            <li>Understand database security issues.</li>
        </ul>
    </li>
</ol>

<h2>Artifacts and Their Relation to Learning Outcomes</h2>

<h3>Lecturecast and Reading Materials:</h3>
<ul>
    <li>Provided insights into the concepts and principles of database design.</li>
    <li>Discussed the implementation of DBMS and the importance of database security.</li>
</ul>

<h3>Practical Exercises:</h3>
<ul>
    <li>Hands-on experience with storing data in a relational database using SQLite.</li>
    <li>Exercises on setting up a local database with Python and saving cleaned datasets into the SQLite database.</li>
</ul>

<h2>Reading and Study Hours</h2>

<h3>Primary Reading:</h3>
<ul>
    <li>Zoiner, T. (2024) Big Data Architectures.
        <ul>
            <li class="time">Time Spent: Approximately 1 hours.</li>
        </ul>
    </li>
</ul>



      
          
        <h3>Collaborative Discussion 2 (WEEK 2) - Comparing Compliance Laws</h3>
      <h4>Discussion Topic</h4>
      <p>Compare the rules of the GDPR - in particular, with relation to the securing of personal data rule, with either similar compliance laws within your country of residence, or with the ICO in the UK.

The ICO refers to this rule as 'Security' and you should discuss your findings in relation to the standards set out and the exemptions that exist:</p>
          <ul>
          <li>'The securing personal data principle of the GDPR: Personal data shall be processed in a manner that ensures appropriate security of the personal data...' (ICO.org.uk).</li>
          <ul>
      <h4>Learning Outcomes</h4>
      
        <ul>
        <li>Identify and manage challenges, security issues and risks, limitations, and opportunities in data wrangling.</li>
        <ul>
          
      </ul>

      <a href="assets/pdf/big-data-initial-post-2.pdf" target="_blank" class="btn btn-primary btn-custom">Initial Post</a>
      <a href="assets/pdf/big-data-peer-responses-2.pdf" target="_blank" class="btn btn-primary btn-custom">Peer Responses</a>


           <h3>Building a DBMS</h3>
     <p>The seminar focused on building a DBMS using Python and SQLite. Key activities included:</p>

<ul>
    <li><strong>Installing SQLite and Setting Up a Relational Database with Python:</strong> Following instructions from the Kazil textbook.</li>
    <li><strong>Saving the Cleaned UNICEF Dataset into the SQLite Database:</strong> Practical application of storing and managing data within SQLite.</li>
</ul>

<h3>Key Points from the Seminar:</h3>
<ul>
    <li><strong>Practical Application:</strong> Emphasis on using Python for data analysis and database management.</li>
    <li><strong>Team Project Discussion:</strong> Opportunity to discuss progress on the team project and integrate feedback.</li>
</ul>

<p>More detailed information and the SQL code are provided in the following link:</p>

      <a href="assets/code/dreamhome.sql" target="_blank" class="btn btn-primary btn-custom">DreamHome SQL</a>
    </div>






























          
    <div class="unit" id="unit10">
      <h1>Unit 10</h1>

      
    <h2> More on APIs (Application Programming Interfaces) for Data Parsing</h2>

<p>In Unit 10, "More on APIs (Application Programming Interfaces) for Data Parsing," we analyzed and evaluated APIs and their role in data parsing and inter-process communication. We also examined the security requirements for robust API functionality and discussed the challenges and issues associated with API implementation.</p>

<h2>Key Learning Outcomes</h2>

<ol>
    <li>
        <span class="section-title">Evaluation of APIs:</span>
        <ul>
            <li>Understand and evaluate APIs for data parsing and inter-process communication.</li>
            <li>Examine different API protocols, types, and formats.</li>
        </ul>
    </li>
    <li>
        <span class="section-title">Security Requirements:</span>
        <ul>
            <li>Specify security requirements for ensuring the resilience and robustness of APIs.</li>
            <li>Understand API authentication methods, including Basic vs. key and OAuth.</li>
        </ul>
    </li>
    <li>
        <span class="section-title">Challenges and Implementation:</span>
        <ul>
            <li>Examine the challenges and issues associated with API implementation.</li>
            <li>Configure APIs for various platforms requiring data parsing and connectivity.</li>
        </ul>
    </li>
</ol>

<h2>Artifacts and Their Relation to Learning Outcomes</h2>

<h3>Lecturecast and Reading Materials:</h3>
<ul>
    <li>Provided insights into the concepts and principles of API design and implementation.</li>
    <li>Discussed the importance of security requirements and how to ensure API robustness.</li>
</ul>

<h3>Practical Exercises:</h3>
<ul>
    <li>Hands-on experience with configuring APIs and understanding their security requirements.</li>
    <li>Exercises on implementing APIs for data sharing, scraping, and connectivity between Python code and different file formats/management systems (XML, JSON, SQL).</li>
</ul>

<h2>Reading and Study Hours</h2>

<h3>Primary Reading:</h3>
<ul>
    <li>Cooksey, B. (2014) Real-Time Communication - An Introduction to APIs.</li>
    <li>Connolly, T. M. & Begg, C. E. (2015) Database Systems: A Practical Approach to Design, Implementation and Management.
        <ul>
            <li class="time">Time Spent: Approximately 3 hours for both chapters.</li>
        </ul>
    </li>
</ul>



          
        <h2>Collaborative Discussion 2 (WEEK 3) - Comparing Compliance Laws</h2>
      <h4>Discussion Topic</h4>
      <p>In the final week of Collaborative Discussion 2, we provided a summary post based on initial posts, peer feedback, and content from Units 8, 9, and 10. The summary post is available in the following link:</p>
      <a href="assets/pdf/big-data-initial-post-2.pdf" target="_blank" class="btn btn-primary btn-custom">Initial Post</a>
      <a href="assets/pdf/big-data-peer-responses-2.pdf" target="_blank" class="btn btn-primary btn-custom">Peer Responses</a>
      <a href="assets/pdf/big-data-summary-post-2.pdf" target="_blank" class="btn btn-primary btn-custom">Summary Post</a>

           <h3>API Security Requirements</h3>
      <p>I evaluated the security requirements of an API of our choice and wrote a brief security requirements specification to mitigate risks associated with the API for enabling data sharing, scraping, and connectivity. The report detailing this task is available in the following link:</p>
          
  
      <a href="assets/pdf/big-data-api.pdf" target="_blank" class="btn btn-primary btn-custom">Report </a>
    
    
        
          </div>




































          
    <div class="unit" id="unit11">

      
      <h1>Unit 11</h1>

      
    <h2> DBMS Transaction and Recovery</h2>


<p>In Unit 11, "DBMS Transaction and Recovery," we explored how database systems handle failures and ensure data integrity through transaction processing. The unit focused on achieving a consistent database state, where transactions are fully committed or not committed at all, preventing errors within the database.</p>

<h2>Key Learning Outcomes</h2>

<ol>
    <li>
        <span class="section-title">Transaction Processing:</span>
        <ul>
            <li>Understand the importance of transaction consistency in database systems.</li>
            <li>Identify that data transactions are fast-moving and interleaved.</li>
        </ul>
    </li>
    <li>
        <span class="section-title">ACID Transaction State:</span>
        <ul>
            <li>Introduce and understand the ACID properties (Atomicity, Consistency, Isolation, Durability) and their effects on the transaction cycle.</li>
            <li>Examine scheduled transactions, system failures, and checkpoints.</li>
        </ul>
    </li>
    <li>
        <span class="section-title">Role of Transaction Manager:</span>
        <ul>
            <li>Understand the purpose and function of a transaction manager in maintaining data integrity and handling system failures.</li>
        </ul>
    </li>
</ol>

<h2>Artifacts and Their Relation to Learning Outcomes</h2>

<h3>Lecturecast and Reading Materials:</h3>
<ul>
    <li>Provided insights into the concepts and principles of transaction processing and the ACID properties.</li>
    <li>Discussed the importance of transaction consistency and the role of a transaction manager.</li>
</ul>

<h3>Practical Exercises:</h3>
<ul>
    <li>Hands-on experience with transaction processing and recovery techniques.</li>
    <li>Exercises on implementing transaction management and ensuring data integrity.</li>
</ul>

<h2>Reading and Study Hours</h2>

<h3>Primary Reading:</h3>
<ul>
    <li>Sarkar, T. & Roychowdhury, S. (2019) Data Wrangling with Python.</li>
    <li>Cove, V. (2020) Visualize COVID-19 Trends in ArcGIS Insights.
        <ul>
            <li class="time">Time Spent: Approximately 3 hours for both chapters.</li>
        </ul>
    </li>
</ul>

      
        

           <h3>Back Up Procedure</h3>
      <p>In this task, we critically evaluated the Grandfather-Father-Son (GFS) backup procedure, highlighting its effectiveness in making large database backups less resource-heavy compared to other methods. Detailed findings are available in the following link:</p>
          
  

      <a href="assets/pdf/big-data-backup.pdf" target="_blank" class="btn btn-primary btn-custom">Report </a>
    
    
<h2>Individual Project: Executive Summary</h2>

<p>The individual project involved writing an executive summary of the completed design and build of a logical database, based on the team project report from Unit 6. This summary included:</p>

<ul>
    <li><strong>Summary of Work:</strong> Presented in an easy-to-understand, non-technical manner, supported by graphics and charts.</li>
    <li><strong>Database Modelling Concepts:</strong> Review and critical evaluation of the strengths and weaknesses of the data models used.</li>
    <li><strong>DBMS Analysis:</strong> Outcomes of the analysis of the chosen DBMS, highlighting SQL and No-SQL options.</li>
    <li><strong>Compliance and Legal Requirements:</strong> Meeting current standards (including GDPR) and outlining legal and compliance requirements.</li>
    <li><strong>Conclusions and Recommendations:</strong> Organized in order of priority to the business requirements.</li>
</ul>

<p>The detailed report is available in the following link:</p>
 <a href="assets/pdf/big-data-executive-summary.pdf" target="_blank" class="btn btn-primary btn-custom">Executive Summary </a>

      
    
    </div>


















          
    <div class="unit" id="unit12">
      <h1>Unit 12</h1>

      
         
    <h2> Future of Big Data Analytics</h2>



      <p>In Unit 12, "Future of Big Data Analytics," we explored the role of machine learning in driving advances in big data analysis and examined compliance frameworks to ensure data privacy. We also reflected on the applicability of these emerging trends and technologies to organizational contexts.</p>

<h2>Key Learning Outcomes</h2>

<ol>
    <li>
        <span class="section-title">Future Trends in Big Data Analytics:</span>
        <ul>
            <li>Understand emerging and future trends in big data analytics.</li>
            <li>Explore how machine learning strategies are applied to model large and complex data.</li>
        </ul>
    </li>
    <li>
        <span class="section-title">Machine Learning Applications:</span>
        <ul>
            <li>Gain insights into machine learning applications and technologies in database development.</li>
            <li>Reflect on the applicability of these topics to an organization.</li>
        </ul>
    </li>
    <li>
        <span class="section-title">Compliance Frameworks:</span>
        <ul>
            <li>Examine compliance frameworks for ensuring data privacy.</li>
            <li>Understand existing regulations, laws, rules, and standards related to data management.</li>
        </ul>
    </li>
</ol>

<h2>Artifacts and Their Relation to Learning Outcomes</h2>

<h3>Lecturecast and Reading Materials:</h3>
<ul>
    <li>Provided insights into emerging trends and technologies in big data analytics and machine learning.</li>
    <li>Discussed compliance frameworks and the importance of data privacy regulations.</li>
</ul>

<h3>Practical Exercises:</h3>
<ul>
    <li>Hands-on experience with machine learning applications and data privacy compliance.</li>
    <li>Exercises on understanding and applying compliance frameworks to ensure data security.</li>
</ul>

<h2>Reading and Study Hours</h2>

<h3>Primary Reading:</h3>
<ul>
    <li>Ethem, A. (2010) Introduction to Machine Learning.</li>
    <li>Williams, G. (2017) The Cybercitizen and Homeland Security.</li>
    <li>Bhatnagar, A. & Gajjar, D. (2024) Policy Implications of Artificial Intelligence.
        <ul>
            <li class="time">Time Spent: Approximately 4 hours for all chapters and articles.</li>
        </ul>
    </li>
</ul>

        

           <h2>Seminar: Content Challenge</h2>

<p>The seminar provided a discursive overview of the module, helping with final reflection and e-portfolio development. Key discussion points included:</p>

<ul>
    <li><strong>Disadvantages of File-Based Systems:</strong> Addressed by the DBMS approach.</li>
    <li><strong>ACID Properties:</strong> Discussed consistency, reliability, and concurrency control in transactions.</li>
    <li><strong>Database User Privileges:</strong> Commonly granted privileges.</li>
    <li><strong>View Updatability:</strong> Necessary restrictions for maintaining updatable views.</li>
    <li><strong>Materialized Views:</strong> Maintaining views without accessing the underlying base table.</li>
</ul>

<p>The detailed report with questions and answers is available in the following link:</p>


      <a href="assets/pdf/big-data-challenge.pdf" target="_blank" class="btn btn-primary btn-custom">Questions & Answers </a>
    
    
    </div>
  </div>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0/dist/js/bootstrap.bundle.min.js"
    integrity="sha384-p34f1UUtsS3wqzfto5wAAmdvj+osOnFyQFpp4Ua3gs/ZVWx6oOypYoCJhGGScy+8"
    crossorigin="anonymous"></script>
  <script src="https://kit.fontawesome.com/4ad03fe072.js" crossorigin="anonymous"></script>
</body>

</html>
