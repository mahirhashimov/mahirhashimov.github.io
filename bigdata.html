<!doctype html>
<html lang="en">

<head>
  <!-- Required meta tags -->
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- Bootstrap CSS -->
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0/dist/css/bootstrap.min.css" rel="stylesheet"
    integrity="sha384-wEmeIV1mKuiNpC+IOBjI7aAzPcEZeedi5yW5f2yOq55WWLwNGmvvx4Um1vskeMj0" crossorigin="anonymous">
  <link rel="stylesheet" href="assets/css/style.css">
  <title>Deciphering Big Data - Individual e-Portfolio</title>
  <style>
    body {
      background-color: #000; /* Black background */
      color: #fff; /* White text */
    }

    .sidebar {
      height: 100%;
      width: 250px;
      position: fixed;
      top: 0;
      left: 0;
      background-color: #262626;
      padding-top: 20px;
    }

    .sidebar a {
      padding: 10px 15px;
      text-decoration: none;
      font-size: 18px;
      color: #fff;
      display: block;
    }

    .sidebar a:hover {
      background-color: #575757;
    }

    .content {
      margin-left: 270px;
      padding: 20px;
    }

    .unit {
      background-color: #fff; /* White background for units */
      color: #000; /* Black text for units */
      margin-bottom: 20px;
      padding: 20px;
      border-radius: 10px;
      box-shadow: 0 0 10px rgba(0, 0, 0, 0.5);
    }

    .btn-custom {
      margin-right: 10px;
    }
  </style>
</head>

<body>
  <div class="sidebar">
    <a href="#unit1"><h1>Collaborative Discussion Forum Summaries</a>
    <a href="#unit2">Outcomes from the Team Exercises</a>
    <a href="#unit3">Formative, Wiki, and e-Portfolio Activities</a>
    <a href="#unit4">Unit 4</a>
    <a href="#unit5">Unit 5</a>
    <a href="#unit6">Unit 6</a>
    <a href="#unit7">Unit 7</a>
    <a href="#unit8">Unit 8</a>
    <a href="#unit9">Unit 9</a>
    <a href="#unit10">Unit 10</a>
    <a href="#unit11">Unit 11</a>
    <a href="#unit12">Unit 12</a>
  </div>

  <div class="content">
    <h1 class="mt-5">Deciphering Big Data - Individual e-Portfolio</h1>
    <div class="unit" id="unit1">
 

      <h2>Collaborative Discussion Forum Summaries - Unit 1-3 & 8-10</h2>

<h3>Summary:</h3>
<p>We examined the revolutionary possibilities of the Internet of Things (IoT) and the strict GDPR and ICO data protection laws in our collaborative discussion sessions. These talks brought to light the ways that IoT is affecting different industries as well as how crucial data protection is.</p>

<p>By improving operations and decision-making through device connectivity and real-time data analysis, the Internet of Things (IoT) has the potential to completely transform industries including smart cities and healthcare (Sundmaeker et al., 2010). However, obstacles including unclear protocols, problems integrating data, and the requirement for sophisticated big data infrastructures impede the adoption of IoT (Weber, 2010). Due to serious security and privacy issues, strong measures like encryption and frequent software upgrades are required (Panagiotis Mourtzas, 2023). While the ICO concentrates on encryption and ongoing inspections to maintain data privacy, the GDPR mandates suitable protections based on threats to people' rights (Voigt & von dem Bussche, 2017). The flexibility of the ICO in complying with regulations and the UK's independent data agreements following Brexit were also deliberated (Bradford, 2020; Edwards, 2018).<p>

  
<h3>Table 1: Key Aspects of Discussions</h3>
<table border="1" cellpadding="10" cellspacing="0">
    <thead>
        <tr>
            <th>Aspect</th>
            <th>Details</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>Internet of Things (IoT)</td>
            <td>IoT improves operations and decision-making through device connectivity and real-time data analysis in sectors like smart cities and healthcare.</td>
        </tr>
        <tr>
            <td>Challenges in IoT</td>
            <td>Issues include lack of defined protocols, data integration problems, and need for advanced big data infrastructures. Security and privacy are major concerns.</td>
        </tr>
        <tr>
            <td>Security Measures</td>
            <td>Emphasized the importance of robust security measures, regular evaluations, and affordable cloud-based platforms to mitigate IoT risks.</td>
        </tr>
        <tr>
            <td>GDPR and ICO Regulations</td>
            <td>GDPR mandates safeguards based on threat levels, while ICO focuses on encryption and ongoing assessments, providing compliance flexibility post-Brexit.</td>
        </tr>
        <tr>
            <td>Impact of Brexit</td>
            <td>Examined UK's independent data agreements and ICO's ability to offer customized compliance guidance post-Brexit.</td>
        </tr>
    </tbody>
</table>


<p></p>
<p></p>
<p></p>
<p></p>

      
      <a href="assets/pdf/bigdata-initial-post-1.pdf" target="_blank" class="btn btn-primary btn-custom">Collaboration Discussion 1 - Initial Post</a>
      <a href="assets/pdf/bigdata-peer-responses-1.pdf" target="_blank" class="btn btn-primary">Collaboration Discussion 1 - Peer Responses</a>
      <a href="assets/pdf/bigdata-summary-post-1.pdf" target="_blank" class="btn btn-primary">Collaboration Discussion 1 - Summary Post</a>
      <a href="assets/pdf/big-data-initial-post-2.pdf" target="_blank" class="btn btn-primary btn-custom">Collaboration Discussion 2 - Initial Post</a>
      <a href="assets/pdf/big-data-peer-responses-2.pdf" target="_blank" class="btn btn-primary btn-custom">Collaboration Discussion 2 - Peer Responses</a>
      <a href="assets/pdf/big-data-summary-post-2.pdf" target="_blank" class="btn btn-primary btn-custom">Collaboration Discussion 2 - Summary Post</a>

    </div>








    
        <div class="unit" id="unit2">


<h2>Outcomes from Team Exercises</h2>

<p>Through exercises, our team developed solutions for processing datasets and solving complex problems in various environments, honing the skills necessary to be productive team members in a virtual professional environment. We critically analysed data wrangling problems and applied appropriate methodologies, tools, and techniques.</p>

<h2>Team Exercise 1: Database Design for ABC Electronics - Unit 6</h2>

<h3>Summary:</h3>
<p>For ABC Electronics, we created a logical database in the first exercise to improve data management and expedite procedures. The project required selecting an appropriate database management system (DBMS), putting in place a data management pipeline, and defining entities, characteristics, relationships, data types, and formats.</p>

<h2>Team Meeting Notes:</h2>
<ul>
    <li><strong>Initial Phase:</strong> Discussion about entity identification, database structure, and project scope. low level of early member involvement.</li>
    <li><strong>Mid-Phase:</strong> Assignment of duties, beginning data integration, and preliminary design work. stressing the need of continuing contact.</li>
    <li><strong>Final Phase:</strong> Members' someÂ contributions, particularly those from Robert and Funmilayo. completion of the report authoring, data cleansing, and database design.</li>
</ul>

          
<h3>Table 2: Key Aspects of Exercise 1</h3>
<table border="1" cellpadding="10" cellspacing="0">
    <thead>
        <tr>
            <th>Aspect</th>
            <th>Details</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>Logical Design</td>
            <td>Defined entities, attributes, and relationships in a relational database model.</td>
        </tr>
        <tr>
            <td>Database Management System</td>
            <td>Chose MySQL for its scalability, performance, security, and cost-effectiveness.  (Mirayala, 2024 and Ma 
& Wang, 2024)</td>
        </tr>
        <tr>
            <td>Data Management Pipeline</td>
            <td>Captured data from various sources, used ETL processes, and implemented data cleaning techniques.</td>
        </tr>
        <tr>
            <td>Data Cleaning Techniques</td>
            <td>Data validation, managing missing values, eliminating duplicates, standardization, and normalization.  (Cuzzocrea et al., 2011)</td>
        </tr>
        <tr>
            <td>Tools and Methodologies</td>
            <td>Utilized Python libraries (Pandas, NumPy), SQL queries, and ETL tools. (Panda & Patra, 2015)</td>
        </tr>
    </tbody>
</table>

<h2>Feedback from Peers and Tutors:</h2>

<h3>Table 3: Feedback Summary</h3>
<table border="1" cellpadding="10" cellspacing="0">
    <thead>
        <tr>
            <th>Feedback Source</th>
            <th>Positive Points</th>
            <th>Areas for Improvement</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>Peer Feedback</td>
            <td>
                <ul>
                    <li>Robert's insightful commentary on stock orders and financial transactions.</li>
                    <li>Funmilayo's attention to detail in organizing the introduction and critical assessment sections.</li>
                    <li>Improved overall performance in the final stages of the project.</li>
                </ul>
            </td>
            <td>
                <ul>
                    <li>Initial low engagement from some members.</li>
                    <li>Need for early and consistent engagement to avoid last-minute rushes.</li>
                    <li>Encourage proactive participation from all team members from the beginning.</li>
                </ul>
            </td>
        </tr>
        <tr>
            <td>Tutor Feedback</td>
            <td>
                <ul>
                    <li>Demonstrated very good knowledge and understanding of module topics.</li>
                    <li>Thoroughly described hybrid inventory system application.</li>
                    <li>Good content and structure, with sound English language.</li>
                    <li>Proper referencing in Harvard style.</li>
                </ul>
            </td>
            <td>
                <ul>
                    <li>Include figures and diagrams within the main body of the report for clarity.</li>
                    <li>Focus more on relating the data management pipeline to the application data.</li>
                    <li>Enhance critical discussions and evaluations by linking theory to practical implementation.</li>
                    <li>Increase the number of relevant references to support critical discussions.</li>
                    <li>Address critical evaluation section to connect theoretical concepts with practical application.</li>
                </ul>
            </td>
        </tr>
    </tbody>
</table>

          <p></p>
          <p></p>
                <a href="assets/pdf/team-project-report.pdf" target="_blank" class="btn btn-primary btn-custom">Project Report</a>
          <p></p>
          <p></p>

<h2>Team Exercise 2: DreamHome Property Management Case Study - Unit 8</h2>

<h3>Summary:</h3>
<p>The first stages of DreamHome Property Management's Database Development Lifecycle (DSDLC) were the subject of the second exercise. We used fact-finding methods, determined user requirements, and recorded the lifecycle phases to guarantee a methodical and thorough approach to creating a successful database system.</p>

<h3>Table 4: Key Aspects of Exercise 2</h3>
<table border="1" cellpadding="10" cellspacing="0">
    <thead>
        <tr>
            <th>Aspect</th>
            <th>Details</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>Database Development Lifecycle (DSDLC)</td>
            <td>Followed structured steps: planning, system definition, requirements collection, design, application design, DBMS selection, implementation, data conversion, testing, and maintenance. (Coronel and Morris, 2019),</td>
        </tr>
        <tr>
            <td>Fact-Finding Techniques</td>
            <td>Employed techniques like documentation analysis, interviews, observations, research, and questionnaires to gather precise and comprehensive informatio. (Hoffer, Ramesh, & Topi, 2016).</td>
        </tr>
        <tr>
            <td>Database Requirements</td>
            <td>Defined data and transaction requirements for Branch and Staff user views, ensuring support for DreamHome's operations.</td>
        </tr>
        <tr>
            <td>Documentation</td>
            <td>Produced mission statements, system definition documents, and requirements specifications to guide the development process and ensure user needs are met. (Rob & Coronel, 2017)</td>
        </tr>
    </tbody>
</table>

          <h2>Feedback from Peers:</h2>

<h3>Table 5: Feedback Summary</h3>
<table border="1" cellpadding="10" cellspacing="0">
    <thead>
        <tr>
            <th>Feedback Source</th>
            <th>Positive Points</th>
            <th>Areas for Improvement</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>Peer Feedback</td>
            <td>
                <ul>
                    <li>The initial plan and scope were well-defined.</li>
                    <li>Effective use of fact-finding techniques despite low engagement.</li>
                </ul>
            </td>
            <td>
                <ul>
                    <li>Team members were not active and did not contribute much.</li>
                    <li>Need for early and consistent engagement from all members.</li>
                    <li>Encourage more collaboration and communication throughout the project.</li>
                </ul>
            </td>
        </tr>
    </tbody>
</table>

<p></p>
<p></p>

                <a href="assets/pdf/big-data-dreamHome.pdf" target="_blank" class="btn btn-primary btn-custom">DreamHome Case</a>
<p></p>
<p></p>












          

<div class="unit" id="unit3">


  <h2>Formative, Wiki, and e-Portfolio Activities</h2>

<h3>Task 1: Web Scraping - Unit 3</h3>

<h4>Objective:</h4>
<p>Writing a Python web scraping script to collect Indeed.com job listings for the term "Data Scientist" and parse the resulting data into an XML file using the requests and beautifulsoup4 libraries was the task's goal.</p>


<h4>Table 6: Key Steps and Outcomes</h4>
<table border="1" cellpadding="10" cellspacing="0">
    <thead>
        <tr>
            <th>Step</th>
            <th>Description</th>
            <th>Outcome</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>Environment Setup</td>
            <td>Installed requests and beautifulsoup4 libraries</td>
            <td>Ready to perform web scraping</td>
        </tr>
        <tr>
            <td>Webpage Identification</td>
            <td>Identified target URL and data elements</td>
            <td>Clear understanding of data to be extracted</td>
        </tr>
        <tr>
            <td>Initial Coding</td>
            <td>Sent HTTP request, parsed HTML, located job elements</td>
            <td>Successfully retrieved HTML content and located required elements</td>
        </tr>
        <tr>
            <td>Error Handling</td>
            <td>Added headers to mimic browser request, resolved HTTP 403 error</td>
            <td>Able to access and scrape the webpage</td>
        </tr>
        <tr>
            <td>Data Parsing and Storing</td>
            <td>Parsed job data, stored in XML format</td>
            <td>Structured and saved job listings in XML</td>
        </tr>
    </tbody>
</table>

<h4>Conclusion:</h4>
<p>The project brought together HTML parsing, data organisation in XML, online scraping, and managing HTTP failures. Methodical troubleshooting helped overcome obstacles and produce a script that effectively retrieves and stores Indeed.com job listings.</p>


    <p></p>
    <p></p>

    <a href="assets/code/bigdata-webscraping-1.jpg" target="_blank" class="btn btn-primary btn-custom">Python code for Web Scraping </a>
      <a href="assets/pdf/web-scraping-report.pdf" target="_blank" class="btn btn-primary">Report: Web Scraping</a>

    <p></p>
    <p></p>



<h3>Task 2: Data Cleaning and Processing - Unit 4</h3>

<h4>Objective:</h4>
<p>Using the human-readable headers from the mn_headers.csv file, the task's objective was to clean and preprocess the raw data in the mn.csv file in order to create a clean dataset for further analysis. This included renaming columns, handling missing values, and removing duplicate rows.<p>

<h4>Table 7: Key Steps and Outcomes</h4>
<table border="1" cellpadding="10" cellspacing="0">
    <thead>
        <tr>
            <th>Step</th>
            <th>Description</th>
            <th>Outcome</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>Data Loading</td>
            <td>Loaded data from mn.csv and mn_headers.csv using pandas</td>
            <td>Data successfully loaded into DataFrames</td>
        </tr>
        <tr>
            <td>Headers Dictionary</td>
            <td>Created a mapping dictionary from mn_headers.csv</td>
            <td>Facilitated renaming columns</td>
        </tr>
        <tr>
            <td>Renaming Columns</td>
            <td>Renamed columns using the headers dictionary</td>
            <td>Improved readability and interpretability</td>
        </tr>
        <tr>
            <td>Removing Duplicates</td>
            <td>Used drop_duplicates method to remove duplicate rows</td>
            <td>Ensured data integrity by eliminating redundant information</td>
        </tr>
        <tr>
            <td>Checking for Missing Values</td>
            <td>Checked for missing values using isnull().sum() method</td>
            <td>Identified columns requiring further cleaning or imputation</td>
        </tr>
        <tr>
            <td>Saving Cleaned Data</td>
            <td>Saved cleaned data to a new CSV file</td>
            <td>Cleaned dataset ready for further analysis</td>
        </tr>
    </tbody>
</table>

<h4>Conclusion:</h4>
<p>After the preprocessing and data cleaning were finished successfully, the dataset was cleaned and prepared for additional analysis. Accurate processing was secured by the methodical approach, and problems were handled with suitable technological fixes. The significance of thorough data cleaning procedures in data analysis is highlighted by this activity, laying the groundwork for dependable and perceptive outcomes.<p>

  <p></p>
  <p></p>
       <a href="assets/code/big-data-data-cleaning-1.jpg" target="_blank" class="btn btn-primary btn-custom">Python code for Data Cleaning</a>
        <a href="assets/pdf/Report%20on%20Data%20Cleaning%20and%20Processing%20Task.pdf" target="_blank" class="btn btn-primary btn-custom">Report: Data Cleaning</a>

  <p></p>
  <p></p>

  <h3>Task 3: Writing Good Code and Documentation</h3>

<h4>Objective:</h4>
<p>This task's goal was to use the best practices for writing code to create a comprehensive script that included documentation for preprocessing and data cleaning on a dataset that was derived from UNICEF survey data.</p>

<table border="1" cellpadding="10" cellspacing="0">
  <h4>Table 7: Steps Taken for Data Cleaning and Documentation</h4>

    <thead>
        <tr>
            <th>Step</th>
            <th>Description</th>
            <th>Code Snippet / Outcome</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>Data Loading</td>
            <td>Loaded raw data from mn.csv and headers from mn_headers.csv into pandas DataFrames.</td>
            <td><code>data = pd.read_csv('mn.csv', low_memory=False)</code><br>
                <code>headers = pd.read_csv('mn_headers.csv')</code></td>
        </tr>
        <tr>
            <td>Headers Dictionary</td>
            <td>Created a dictionary mapping acronyms to their human-readable labels using pandas.</td>
            <td><code>headers_dict = pd.Series(headers.Name.values, index=headers.Acronym).to_dict()</code></td>
        </tr>
        <tr>
            <td>Renaming Columns</td>
            <td>Renamed columns using the headers dictionary to transform cryptic headers into meaningful names.</td>
            <td><code>data.rename(columns=headers_dict, inplace=True)</code></td>
        </tr>
        <tr>
            <td>Removing Duplicates</td>
            <td>Identified and removed duplicate rows using pandas' drop_duplicates method.</td>
            <td><code>data.drop_duplicates(inplace=True)</code></td>
        </tr>
        <tr>
            <td>Checking for Missing Values</td>
            <td>Checked for missing values using the isnull().sum() method, identifying columns requiring further cleaning or imputation.</td>
            <td><code>missing_values = data.isnull().sum()</code></td>
        </tr>
        <tr>
            <td>Saving Cleaned Data</td>
            <td>Saved the cleaned dataset to a SQLite database using the sqlite3 module, creating a suitable table structure and inserting the data.</td>
            <td><code>conn = sqlite3.connect('cleaned_data.db')</code><br>
                <code>data.to_sql('cleaned_data', conn, if_exists='replace', index=False)</code></td>
        </tr>
    </tbody>
</table>

<h4>Conclusion:</h4>
<p>The data cleaning and preprocessing task was successfully completed, resulting in a cleaned dataset ready for further analysis. The systematic approach ensured accurate processing, with challenges effectively managed through appropriate technical solutions. This task highlights the importance of meticulous data cleaning practices in data analysis, laying a strong foundation for reliable and insightful results.</p>


     <a href="assets/code/big-data-data-cleaning-2.jpg" target="_blank" class="btn btn-primary btn-custom">Python code for Task 3</a>
        <a href="assets/pdf/Report%20on%20Data%20Cleaning%20and%20Processing%20Task%20-%20Documenting%20the%20Example.pdf" target="_blank" class="btn btn-primary btn-custom">Report: Documenting the Example</a>

  

  











































        
    <div class="unit" id="unit4">
      <h1>Unit 4</h1>
      <h2> Data Cleaning and Transformation</h2>

     <p>In Unit 4, "Data Cleaning and Transformation," we explored various concepts, techniques, and methods essential for cleaning and transforming data. This unit emphasized the importance of understanding the data management pipeline, evaluating factors affecting data cleaning, and the requirements critical for data design and process automation.</p>

<h2>Key Learning Outcomes</h2>

<ol>
    <li>
        <span class="section-title">Data Cleaning and Transformation:</span>
        <ul>
            <li>Examine and apply data cleaning and transformation concepts.</li>
            <li>Understand and implement the data management pipeline.</li>
            <li>Evaluate factors affecting data cleaning.</li>
        </ul>
    </li>
    <li>
        <span class="section-title">Design and Process Automation:</span>
        <ul>
            <li>Understand the requirements for design automation.</li>
            <li>Compile and document Python scripts for data cleaning and transformation.</li>
        </ul>
    </li>
    <li>
        <span class="section-title">Practical Application:</span>
        <ul>
            <li>Compile data sets and convert data into different formats.</li>
            <li>Perform practical data cleaning tasks using Python.</li>
        </ul>
    </li>
</ol>

      <h2>Reading and Study Hours</h2>

<h3>Primary Reading:</h3>
<ul>
    <li>
        <span class="book-title">Sarkar, T. & Roychowdhury, S. (2019) Data Wrangling with Python. 1st ed. Packt.</span>
        <ul>
            <li>Chapters 6 and 7: Covered data wrangling secrets, handling outliers, missing data, and advanced web scraping.</li>
            <li class="time">Time Spent: Approximately 4 hours.</li>
        </ul>
    </li>
    <li>
        <span class="book-title">Kazil, J & Jarmul, K. (2016) Data Wrangling with Python. O'Reilly Media Inc.</span>
        <ul>
            <li>Chapters 6, 7, and 14: Discussed data acquisition, cleanup, and automation.</li>
            <li class="time">Time Spent: Approximately 5 hours.</li>
        </ul>
    </li>
    <li>
        <span class="book-title">Huxley et al. (2020) Data Cleaning. Sage Foundation.</span>
        <ul>
            <li>Provided insights into best practices for data cleaning.</li>
            <li class="time">Time Spent: Approximately 1 hours.</li>
        </ul>
    </li>
</ul>


      
          
        <h2>Data Management Pipeline Test.</h2>
      
<p>The Data Management Pipeline Test was completed on Sunday, 26 May 2024, in 28 minutes and 5 seconds, with a perfect score of 2.00 out of 2.00, resulting in a grade of 10.00 out of 10.00 (100%).</p>

<p>The test focused on matching Python concepts/libraries with their purposes and best practices for Python development. This exercise reinforced key concepts and best practices, emphasizing efficiency, clarity, and proper coding standards.</p>

<p>For detailed questions and answers, please refer to the following link:</p>



      <a href="assets/pdf/bigdata-pipeline-test-1.pdf" target="_blank" class="btn btn-primary btn-custom">Test</a>


      
        <h2>UNIT EXERCISES</h2>

        <h3>Exercise 1</h3>
        <p>Follow the instructions on page 150-151 of the Data Wrangling with Python textbook to manually produce data files mn.csv and mn_headers.csv. 
        Perhaps the simplest approach to saving the cleaned data is to export it to a simple file, and the listed code saved to a new CSV file.</p>
        <a href="assets/code/big-data-data-cleaning-1.jpg" target="_blank" class="btn btn-primary btn-custom">Python code for Ex. 1</a>
        <a href="assets/pdf/Report%20on%20Data%20Cleaning%20and%20Processing%20Task.pdf" target="_blank" class="btn btn-primary btn-custom">Report: Data Cleaning</a>
        
        <h3>Exercise 2</h3>
        <p>The textbook continues to apply the rules for writing good code and produces a complete script with documentation. 
          Study the complete process listed on pages 199-209 of the Data Wrangling with Python textbook.</p>
   

    </div>














    

    
    <div class="unit" id="unit5">
      <h1>Unit 5</h1>

       <h2> Data Cleaning and Automating Data Collections</h2>

<p>In Unit 5, "Data Cleaning and Automating Data Collections," we explored practical aspects of data cleaning using Python, focused on automating data collections, and discussed the considerations for cleaning data for organizational use.</p>

<h2>Key Learning Outcomes</h2>

<ol>
    <li>
        <span class="section-title">Data Cleaning with Python:</span>
        <ul>
            <li>Examine and apply data cleaning techniques using Python examples.</li>
            <li>Create and convert data files to CSV format.</li>
            <li>Scrape appropriate web pages for data collection.</li>
        </ul>
    </li>
    <li>
        <span class="section-title">Automation of Data Cleaning:</span>
        <ul>
            <li>Evaluate how Python scripts can automate the data cleaning process.</li>
            <li>Understand how automation integrates machine learning strategies.</li>
        </ul>
    </li>
    <li>
        <span class="section-title">Database Representation and Architecture:</span>
        <ul>
            <li>Understand the use of database representation and architecture.</li>
            <li>Explain data models as conceptual representations of data relationships.</li>
        </ul>
    </li>
</ol>


      <h2>Artifacts and Their Relation to Learning Outcomes</h2>

<h3>Lecturecast and Reading Materials:</h3>
<ul>
    <li>Provided insights into data cleaning techniques and the importance of automation.</li>
    <li>Emphasized critical outcomes for data design and the use of machine learning strategies.</li>
</ul>

<h3>Practical Exercises:</h3>
<ul>
    <li>Hands-on experience with data cleaning and web scraping using Python.</li>
    <li>Exercises on automating data collection and cleaning processes.</li>
</ul>

<h2>Reading and Study Hours</h2>

<h3>Primary Reading:</h3>
<ul>
    <li>
        <span class="book-title">IBM Intellas (2016) Qradar and KAIF Integration Report</span>
    </li>
    <li>
        <span class="book-title">Datanami (2019) Data Pipeline Automation: The Next Step Forward in DataOps</span>
        <ul>
            <li class="time">Time Spent: Approximately 4 hours for both articles.</li>
        </ul>
    </li>
</ul>


      <h2>Summary of Articles</h2>

<h3>Data Pipeline Automation: The Next Step Forward in DataOps</h3>
<p>From the Datanami article, I learned about the importance of automating data pipelines to streamline data engineering tasks. The article highlighted how tools like Kubeflow and Airflow help automate machine learning workflows, while new DataOps tools manage data pipelines. This automation increases agility, reduces errors, and enhances the productivity of data engineers, allowing them to handle large-scale data more effectively.</p>

<h3>IBM Intellas: Qradar and KAIF Integration Report</h3>
<p>The IBM Intellas report described the integration of KAIF and QRADAR for advanced data analysis and cyber security. The report detailed how KAIF's machine learning strategies enhance QRADAR's capabilities, providing real-time threat analysis and intelligence. The integration ensures a robust system for digital security and forensic analysis, essential for handling complex data sets and cyber threats.</p>





      
    
    
    </div>
    <div class="unit" id="unit6">
      <h1>Unit 6</h1>
       <h2> Database Design and Normalisation</h2>

<p>In Unit 6, "Database Design and Normalisation," we focused on the principles of database design and the importance of normalization in relational databases. This unit emphasized the creation and management of databases, ensuring data integrity and efficient storage.</p>

<h2>Key Learning Outcomes</h2>

<ol>
    <li>
        <span class="section-title">Database Construction and Terminology:</span>
        <ul>
            <li>Discuss the construction and terminology associated with creating a database.</li>
            <li>Identify technical terminology used in the construction of a relational database.</li>
        </ul>
    </li>
    <li>
        <span class="section-title">Logical and Physical Database Design:</span>
        <ul>
            <li>Be familiar with the process involved in creating a logical database design.</li>
            <li>Evaluate the requirements for creating a physical architecture database design.</li>
            <li>Analyze and evaluate the requirements and limitations for creating a physical design.</li>
        </ul>
    </li>
    <li>
        <span class="section-title">Normalization:</span>
        <ul>
            <li>Understand the fundamentals of normalization.</li>
            <li>Look at the reasoning behind the use of different normal forms.</li>
        </ul>
    </li>
</ol>

<h2>Artifacts and Their Relation to Learning Outcomes</h2>

<h3>Lecturecast and Reading Materials:</h3>
<ul>
    <li>Provided insights into how data cleaning methods help with the storage of usable datasets.</li>
    <li>Explained the creation of databases and the use of key fields to link data.</li>
    <li>Analyzed anomalies that affect database integrity and the importance of normalization.</li>
</ul>

<h3>Practical Exercises:</h3>
<ul>
    <li>Hands-on activities involving real-life data wrangling tasks, such as fixing UN data, cleaning GDP data, and merging datasets.</li>
    <li>Exercises on connecting cleaned data to a database and extending data wrangling techniques.</li>
</ul>

<h2>Reading and Study Hours</h2>

<h3>Primary Reading:</h3>
<ul>
    <li>
        <span class="book-title">Sarkar, T. & Roychowdhury, S. (2019) Data Wrangling with Python. 1st ed. Packt.</span>
        <ul>
            <li>Chapter 9: Covered real-life applications of data wrangling and database connections.</li>
            <li class="time">Time Spent: Approximately 3 hours.</li>
        </ul>
    </li>
</ul>


      
          
        <h2>Development Team Project: Project Report</h2>

<p>In Unit 6, our team undertook a project to design and build a logical database for a chosen application. We acted as software consultants and developers, focusing on the following tasks:</p>

<ul>
    <li><strong>Logical Design:</strong> Identified data items/entities, their attributes, relationships, and associations.</li>
    <li><strong>Database Model Proposal:</strong> Proposed a database management system considering the client's requirements for storage, user access, and data manipulation.</li>
    <li><strong>Data Management Pipeline Evaluation:</strong> Discussed the data capture process, applied data cleaning techniques, and documented the implementation stages.</li>
</ul>

    

<p>The detailed team report is available in the following link:</p>

      

      <a href="assets/pdf/team-project-report.pdf" target="_blank" class="btn btn-primary btn-custom">Project Report</a>
   


<h2>Tutor Feedback</h2>

<ul>
    <li><strong>Knowledge and Understanding:</strong> The report shows very good knowledge and understanding of module topics, incorporated within a practical application as required.</li>
    <li><strong>Application Description:</strong> The application is a hybrid inventory system, which is described thoroughly.</li>
    <li><strong>Content and Structure:</strong> The content and structure of the report are good, the word count is within limit, and the English language is sound.</li>
    <li><strong>References:</strong> There are seven references in total listed and cited in the Harvard style.</li>
    <li><strong>Figures and Diagrams:</strong> The figure in the appendix is core material and should have been included within the main body of the report. Including a diagram showing tables' relationships and associations would have given more clarity.</li>
    <li><strong>Critical Discussions:</strong> The critical discussions are limited and need to focus more on the application. Discussing the data management pipeline should better relate to the application data rather than merely stating theoretical stages and steps.</li>
    <li><strong>Critical Evaluation:</strong> The critical evaluation section is limited to data wrangling and lists theoretical concepts without relating them to the practical application.</li>
    <li><strong>References for Critical Discussions:</strong> Having more references relevant to these topics would have assisted with better critical discussions relating theory to practice.</li>
    <li><strong>Theory and Practical Implementation:</strong> Overall, the report is theory-focused and requires better attention to linking theory to practical implementation relevant to the case at hand.</li>
</ul>



<h2>Feedback on Team Dynamics</h2>

<p>The team performed well, especially in the final stages of the project. Initial engagement was low due to other obligations, but Robert and Funmilayo significantly contributed in the last week, improving overall performance.</p>

<h3>Valuable Behaviors:</h3>
<ul>
    <li>Robert's insightful commentary on stock orders, financial transactions, and firm processes made the report more polished and effective.</li>
    <li>Funmilayo's attention to detail in organizing the introduction and critical assessment section improved the report's clarity and thoroughness.</li>
</ul>

<h3>Detrimental Behaviors:</h3>
<ul>
    <li>I initially felt pressured to oversee the project because Robert and Funmilayo were delayed in their contributions. Their efforts in the last week helped alleviate this issue. Early and consistent engagement is crucial for team cohesiveness.</li>
</ul>

<p>From this project, I learned the importance of early and frequent communication to ensure agreement and fair assignment of responsibilities. Including diverse viewpoints improves the project's outcome. In future projects, I will encourage proactive participation from all team members from the beginning to avoid last-minute rushes and ensure equal effort.</p>




 </div>










    





    
    <div class="unit" id="unit7">
      <h1>Unit 7</h1>
 <h2> Constructing Normalised Tables and Database Build</h2>

<p>In Unit 7, "Constructing Normalised Tables and Database Build," we focused on breaking down an un-normalised table into 1NF, 2NF, and 3NF, and using this data to create a relational database. This unit emphasized the importance of normalization in database design and provided practical experience in constructing and testing a relational database.</p>

<h2>Key Learning Outcomes</h2>

<ol>
    <li>
        <span class="section-title">Understanding Normalisation:</span>
        <ul>
            <li>Transform a flat file database model into a normalized model.</li>
            <li>Understand and apply the concepts of 1NF, 2NF, and 3NF.</li>
        </ul>
    </li>
    <li>
        <span class="section-title">Building and Testing a Relational Database:</span>
        <ul>
            <li>Construct a relational database model based on normalized data.</li>
            <li>Test the database for errors or anomalies to ensure referential integrity.</li>
        </ul>
    </li>
    <li>
        <span class="section-title">Identifying Functional Dependencies:</span>
        <ul>
            <li>Identify functional dependencies of data items in the dataset.</li>
            <li>Evaluate constraints and limitations associated with the dataset.</li>
        </ul>
    </li>
    <li>
        <span class="section-title">Specifying Data Attributes:</span>
        <ul>
            <li>Specify data attributes relevant and critical to using a given dataset.</li>
        </ul>
    </li>
</ol>

<h2>Artifacts and Their Relation to Learning Outcomes</h2>

<h3>Lecturecast and Reading Materials:</h3>
<ul>
    <li>Provided insights into data attributes, associations, operations, and relationships.</li>
    <li>Explained the process of normalizing data and the reasoning behind using different normal forms.</li>
</ul>

<h3>Practical Exercises:</h3>
<ul>
    <li>Hands-on experience with normalizing a dataset to 3NF and constructing a relational database model.</li>
</ul>

      <h2>Reading and Study Hours</h2>

<h3>Primary Reading:</h3>
<ul>
    <li>Letkowski, J. (2015) Doing Database Design with MySQL.</li>
    <li>IBM Intellas. (2010) What is a Database Management System?</li>
    <li>Taipalus, T. (2024) Database management system performance comparisons: A systematic literature review.</li>
    <li>Chen, W. (2023) Database Design and Implementation.
        <ul>
            <li class="time">Time Spent: Approximately 4 hours for all articles.</li>
        </ul>
    </li>
</ul>



      <h2>Tasks</h2>
<h2>Normalisation Task</h2>
<p>For the normalization task, we took an un-normalised table and transformed it into 1NF, 2NF, and 3NF, demonstrating each step. The Python code and detailed report explaining the process are available in the following link:</p>


           <a href="assets/excel/DBD_PCOM7E_Table.xlsx" target="_blank" class="btn btn-primary btn-custom">Table</a>
           <a href="assets/code/Normalisation-task-python.jpg" target="_blank" class="btn btn-primary btn-custom">Python code</a>
           <a href="assets/pdf/data-normalization.pdf" target="_blank" class="btn btn-primary btn-custom">Report: Data Normalization</a>
      <p>     </p>
      
<h2>Data Build Task</h2>
<p>Following the normalization task, we built a relational database system with linked tables, demonstrating knowledge of primary and foreign keys. We tested the database to ensure referential integrity. The Python code and report explaining this task are available in the following link:</p>

  <a href="assets/code/data-build-task-python.jpg" target="_blank" class="btn btn-primary btn-custom">Python code</a>
           <a href="assets/pdf/Report-on Data-Build-Task.pdf" target="_blank" class="btn btn-primary btn-custom">Report: Data Build Task</a>

           <p>     </p>
      
<h2>SQL and Normalisation Activities</h2>
<p>During this week's seminar, we reviewed the SQL website and discussed SQL features to incorporate into our database builds. We also discussed the database design and normalization tasks completed this week.</p>
<p>For the SQL quiz, please refer to the following link:</p>

  <a href="assets/pdf/sql-quiz.pdf" target="_blank" class="btn btn-primary btn-custom">SQL QUIZ & Results</a>

    
          
    </div>

























    
    <div class="unit" id="unit8">
      <h1>Unit 8</h1>
      <h2> Compliance and Regulatory Framework for Managing Data</h2>

<p>In Unit 8, "Compliance and Regulatory Framework for Managing Data," we explored the compliance frameworks regarding data management, focusing on the rights of individuals and the obligations of organizations to stakeholders.</p>

<h2>Key Learning Outcomes</h2>

<ol>
    <li>
        <span class="section-title">Compliance Obligations:</span>
        <ul>
            <li>Analyse compliance obligations for data stakeholders.</li>
            <li>Understand and apply applicable standards.</li>
            <li>Examine existing regulatory frameworks and evaluate activities that require regulation.</li>
        </ul>
    </li>
    <li>
        <span class="section-title">Individual Rights:</span>
        <ul>
            <li>Examine rights accorded to individuals with respect to data usage.</li>
            <li>Exercise rights with respect to data held about individuals, organizations, and stakeholders.</li>
        </ul>
    </li>
    <li>
        <span class="section-title">Regulatory Requirements:</span>
        <ul>
            <li>Understand regulatory requirements and obligations that come with data storage.</li>
            <li>Introduce standards applicable to various organizations.</li>
            <li>Evaluate best practices related to managing a database system.</li>
        </ul>
    </li>
</ol>

<h2>Artifacts and Their Relation to Learning Outcomes</h2>

<h3>Lecturecast and Reading Materials:</h3>
<ul>
    <li>Provided insights into compliance obligations and standards ensuring compliance.</li>
    <li>Explained how businesses must understand the implications arising from the use and storage of big data and the requirement for compliance.</li>
</ul>

<h3>Practical Exercises:</h3>
<ul>
    <li>Hands-on activities to apply compliant frameworks in different scenarios and understand regulatory requirements.</li>
</ul>

<h2>Reading and Study Hours</h2>

<h3>Primary Reading:</h3>
<ul>
    <li>Williams, G. (2017) The Cybercitizen and Homeland Security.</li>
    <li>Wired. (2020) What is GDPR? The summary guide to GDPR compliance in the UK.
        <ul>
            <li class="time">Time Spent: Approximately 3 hours for both articles.</li>
        </ul>
    </li>
</ul>





      
          
        <h2>Collaborative Discussion 2 (WEEK 1)- Comparing Compliance Laws</h2>
      <h3>Discussion Topic</h3>
      <p>In this discussion, we compared the GDPR's rules, particularly regarding the securing of personal data, with similar compliance laws in other countries or with the ICO in the UK. Only two students joined this discussion. More detailed information is available in the following link:</p>
         
      <a href="assets/pdf/big-data-initial-post-2.pdf" target="_blank" class="btn btn-primary btn-custom">Initial Post</a>
      <a href="assets/pdf/big-data-peer-responses-2.pdf" target="_blank" class="btn btn-primary btn-custom">Peer Responses</a>


 <h2>DreamHome Property Management Case Study</h2>
      <p>The seminar covered the DreamHome Property Management case study to illustrate how to establish a database project. We explored fact-finding techniques and documentation produced in the early stages of database development, including database planning, system definition, and requirements collection and analysis.</p>

<h3>Key Points from the Seminar:</h3>
<ul>
    <li><strong>Business Information System Concepts:</strong> Importance of database design and how it can be decomposed into conceptual, logical, and physical phases.</li>
    <li><strong>Fact-Finding Techniques:</strong> Commonly used techniques, their advantages, and disadvantages.</li>
    <li><strong>DreamHome Database Example:</strong> Overview of the current system, planning phase contributions, system definition, and requirements capturing.</li>
</ul>

<p>More detailed information about the DreamHome case is provided in the following report:</p>


      <a href="assets/pdf/big-data-dreamHome.pdf" target="_blank" class="btn btn-primary btn-custom">DreamHome Case</a>

          
    </div>





















    
        
          
    <div class="unit" id="unit9">
      <h1>Unit 9</h1>
      <h2> Database Management Systems (DBMS) and Models</h2>

<p>In Unit 9, "Database Management Systems (DBMS) and Models," we explored the foundational concepts and theories underlying various DBMS types, including flat files, relational databases, non-relational databases, data warehouses, clouds, Hadoop, and data lakes.</p>

<h2>Key Learning Outcomes</h2>

<ol>
    <li>
        <span class="section-title">DBMS Concepts and Theories:</span>
        <ul>
            <li>Evaluate design concepts and theories underpinning databases.</li>
            <li>Understand the principles of database design and development.</li>
        </ul>
    </li>
    <li>
        <span class="section-title">DBMS Strengths, Limitations, and Applications:</span>
        <ul>
            <li>Analyze the strengths and limitations of different DBMS.</li>
            <li>Examine the relevance of database designs to various programming paradigms.</li>
        </ul>
    </li>
    <li>
        <span class="section-title">Practical Application:</span>
        <ul>
            <li>Develop the ability to implement a database management system.</li>
            <li>Apply these concepts in designing applications that require large datasets.</li>
            <li>Understand database security issues.</li>
        </ul>
    </li>
</ol>

<h2>Artifacts and Their Relation to Learning Outcomes</h2>

<h3>Lecturecast and Reading Materials:</h3>
<ul>
    <li>Provided insights into the concepts and principles of database design.</li>
    <li>Discussed the implementation of DBMS and the importance of database security.</li>
</ul>

<h3>Practical Exercises:</h3>
<ul>
    <li>Hands-on experience with storing data in a relational database using SQLite.</li>
    <li>Exercises on setting up a local database with Python and saving cleaned datasets into the SQLite database.</li>
</ul>

<h2>Reading and Study Hours</h2>

<h3>Primary Reading:</h3>
<ul>
    <li>Zoiner, T. (2024) Big Data Architectures.
        <ul>
            <li class="time">Time Spent: Approximately 1 hours.</li>
        </ul>
    </li>
</ul>



      
          
        <h3>Collaborative Discussion 2 (WEEK 2) - Comparing Compliance Laws</h3>
      <h4>Discussion Topic</h4>
      <p>Compare the rules of the GDPR - in particular, with relation to the securing of personal data rule, with either similar compliance laws within your country of residence, or with the ICO in the UK.

The ICO refers to this rule as 'Security' and you should discuss your findings in relation to the standards set out and the exemptions that exist:</p>
          <ul>
          <li>'The securing personal data principle of the GDPR: Personal data shall be processed in a manner that ensures appropriate security of the personal data...' (ICO.org.uk).</li>
          <ul>
      <h4>Learning Outcomes</h4>
      
        <ul>
        <li>Identify and manage challenges, security issues and risks, limitations, and opportunities in data wrangling.</li>
        <ul>
          
      </ul>

      <a href="assets/pdf/big-data-initial-post-2.pdf" target="_blank" class="btn btn-primary btn-custom">Initial Post</a>
      <a href="assets/pdf/big-data-peer-responses-2.pdf" target="_blank" class="btn btn-primary btn-custom">Peer Responses</a>


           <h3>Building a DBMS</h3>
     <p>The seminar focused on building a DBMS using Python and SQLite. Key activities included:</p>

<ul>
    <li><strong>Installing SQLite and Setting Up a Relational Database with Python:</strong> Following instructions from the Kazil textbook.</li>
    <li><strong>Saving the Cleaned UNICEF Dataset into the SQLite Database:</strong> Practical application of storing and managing data within SQLite.</li>
</ul>

<h3>Key Points from the Seminar:</h3>
<ul>
    <li><strong>Practical Application:</strong> Emphasis on using Python for data analysis and database management.</li>
    <li><strong>Team Project Discussion:</strong> Opportunity to discuss progress on the team project and integrate feedback.</li>
</ul>

<p>More detailed information and the SQL code are provided in the following link:</p>

      <a href="assets/code/dreamhome.sql" target="_blank" class="btn btn-primary btn-custom">DreamHome SQL</a>
    </div>






























          
    <div class="unit" id="unit10">
      <h1>Unit 10</h1>

      
    <h2> More on APIs (Application Programming Interfaces) for Data Parsing</h2>

<p>In Unit 10, "More on APIs (Application Programming Interfaces) for Data Parsing," we analyzed and evaluated APIs and their role in data parsing and inter-process communication. We also examined the security requirements for robust API functionality and discussed the challenges and issues associated with API implementation.</p>

<h2>Key Learning Outcomes</h2>

<ol>
    <li>
        <span class="section-title">Evaluation of APIs:</span>
        <ul>
            <li>Understand and evaluate APIs for data parsing and inter-process communication.</li>
            <li>Examine different API protocols, types, and formats.</li>
        </ul>
    </li>
    <li>
        <span class="section-title">Security Requirements:</span>
        <ul>
            <li>Specify security requirements for ensuring the resilience and robustness of APIs.</li>
            <li>Understand API authentication methods, including Basic vs. key and OAuth.</li>
        </ul>
    </li>
    <li>
        <span class="section-title">Challenges and Implementation:</span>
        <ul>
            <li>Examine the challenges and issues associated with API implementation.</li>
            <li>Configure APIs for various platforms requiring data parsing and connectivity.</li>
        </ul>
    </li>
</ol>

<h2>Artifacts and Their Relation to Learning Outcomes</h2>

<h3>Lecturecast and Reading Materials:</h3>
<ul>
    <li>Provided insights into the concepts and principles of API design and implementation.</li>
    <li>Discussed the importance of security requirements and how to ensure API robustness.</li>
</ul>

<h3>Practical Exercises:</h3>
<ul>
    <li>Hands-on experience with configuring APIs and understanding their security requirements.</li>
    <li>Exercises on implementing APIs for data sharing, scraping, and connectivity between Python code and different file formats/management systems (XML, JSON, SQL).</li>
</ul>

<h2>Reading and Study Hours</h2>

<h3>Primary Reading:</h3>
<ul>
    <li>Cooksey, B. (2014) Real-Time Communication - An Introduction to APIs.</li>
    <li>Connolly, T. M. & Begg, C. E. (2015) Database Systems: A Practical Approach to Design, Implementation and Management.
        <ul>
            <li class="time">Time Spent: Approximately 3 hours for both chapters.</li>
        </ul>
    </li>
</ul>



          
        <h2>Collaborative Discussion 2 (WEEK 3) - Comparing Compliance Laws</h2>
      <h4>Discussion Topic</h4>
      <p>In the final week of Collaborative Discussion 2, we provided a summary post based on initial posts, peer feedback, and content from Units 8, 9, and 10. The summary post is available in the following link:</p>
      <a href="assets/pdf/big-data-initial-post-2.pdf" target="_blank" class="btn btn-primary btn-custom">Initial Post</a>
      <a href="assets/pdf/big-data-peer-responses-2.pdf" target="_blank" class="btn btn-primary btn-custom">Peer Responses</a>
      <a href="assets/pdf/big-data-summary-post-2.pdf" target="_blank" class="btn btn-primary btn-custom">Summary Post</a>

           <h3>API Security Requirements</h3>
      <p>I evaluated the security requirements of an API of our choice and wrote a brief security requirements specification to mitigate risks associated with the API for enabling data sharing, scraping, and connectivity. The report detailing this task is available in the following link:</p>
          
  
      <a href="assets/pdf/big-data-api.pdf" target="_blank" class="btn btn-primary btn-custom">Report </a>
    
    
        
          </div>




































          
    <div class="unit" id="unit11">

      
      <h1>Unit 11</h1>

      
    <h2> DBMS Transaction and Recovery</h2>


<p>In Unit 11, "DBMS Transaction and Recovery," we explored how database systems handle failures and ensure data integrity through transaction processing. The unit focused on achieving a consistent database state, where transactions are fully committed or not committed at all, preventing errors within the database.</p>

<h2>Key Learning Outcomes</h2>

<ol>
    <li>
        <span class="section-title">Transaction Processing:</span>
        <ul>
            <li>Understand the importance of transaction consistency in database systems.</li>
            <li>Identify that data transactions are fast-moving and interleaved.</li>
        </ul>
    </li>
    <li>
        <span class="section-title">ACID Transaction State:</span>
        <ul>
            <li>Introduce and understand the ACID properties (Atomicity, Consistency, Isolation, Durability) and their effects on the transaction cycle.</li>
            <li>Examine scheduled transactions, system failures, and checkpoints.</li>
        </ul>
    </li>
    <li>
        <span class="section-title">Role of Transaction Manager:</span>
        <ul>
            <li>Understand the purpose and function of a transaction manager in maintaining data integrity and handling system failures.</li>
        </ul>
    </li>
</ol>

<h2>Artifacts and Their Relation to Learning Outcomes</h2>

<h3>Lecturecast and Reading Materials:</h3>
<ul>
    <li>Provided insights into the concepts and principles of transaction processing and the ACID properties.</li>
    <li>Discussed the importance of transaction consistency and the role of a transaction manager.</li>
</ul>

<h3>Practical Exercises:</h3>
<ul>
    <li>Hands-on experience with transaction processing and recovery techniques.</li>
    <li>Exercises on implementing transaction management and ensuring data integrity.</li>
</ul>

<h2>Reading and Study Hours</h2>

<h3>Primary Reading:</h3>
<ul>
    <li>Sarkar, T. & Roychowdhury, S. (2019) Data Wrangling with Python.</li>
    <li>Cove, V. (2020) Visualize COVID-19 Trends in ArcGIS Insights.
        <ul>
            <li class="time">Time Spent: Approximately 3 hours for both chapters.</li>
        </ul>
    </li>
</ul>

      
        

           <h3>Back Up Procedure</h3>
      <p>In this task, we critically evaluated the Grandfather-Father-Son (GFS) backup procedure, highlighting its effectiveness in making large database backups less resource-heavy compared to other methods. Detailed findings are available in the following link:</p>
          
  

      <a href="assets/pdf/big-data-backup.pdf" target="_blank" class="btn btn-primary btn-custom">Report </a>
    
    
<h2>Individual Project: Executive Summary</h2>

<p>The individual project involved writing an executive summary of the completed design and build of a logical database, based on the team project report from Unit 6. This summary included:</p>

<ul>
    <li><strong>Summary of Work:</strong> Presented in an easy-to-understand, non-technical manner, supported by graphics and charts.</li>
    <li><strong>Database Modelling Concepts:</strong> Review and critical evaluation of the strengths and weaknesses of the data models used.</li>
    <li><strong>DBMS Analysis:</strong> Outcomes of the analysis of the chosen DBMS, highlighting SQL and No-SQL options.</li>
    <li><strong>Compliance and Legal Requirements:</strong> Meeting current standards (including GDPR) and outlining legal and compliance requirements.</li>
    <li><strong>Conclusions and Recommendations:</strong> Organized in order of priority to the business requirements.</li>
</ul>

<p>The detailed report is available in the following link:</p>
 <a href="assets/pdf/big-data-executive-summary.pdf" target="_blank" class="btn btn-primary btn-custom">Executive Summary </a>

      
    
    </div>


















          
    <div class="unit" id="unit12">
      <h1>Unit 12</h1>

      
         
    <h2> Future of Big Data Analytics</h2>



      <p>In Unit 12, "Future of Big Data Analytics," we explored the role of machine learning in driving advances in big data analysis and examined compliance frameworks to ensure data privacy. We also reflected on the applicability of these emerging trends and technologies to organizational contexts.</p>

<h2>Key Learning Outcomes</h2>

<ol>
    <li>
        <span class="section-title">Future Trends in Big Data Analytics:</span>
        <ul>
            <li>Understand emerging and future trends in big data analytics.</li>
            <li>Explore how machine learning strategies are applied to model large and complex data.</li>
        </ul>
    </li>
    <li>
        <span class="section-title">Machine Learning Applications:</span>
        <ul>
            <li>Gain insights into machine learning applications and technologies in database development.</li>
            <li>Reflect on the applicability of these topics to an organization.</li>
        </ul>
    </li>
    <li>
        <span class="section-title">Compliance Frameworks:</span>
        <ul>
            <li>Examine compliance frameworks for ensuring data privacy.</li>
            <li>Understand existing regulations, laws, rules, and standards related to data management.</li>
        </ul>
    </li>
</ol>

<h2>Artifacts and Their Relation to Learning Outcomes</h2>

<h3>Lecturecast and Reading Materials:</h3>
<ul>
    <li>Provided insights into emerging trends and technologies in big data analytics and machine learning.</li>
    <li>Discussed compliance frameworks and the importance of data privacy regulations.</li>
</ul>

<h3>Practical Exercises:</h3>
<ul>
    <li>Hands-on experience with machine learning applications and data privacy compliance.</li>
    <li>Exercises on understanding and applying compliance frameworks to ensure data security.</li>
</ul>

<h2>Reading and Study Hours</h2>

<h3>Primary Reading:</h3>
<ul>
    <li>Ethem, A. (2010) Introduction to Machine Learning.</li>
    <li>Williams, G. (2017) The Cybercitizen and Homeland Security.</li>
    <li>Bhatnagar, A. & Gajjar, D. (2024) Policy Implications of Artificial Intelligence.
        <ul>
            <li class="time">Time Spent: Approximately 4 hours for all chapters and articles.</li>
        </ul>
    </li>
</ul>

        

           <h2>Seminar: Content Challenge</h2>

<p>The seminar provided a discursive overview of the module, helping with final reflection and e-portfolio development. Key discussion points included:</p>

<ul>
    <li><strong>Disadvantages of File-Based Systems:</strong> Addressed by the DBMS approach.</li>
    <li><strong>ACID Properties:</strong> Discussed consistency, reliability, and concurrency control in transactions.</li>
    <li><strong>Database User Privileges:</strong> Commonly granted privileges.</li>
    <li><strong>View Updatability:</strong> Necessary restrictions for maintaining updatable views.</li>
    <li><strong>Materialized Views:</strong> Maintaining views without accessing the underlying base table.</li>
</ul>

<p>The detailed report with questions and answers is available in the following link:</p>


      <a href="assets/pdf/big-data-challenge.pdf" target="_blank" class="btn btn-primary btn-custom">Questions & Answers </a>
    
    
    </div>
  </div>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0/dist/js/bootstrap.bundle.min.js"
    integrity="sha384-p34f1UUtsS3wqzfto5wAAmdvj+osOnFyQFpp4Ua3gs/ZVWx6oOypYoCJhGGScy+8"
    crossorigin="anonymous"></script>
  <script src="https://kit.fontawesome.com/4ad03fe072.js" crossorigin="anonymous"></script>
</body>

</html>
