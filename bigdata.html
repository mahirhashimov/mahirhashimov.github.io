<!doctype html>
<html lang="en">

<head>
  <!-- Required meta tags -->
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- Bootstrap CSS -->
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0/dist/css/bootstrap.min.css" rel="stylesheet"
    integrity="sha384-wEmeIV1mKuiNpC+IOBjI7aAzPcEZeedi5yW5f2yOq55WWLwNGmvvx4Um1vskeMj0" crossorigin="anonymous">
  <link rel="stylesheet" href="assets/css/style.css">
  <title>Deciphering Big Data - Individual e-Portfolio</title>
  <style>
    body {
      background-color: #000; /* Black background */
      color: #fff; /* White text */
    }

    .sidebar {
      height: 100%;
      width: 250px;
      position: fixed;
      top: 0;
      left: 0;
      background-color: #262626;
      padding-top: 20px;
    }

    .sidebar a {
      padding: 10px 15px;
      text-decoration: none;
      font-size: 18px;
      color: #fff;
      display: block;
    }

    .sidebar a:hover {
      background-color: #575757;
    }

    .content {
      margin-left: 270px;
      padding: 20px;
    }

    .unit {
      background-color: #fff; /* White background for units */
      color: #000; /* Black text for units */
      margin-bottom: 20px;
      padding: 20px;
      border-radius: 10px;
      box-shadow: 0 0 10px rgba(0, 0, 0, 0.5);
    }

    .btn-custom {
      margin-right: 10px;
    }
  </style>
</head>

<body>
  <div class="sidebar">
    <a href="#unit1"><h1>Collaborative Discussion Forum Summaries</a>
    <a href="#unit2">Outcomes from the Team Exercises</a>
    <a href="#unit3">Formative, Wiki, and e-Portfolio Activities</a>
    <a href="#unit4">Technical Activities</a>
    <a href="#unit5">Limitations and Opportunities in Data Wrangling</a>
    <a href="#unit6">Evaluation of the Final Project (Unit 11) vs. Initial Project Proposal (Unit 6)</a>
    <a href="#unit7">Professional Skills Matrix and Action Plan (PDP)</a>
    <a href="#unit8">SWOT Analysis</a>
    <a href="#unit9">Unit 9</a>
    <a href="#unit10">Unit 10</a>
    <a href="#unit11">Unit 11</a>
    <a href="#unit12">Unit 12</a>
  </div>

  <div class="content">
    <h1 class="mt-5">Deciphering Big Data - Individual e-Portfolio</h1>
    <div class="unit" id="unit1">
 

      <h2>Collaborative Discussion Forum Summaries - Unit 1-3 & 8-10</h2>

<h3>Summary:</h3>
<p>We examined the revolutionary possibilities of the Internet of Things (IoT) and the strict GDPR and ICO data protection laws in our collaborative discussion sessions. These talks brought to light the ways that IoT is affecting different industries as well as how crucial data protection is.</p>

<p>By improving operations and decision-making through device connectivity and real-time data analysis, the Internet of Things (IoT) has the potential to completely transform industries including smart cities and healthcare (Sundmaeker et al., 2010). However, obstacles including unclear protocols, problems integrating data, and the requirement for sophisticated big data infrastructures impede the adoption of IoT (Weber, 2010). Due to serious security and privacy issues, strong measures like encryption and frequent software upgrades are required (Panagiotis Mourtzas, 2023). While the ICO concentrates on encryption and ongoing inspections to maintain data privacy, the GDPR mandates suitable protections based on threats to people' rights (Voigt & von dem Bussche, 2017). The flexibility of the ICO in complying with regulations and the UK's independent data agreements following Brexit were also deliberated (Bradford, 2020; Edwards, 2018).<p>

  
<h3>Table 1: Key Aspects of Discussions</h3>
<table border="1" cellpadding="10" cellspacing="0">
    <thead>
        <tr>
            <th>Aspect</th>
            <th>Details</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>Internet of Things (IoT)</td>
            <td>IoT improves operations and decision-making through device connectivity and real-time data analysis in sectors like smart cities and healthcare.</td>
        </tr>
        <tr>
            <td>Challenges in IoT</td>
            <td>Issues include lack of defined protocols, data integration problems, and need for advanced big data infrastructures. Security and privacy are major concerns.</td>
        </tr>
        <tr>
            <td>Security Measures</td>
            <td>Emphasized the importance of robust security measures, regular evaluations, and affordable cloud-based platforms to mitigate IoT risks.</td>
        </tr>
        <tr>
            <td>GDPR and ICO Regulations</td>
            <td>GDPR mandates safeguards based on threat levels, while ICO focuses on encryption and ongoing assessments, providing compliance flexibility post-Brexit.</td>
        </tr>
        <tr>
            <td>Impact of Brexit</td>
            <td>Examined UK's independent data agreements and ICO's ability to offer customized compliance guidance post-Brexit.</td>
        </tr>
    </tbody>
</table>


<p></p>
<p></p>
<p></p>
<p></p>

      
      <a href="assets/pdf/bigdata-initial-post-1.pdf" target="_blank" class="btn btn-primary btn-custom">Collaboration Discussion 1 - Initial Post</a>
      <a href="assets/pdf/bigdata-peer-responses-1.pdf" target="_blank" class="btn btn-primary">Collaboration Discussion 1 - Peer Responses</a>
      <a href="assets/pdf/bigdata-summary-post-1.pdf" target="_blank" class="btn btn-primary">Collaboration Discussion 1 - Summary Post</a>
      <a href="assets/pdf/big-data-initial-post-2.pdf" target="_blank" class="btn btn-primary btn-custom">Collaboration Discussion 2 - Initial Post</a>
      <a href="assets/pdf/big-data-peer-responses-2.pdf" target="_blank" class="btn btn-primary btn-custom">Collaboration Discussion 2 - Peer Responses</a>
      <a href="assets/pdf/big-data-summary-post-2.pdf" target="_blank" class="btn btn-primary btn-custom">Collaboration Discussion 2 - Summary Post</a>

    </div>








    
        <div class="unit" id="unit2">


<h2>Outcomes from Team Exercises</h2>

<p>Through exercises, our team developed solutions for processing datasets and solving complex problems in various environments, honing the skills necessary to be productive team members in a virtual professional environment. We critically analysed data wrangling problems and applied appropriate methodologies, tools, and techniques.</p>

<h2>Team Exercise 1: Database Design for ABC Electronics - Unit 6</h2>

<h3>Summary:</h3>
<p>For ABC Electronics, we created a logical database in the first exercise to improve data management and expedite procedures. The project required selecting an appropriate database management system (DBMS), putting in place a data management pipeline, and defining entities, characteristics, relationships, data types, and formats.</p>

<h2>Team Meeting Notes:</h2>
<ul>
    <li><strong>Initial Phase:</strong> Discussion about entity identification, database structure, and project scope. low level of early member involvement.</li>
    <li><strong>Mid-Phase:</strong> Assignment of duties, beginning data integration, and preliminary design work. stressing the need of continuing contact.</li>
    <li><strong>Final Phase:</strong> Members' someÂ contributions, particularly those from Robert and Funmilayo. completion of the report authoring, data cleansing, and database design.</li>
</ul>

          
<h3>Table 2: Key Aspects of Exercise 1</h3>
<table border="1" cellpadding="10" cellspacing="0">
    <thead>
        <tr>
            <th>Aspect</th>
            <th>Details</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>Logical Design</td>
            <td>Defined entities, attributes, and relationships in a relational database model.</td>
        </tr>
        <tr>
            <td>Database Management System</td>
            <td>Chose MySQL for its scalability, performance, security, and cost-effectiveness.  (Mirayala, 2024 and Ma 
& Wang, 2024)</td>
        </tr>
        <tr>
            <td>Data Management Pipeline</td>
            <td>Captured data from various sources, used ETL processes, and implemented data cleaning techniques.</td>
        </tr>
        <tr>
            <td>Data Cleaning Techniques</td>
            <td>Data validation, managing missing values, eliminating duplicates, standardization, and normalization.  (Cuzzocrea et al., 2011)</td>
        </tr>
        <tr>
            <td>Tools and Methodologies</td>
            <td>Utilized Python libraries (Pandas, NumPy), SQL queries, and ETL tools. (Panda & Patra, 2015)</td>
        </tr>
    </tbody>
</table>

<h2>Feedback from Peers and Tutors:</h2>

<h3>Table 3: Feedback Summary</h3>
<table border="1" cellpadding="10" cellspacing="0">
    <thead>
        <tr>
            <th>Feedback Source</th>
            <th>Positive Points</th>
            <th>Areas for Improvement</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>Peer Feedback</td>
            <td>
                <ul>
                    <li>Robert's insightful commentary on stock orders and financial transactions.</li>
                    <li>Funmilayo's attention to detail in organizing the introduction and critical assessment sections.</li>
                    <li>Improved overall performance in the final stages of the project.</li>
                </ul>
            </td>
            <td>
                <ul>
                    <li>Initial low engagement from some members.</li>
                    <li>Need for early and consistent engagement to avoid last-minute rushes.</li>
                    <li>Encourage proactive participation from all team members from the beginning.</li>
                </ul>
            </td>
        </tr>
        <tr>
            <td>Tutor Feedback</td>
            <td>
                <ul>
                    <li>Demonstrated very good knowledge and understanding of module topics.</li>
                    <li>Thoroughly described hybrid inventory system application.</li>
                    <li>Good content and structure, with sound English language.</li>
                    <li>Proper referencing in Harvard style.</li>
                </ul>
            </td>
            <td>
                <ul>
                    <li>Include figures and diagrams within the main body of the report for clarity.</li>
                    <li>Focus more on relating the data management pipeline to the application data.</li>
                    <li>Enhance critical discussions and evaluations by linking theory to practical implementation.</li>
                    <li>Increase the number of relevant references to support critical discussions.</li>
                    <li>Address critical evaluation section to connect theoretical concepts with practical application.</li>
                </ul>
            </td>
        </tr>
    </tbody>
</table>

          <p></p>
          <p></p>
                <a href="assets/pdf/team-project-report.pdf" target="_blank" class="btn btn-primary btn-custom">Project Report</a>
          <p></p>
          <p></p>

<h2>Team Exercise 2: DreamHome Property Management Case Study - Unit 8 & Unit 9</h2>

<h3>Summary:</h3>
<p>The first stages of DreamHome Property Management's Database Development Lifecycle (DSDLC) were the subject of the second exercise. We used fact-finding methods, determined user requirements, and recorded the lifecycle phases to guarantee a methodical and thorough approach to creating a successful database system.</p>

<h3>Table 4: Key Aspects of Exercise 2</h3>
<table border="1" cellpadding="10" cellspacing="0">
    <thead>
        <tr>
            <th>Aspect</th>
            <th>Details</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>Database Development Lifecycle (DSDLC)</td>
            <td>Followed structured steps: planning, system definition, requirements collection, design, application design, DBMS selection, implementation, data conversion, testing, and maintenance. (Coronel and Morris, 2019),</td>
        </tr>
        <tr>
            <td>Fact-Finding Techniques</td>
            <td>Employed techniques like documentation analysis, interviews, observations, research, and questionnaires to gather precise and comprehensive informatio. (Hoffer, Ramesh, & Topi, 2016).</td>
        </tr>
        <tr>
            <td>Database Requirements</td>
            <td>Defined data and transaction requirements for Branch and Staff user views, ensuring support for DreamHome's operations.</td>
        </tr>
        <tr>
            <td>Documentation</td>
            <td>Produced mission statements, system definition documents, and requirements specifications to guide the development process and ensure user needs are met. (Rob & Coronel, 2017)</td>
        </tr>
    </tbody>
</table>

          <h2>Feedback from Peers:</h2>

<h3>Table 5: Feedback Summary</h3>
<table border="1" cellpadding="10" cellspacing="0">
    <thead>
        <tr>
            <th>Feedback Source</th>
            <th>Positive Points</th>
            <th>Areas for Improvement</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>Peer Feedback</td>
            <td>
                <ul>
                    <li>The initial plan and scope were well-defined.</li>
                    <li>Effective use of fact-finding techniques despite low engagement.</li>
                </ul>
            </td>
            <td>
                <ul>
                    <li>Team members were not active and did not contribute much.</li>
                    <li>Need for early and consistent engagement from all members.</li>
                    <li>Encourage more collaboration and communication throughout the project.</li>
                </ul>
            </td>
        </tr>
    </tbody>
</table>

<p></p>
<p></p>

                <a href="assets/pdf/big-data-dreamHome.pdf" target="_blank" class="btn btn-primary btn-custom">DreamHome Case</a>
           <a href="assets/code/dreamhome.sql" target="_blank" class="btn btn-primary btn-custom">DreamHome SQL</a>
<p></p>
<p></p>

    </div>











          

<div class="unit" id="unit3">


  <h2>Formative, Wiki, and e-Portfolio Activities</h2>

<h3>Task 1: Web Scraping - Unit 3</h3>

<h4>Objective:</h4>
<p>Writing a Python web scraping script to collect Indeed.com job listings for the term "Data Scientist" and parse the resulting data into an XML file using the requests and beautifulsoup4 libraries was the task's goal.</p>


<h4>Table 6: Key Steps and Outcomes</h4>
<table border="1" cellpadding="10" cellspacing="0">
    <thead>
        <tr>
            <th>Step</th>
            <th>Description</th>
            <th>Outcome</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>Environment Setup</td>
            <td>Installed requests and beautifulsoup4 libraries</td>
            <td>Ready to perform web scraping</td>
        </tr>
        <tr>
            <td>Webpage Identification</td>
            <td>Identified target URL and data elements</td>
            <td>Clear understanding of data to be extracted</td>
        </tr>
        <tr>
            <td>Initial Coding</td>
            <td>Sent HTTP request, parsed HTML, located job elements</td>
            <td>Successfully retrieved HTML content and located required elements</td>
        </tr>
        <tr>
            <td>Error Handling</td>
            <td>Added headers to mimic browser request, resolved HTTP 403 error</td>
            <td>Able to access and scrape the webpage</td>
        </tr>
        <tr>
            <td>Data Parsing and Storing</td>
            <td>Parsed job data, stored in XML format</td>
            <td>Structured and saved job listings in XML</td>
        </tr>
    </tbody>
</table>

<h4>Conclusion:</h4>
<p>The project brought together HTML parsing, data organisation in XML, online scraping, and managing HTTP failures. Methodical troubleshooting helped overcome obstacles and produce a script that effectively retrieves and stores Indeed.com job listings.</p>


    <p></p>
    <p></p>

    <a href="assets/code/bigdata-webscraping-1.jpg" target="_blank" class="btn btn-primary btn-custom">Python code for Web Scraping </a>
      <a href="assets/pdf/web-scraping-report.pdf" target="_blank" class="btn btn-primary">Report: Web Scraping</a>

    <p></p>
    <p></p>



<h3>Task 2: Data Cleaning and Processing - Unit 4</h3>

<h4>Objective:</h4>
<p>Using the human-readable headers from the mn_headers.csv file, the task's objective was to clean and preprocess the raw data in the mn.csv file in order to create a clean dataset for further analysis. This included renaming columns, handling missing values, and removing duplicate rows.<p>

<h4>Table 7: Key Steps and Outcomes</h4>
<table border="1" cellpadding="10" cellspacing="0">
    <thead>
        <tr>
            <th>Step</th>
            <th>Description</th>
            <th>Outcome</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>Data Loading</td>
            <td>Loaded data from mn.csv and mn_headers.csv using pandas</td>
            <td>Data successfully loaded into DataFrames</td>
        </tr>
        <tr>
            <td>Headers Dictionary</td>
            <td>Created a mapping dictionary from mn_headers.csv</td>
            <td>Facilitated renaming columns</td>
        </tr>
        <tr>
            <td>Renaming Columns</td>
            <td>Renamed columns using the headers dictionary</td>
            <td>Improved readability and interpretability</td>
        </tr>
        <tr>
            <td>Removing Duplicates</td>
            <td>Used drop_duplicates method to remove duplicate rows</td>
            <td>Ensured data integrity by eliminating redundant information</td>
        </tr>
        <tr>
            <td>Checking for Missing Values</td>
            <td>Checked for missing values using isnull().sum() method</td>
            <td>Identified columns requiring further cleaning or imputation</td>
        </tr>
        <tr>
            <td>Saving Cleaned Data</td>
            <td>Saved cleaned data to a new CSV file</td>
            <td>Cleaned dataset ready for further analysis</td>
        </tr>
    </tbody>
</table>

<h4>Conclusion:</h4>
<p>After the preprocessing and data cleaning were finished successfully, the dataset was cleaned and prepared for additional analysis. Accurate processing was secured by the methodical approach, and problems were handled with suitable technological fixes. The significance of thorough data cleaning procedures in data analysis is highlighted by this activity, laying the groundwork for dependable and perceptive outcomes.<p>

  <p></p>
  <p></p>
       <a href="assets/code/big-data-data-cleaning-1.jpg" target="_blank" class="btn btn-primary btn-custom">Python code for Data Cleaning</a>
        <a href="assets/pdf/Report%20on%20Data%20Cleaning%20and%20Processing%20Task.pdf" target="_blank" class="btn btn-primary btn-custom">Report: Data Cleaning</a>

  <p></p>
  <p></p>

  <h3>Task 3: Writing Good Code and Documentation - Unit 4</h3>

<h4>Objective:</h4>
<p>This task's goal was to use the best practices for writing code to create a comprehensive script that included documentation for preprocessing and data cleaning on a dataset that was derived from UNICEF survey data.</p>

<table border="1" cellpadding="10" cellspacing="0">
  <h4>Table 7: Steps Taken for Data Cleaning and Documentation</h4>

    <thead>
        <tr>
            <th>Step</th>
            <th>Description</th>
            <th>Code Snippet / Outcome</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>Data Loading</td>
            <td>Loaded raw data from mn.csv and headers from mn_headers.csv into pandas DataFrames.</td>
            <td><code>data = pd.read_csv('mn.csv', low_memory=False)</code><br>
                <code>headers = pd.read_csv('mn_headers.csv')</code></td>
        </tr>
        <tr>
            <td>Headers Dictionary</td>
            <td>Created a dictionary mapping acronyms to their human-readable labels using pandas.</td>
            <td><code>headers_dict = pd.Series(headers.Name.values, index=headers.Acronym).to_dict()</code></td>
        </tr>
        <tr>
            <td>Renaming Columns</td>
            <td>Renamed columns using the headers dictionary to transform cryptic headers into meaningful names.</td>
            <td><code>data.rename(columns=headers_dict, inplace=True)</code></td>
        </tr>
        <tr>
            <td>Removing Duplicates</td>
            <td>Identified and removed duplicate rows using pandas' drop_duplicates method.</td>
            <td><code>data.drop_duplicates(inplace=True)</code></td>
        </tr>
        <tr>
            <td>Checking for Missing Values</td>
            <td>Checked for missing values using the isnull().sum() method, identifying columns requiring further cleaning or imputation.</td>
            <td><code>missing_values = data.isnull().sum()</code></td>
        </tr>
        <tr>
            <td>Saving Cleaned Data</td>
            <td>Saved the cleaned dataset to a SQLite database using the sqlite3 module, creating a suitable table structure and inserting the data.</td>
            <td><code>conn = sqlite3.connect('cleaned_data.db')</code><br>
                <code>data.to_sql('cleaned_data', conn, if_exists='replace', index=False)</code></td>
        </tr>
    </tbody>
</table>

<h4>Conclusion:</h4>
<p>The data cleaning and preprocessing task was successfully completed, resulting in a cleaned dataset ready for further analysis. The systematic approach ensured accurate processing, with challenges effectively managed through appropriate technical solutions. This task highlights the importance of meticulous data cleaning practices in data analysis, laying a strong foundation for reliable and insightful results.</p>

<p></p>
<p></p>
     <a href="assets/code/big-data-data-cleaning-2.jpg" target="_blank" class="btn btn-primary btn-custom">Python code for Task 3</a>
        <a href="assets/pdf/Report%20on%20Data%20Cleaning%20and%20Processing%20Task%20-%20Documenting%20the%20Example.pdf" target="_blank" class="btn btn-primary btn-custom">Report: Documenting the Example</a>
<p></p>
<p></p>

  
<h3>Task 4: Data Normalization - UNIT 7</h3>

<h4>Objective:</h4>
<p>This work aimed to ensure data integrity and remove duplication by normalising  (DiÃ¨ne et al., 2020) a dataset from an unnormalized form to the Third Normal Form (3NF).</p>

<table border="1" cellpadding="10" cellspacing="0">
    <h4>Table 8: Steps and Outcomes of Data Normalization</h4>

    <thead>
        <tr>
            <th>Step</th>
            <th>Description</th>
            <th>Process</th>
            <th>Outcome</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>Unnormalized Data (UNF)</td>
            <td>Initial dataset with anomalies</td>
            <td>Data included student details, exam scores, course names, exam boards, and teacher names</td>
            <td>Identified missing values and duplicate rows</td>
        </tr>
        <tr>
            <td>First Normal Form (1NF)</td>
            <td>Ensure atomic values and unique records</td>
            <td>- Remove empty rows <br> - Forward-fill missing values</td>
            <td>Structured data with atomic values and unique records</td>
        </tr>
        <tr>
            <td>Second Normal Form (2NF)</td>
            <td>Remove partial dependencies</td>
            <td>- Identify primary entities: Students, Courses, Teachers <br> - Create separate tables for each entity <br> - Merge tables to form a composite table</td>
            <td>Removed partial dependencies, ensuring non-key attributes are fully dependent on the primary key</td>
        </tr>
        <tr>
            <td>Third Normal Form (3NF)</td>
            <td>Remove transitive dependencies</td>
            <td>- Identify transitive dependencies <br> - Create Exam Scores Table</td>
            <td>Eliminated transitive dependencies, resulting in well-structured tables with non-key attributes dependent only on the primary key</td>
        </tr>
    </tbody>
</table>

<h4>Critical Analysis:</h4>
<ol>
    <li><strong>Data Cleaning:</strong>
        <ul>
            <li>Essential to eliminate duplicated and incomplete records, forming the foundation for further normalization.</li>
        </ul>
    </li>
    <li><strong>Entity Identification:</strong>
        <ul>
            <li>Crucial for structuring the database around logical entities, a core principle of database normalization.</li>
        </ul>
    </li>
    <li><strong>Dependency Removal:</strong>
        <ul>
            <li>Vital for efficient querying and updates, reducing data anomalies and redundancy.</li>
        </ul>
    </li>
    <li><strong>Practical Considerations:</strong>
        <ul>
            <li>Balance between normalization and query performance to manage complexity in database queries due to joining multiple tables.</li>
        </ul>
    </li>
</ol>

<h4>Conclusion:</h4>
<p>The dataset was effectively normalised, removing duplication and guaranteeing data integrity, resulting in a collection of well-structured tables in 3NF. Notwithstanding difficulties, the methodical approach made clear how crucial it is to remove dependencies, identify entities, and clean up data. This procedure improves the accuracy and efficiency of data retrieval while also increasing the efficiency of data storage.</p>

  
<p></p>
<p></p>

           <a href="assets/excel/DBD_PCOM7E_Table.xlsx" target="_blank" class="btn btn-primary btn-custom">Table</a>
           <a href="assets/code/Normalisation-task-python.jpg" target="_blank" class="btn btn-primary btn-custom">Python code</a>
           <a href="assets/pdf/data-normalization.pdf" target="_blank" class="btn btn-primary btn-custom">Report: Data Normalization</a>


<p></p>
<p></p>


<h3>Task 5: Data Build Task</h3>

<h4>Objective:</h4>
<p>Building a relational database system with linked tables, proving that you understood primary and secondary keys, and guaranteeing referential integrity were the goals of this work.</p>


<table border="1" cellpadding="10" cellspacing="0">
    <h4>Table 9: Steps and Outcomes of Data Build Task</h4>
    <thead>
        <tr>
            <th>Step</th>
            <th>Description</th>
            <th>Process</th>
            <th>Outcome</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>Designing the Database Schema</td>
            <td>Analyzed the provided data to identify necessary tables and relationships.</td>
            <td>Identified primary tables (Students, Courses, ExamBoards, Teachers) and junction tables for many-to-many relationships.</td>
            <td>Created a detailed schema plan with primary and foreign keys for each table.</td>
        </tr>
        <tr>
            <td>Creating the Database</td>
            <td>Wrote a comprehensive SQL script to create the tables with appropriate primary and foreign keys.</td>
            <td>Included the creation of junction tables to manage many-to-many relationships and ensure referential integrity.</td>
            <td>Successfully created database schema with tables and relationships defined.</td>
        </tr>
        <tr>
            <td>Populating the Database</td>
            <td>Used SQL INSERT statements to populate the tables with the provided data.</td>
            <td>Added records for students, courses, exam boards, and teachers, establishing relationships in junction tables.</td>
            <td>Database populated with accurate data reflecting the original unnormalized dataset.</td>
        </tr>
        <tr>
            <td>Testing the Database</td>
            <td>Used SQL SELECT queries to verify relationships and data integrity.</td>
            <td>Joined tables to verify relationships between students, their courses, exam boards, and teachers.</td>
            <td>Ensured data was correctly inserted and referential integrity was maintained.</td>
        </tr>
    </tbody>
</table>


<h4>Critical Analysis:</h4>

<table border="1" cellpadding="10" cellspacing="0">
    <h4>Table 10: Critical Analysis of Data Build Task</h4>
    <thead>
        <tr>
            <th>Aspect</th>
            <th>Analysis</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>Methodology and Tools</td>
            <td>SQLite was suitable due to its simplicity and ease of use for small to medium-sized databases. Breaking down the task into schema design, data insertion, and testing proved effective. (Gaffney et al., 2022)</td>
        </tr>
        <tr>
            <td>Normalization and Integrity</td>
            <td>Fully normalized schema helped avoid data redundancy and ensured data integrity. Foreign key constraints reinforced the relationships between tables.  (Amato, xxxx)</td>
        </tr>
        <tr>
            <td>Efficiency and Error Handling</td>
            <td>The process was efficient, though initial environment issues highlighted the importance of correct tool selection. Adding scripts to drop existing tables before creation handled errors and ensured a clean slate.</td>
        </tr>
        <tr>
            <td>Scalability Considerations</td>
            <td>The current system is adequate for the provided data but might need migration to more robust systems like PostgreSQL or MySQL for larger datasets.</td>
        </tr>
    </tbody>
</table>

  <p></p>
  <p></p>

<a href="assets/code/data-build-task-python.jpg" target="_blank" class="btn btn-primary btn-custom">Python code</a>
           <a href="assets/pdf/Report-on Data-Build-Task.pdf" target="_blank" class="btn btn-primary btn-custom">Report: Data Build Task</a>

  <p></p>
  <p></p>


<h3>Task 6: Security Requirements Specification for API - UNIT 10</h3>

<h4>Objective:</h4>
<p>This task's goal was to assess an API's security needs in order to provide data sharing, scraping, and networking between a Python application and data management systems that support XML, JSON, and SQL. Reducing the likelihood of data breaches, illegal access, and problems with data integrity is the aim.<p>

<table border="1" cellpadding="10" cellspacing="0">
      <h4>Table 11: Security Requirements for API</h4>

    <thead>
        <tr>
            <th>Step</th>
            <th>Description</th>
            <th>Process</th>
            <th>Outcome</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>Authentication and Authorization</td>
            <td>Ensuring that only authorized users can access the API.</td>
            <td>Implemented secure authentication methods like JWT and OAuth 2.0. Used role-based access control (RBAC).</td>
            <td>API access restricted to authenticated and authorized users.</td>
        </tr>
        <tr>
            <td>Data Transmission Security</td>
            <td>Protecting data in transit between the client and server.</td>
            <td>Used Transport Layer Security (TLS) and HTTPS for all API endpoints. Implemented strong encryption standards like AES-256 and TLS 1.2 or higher.</td>
            <td>Ensured data confidentiality and security during transmission.</td>
        </tr>
        <tr>
            <td>Data Integrity</td>
            <td>Ensuring the validity and unaltered state of data transferred via the API.</td>
            <td>Implemented strict input validation to prevent injection attacks. Used checksums or hashes (e.g., SHA-256) for data verification.</td>
            <td>Maintained data integrity and protected against injection attacks.</td>
        </tr>
        <tr>
            <td>Data Storage Security</td>
            <td>Protecting sensitive data stored in databases.</td>
            <td>Implemented robust encryption techniques and database access controls. Applied the principle of least privilege for data access.</td>
            <td>Ensured data security and confidentiality during storage.</td>
        </tr>
        <tr>
            <td>Input and Output Handling</td>
            <td>Preventing security flaws related to data input and output.</td>
            <td>Sanitized all inputs to avoid injection attacks. Used output encoding to prevent cross-site scripting (XSS) attacks.</td>
            <td>Secured data handling and prevented malicious data processing.</td>
        </tr>
        <tr>
            <td>API Rate Limiting and Throttling</td>
            <td>Protecting the API from misuse and denial-of-service (DoS) attacks.</td>
            <td>Implemented rate limiting and throttling techniques based on usage patterns.</td>
            <td>Maintained API performance and availability under high load conditions.</td>
        </tr>
        <tr>
            <td>Logging and Monitoring</td>
            <td>Identifying and addressing security events.</td>
            <td>Implemented comprehensive logging and real-time monitoring with intrusion detection systems (IDS) and security information and event management (SIEM).</td>
            <td>Enabled effective monitoring and analysis of API activities.</td>
        </tr>
        <tr>
            <td>Vulnerability Management</td>
            <td>Managing and mitigating security vulnerabilities.</td>
            <td>Conducted regular security audits and vulnerability assessments. Ensured timely application of security updates and patches.</td>
            <td>Reduced the risk of exploitation by known vulnerabilities.</td>
        </tr>
        <tr>
            <td>Incident Response</td>
            <td>Handling security breaches efficiently.</td>
            <td>Developed an incident response plan detailing actions for containment, eradication, recovery, and communication.</td>
            <td>Ensured effective management of security incidents and quick resumption of normal operations.</td>
        </tr>
        <tr>
            <td>Compliance</td>
            <td>Adhering to relevant laws and standards.</td>
            <td>Ensured compliance with regulations like GDPR and PCI DSS. Implemented necessary measures to manage data in accordance with legal obligations.</td>
            <td>Protected the organization and its clients through regulatory compliance.</td>
        </tr>
    </tbody>
</table>


<p></p>
<p></p>

     <a href="assets/pdf/big-data-api.pdf" target="_blank" class="btn btn-primary btn-custom">Report </a>
    <p></p>
<p></p>



<h3>Task 7: Back Up Procedure - UNIT 11</h3>

<h4>Objective:</h4>
<p>To critically evaluate the "Grandfather-Father-Son" (GFS) backup procedure and compare its effectiveness against other data backup methods.</p>

<h4>Critical Evaluation of the GFS Backup Procedure:</h4>
<p>A hierarchical technique that strikes a compromise between resource efficiency and dependable data recovery is the Grandfather-Father-Son (GFS) backup plan. By grouping backups into cycles that are daily for the son, weekly for the father, and monthly for the grandpa, it lessens the requirement for ongoing complete backups while guaranteeing numerous recovery points. Large databases benefit from this technique since it keeps a reasonable amount of backups over time.<p>

<table border="1" cellpadding="10" cellspacing="0">
    <h4>Table 12: Key Points of GFS Backup Procedure</h4>
    <thead>
        <tr>
            <th>Aspect</th>
            <th>Description</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>Structure</td>
            <td>Daily (son), weekly (father), and monthly (grandfather) cycles</td>
        </tr>
        <tr>
            <td>Resource Efficiency</td>
            <td>Reduces continuous full backups, minimizes storage needs</td>
        </tr>
        <tr>
            <td>Recovery Points</td>
            <td>Multiple recovery points ensure data reliability</td>
        </tr>
        <tr>
            <td>Storage Management</td>
            <td>FIFO rotation scheme minimizes storage requirements</td>
        </tr>
        <tr>
            <td>Comparison to CDP</td>
            <td>Less resource-intensive than Continuous Data Protection (CDP), which logs every change but requires more resources</td>
        </tr>
        <tr>
            <td>Comparison to Cloud</td>
            <td>More cost-effective than cloud-based backups, which offer off-site storage but can be costly and have latency issues</td>
        </tr>
    </tbody>
</table>




<p></p>
<p></p>

      <a href="assets/pdf/big-data-backup.pdf" target="_blank" class="btn btn-primary btn-custom">Report </a>

<p></p>
<p></p>




    </div>


        
    <div class="unit" id="unit4">
      <h1>Technical Activities</h1>
      
          
        <h2>Data Management Pipeline Test - UNIT 4</h2>
      
<p>The Data Management Pipeline Test was completed on Sunday, 26 May 2024, in 28 minutes and 5 seconds, with a perfect score of 2.00 out of 2.00, resulting in a grade of 10.00 out of 10.00 (100%).</p>

<p>The test focused on matching Python concepts/libraries with their purposes and best practices for Python development. This exercise reinforced key concepts and best practices, emphasizing efficiency, clarity, and proper coding standards.</p>

<p>For detailed questions and answers, please refer to the following link:</p>

<p></p>
<p></p>
      <a href="assets/pdf/bigdata-pipeline-test-1.pdf" target="_blank" class="btn btn-primary btn-custom">Test</a>

<p></p>
<p></p>

<h2>SQL and Normalisation Activities - UNIT 7</h2>
<p>During this week's seminar, we reviewed the SQL website and discussed SQL features to incorporate into our database builds. We also discussed the database design and normalization tasks completed this week.</p>
<p>For the SQL quiz, please refer to the following link:</p>

        <p></p>
<p></p>

  <a href="assets/pdf/sql-quiz.pdf" target="_blank" class="btn btn-primary btn-custom">SQL QUIZ & Results</a>
<p></p>
<p></p>


    <h2> Content Challenge - UNIT 12</h2>
    

<p>The seminar provided a discursive overview of the module, helping with final reflection and e-portfolio development. Key discussion points included:</p>

<ul>
    <li><strong>Disadvantages of File-Based Systems:</strong> Addressed by the DBMS approach.</li>
    <li><strong>ACID Properties:</strong> Discussed consistency, reliability, and concurrency control in transactions.</li>
    <li><strong>Database User Privileges:</strong> Commonly granted privileges.</li>
    <li><strong>View Updatability:</strong> Necessary restrictions for maintaining updatable views.</li>
    <li><strong>Materialized Views:</strong> Maintaining views without accessing the underlying base table.</li>
</ul>

<p>The detailed report with questions and answers is available in the following link:</p>

      <p></p>
      <p></p>

      <a href="assets/pdf/big-data-challenge.pdf" target="_blank" class="btn btn-primary btn-custom">Questions & Answers </a>
        
      <p></p>
      <p></p>
    </div>





    
    <div class="unit" id="unit5">

    <h1>Limitations and Opportunities in Data Wrangling</h1>


<table border="1" cellpadding="10" cellspacing="0">
    <h2>Table 13: Comprehensive Analysis of Data Wrangling</h2>
    <thead>
        <tr>
            <th>Aspect</th>
            <th>Limitations</th>
            <th>Opportunities</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>Data Quality Issues</td>
            <td>
                - Raw data often contains inconsistencies, errors, and missing values, complicating the cleaning process (Gudivada et al., 2017).<br>
                - Mixed data types in columns can trigger warnings and complicate data loading.
            </td>
            <td>
                - Effective data wrangling ensures high data quality, leading to accurate and reliable analysis results (Fan et al., 2021).<br>
                - Normalization and strict input validation enhance data integrity, reducing the risk of errors during analysis.
            </td>
        </tr>
        <tr>
            <td>Resource Intensive</td>
            <td>
                - Data wrangling can be computationally intensive, requiring significant processing power, especially for large datasets.<br>
                - The cleaning and preprocessing stages can be time-consuming, particularly with extensive datasets and complex structures.
            </td>
            <td>
                - Automation through scripts enhances efficiency and repeatability, streamlining the data cleaning process (McKinney, 2022).<br>
                - Techniques such as the Grandfather-Father-Son (GFS) backup method optimize resource usage, making the process less resource-heavy (BackupAssist, 2024).
            </td>
        </tr>
        <tr>
            <td>Scalability Challenges</td>
            <td>
                - Managing and processing large datasets can pose scalability issues, requiring robust infrastructure and tools.
            </td>
            <td>
                - Scalable tools and cloud-based solutions can efficiently handle larger datasets, facilitating seamless data wrangling (Vitanium, 2019).
            </td>
        </tr>
        <tr>
            <td>Technical Complexity</td>
            <td>
                - Ensuring compatibility and proper functioning of various data wrangling tools and libraries can be complex (Hossenizadeh et al., 2021).
            </td>
            <td>
                - Data wrangling enables the integration of data from multiple sources, providing a comprehensive dataset for analysis (Cuzzocrea et al., 2011).
            </td>
        </tr>
        <tr>
            <td>Security Enhancements</td>
            <td>
                - Implementing secure APIs for data wrangling is critical to prevent data breaches and unauthorized access (Taipalus, 2024).
            </td>
            <td>
                - Secure APIs ensure safe data exchange, maintaining data integrity and confidentiality throughout the process.
            </td>
        </tr>
    </tbody>
</table>

<p>There are several obstacles involved with data wrangling, including managing problems with data quality, resource-intensive procedures, scalability constraints, and technological complexity. It does, however, also present significant chances to improve data security, efficiency, and quality. Organisations may overcome these constraints and optimise their data wrangling processes to promote accurate and efficient data analysis by utilising cutting-edge technologies, techniques, and security measures. Effective data management and analysis require striking a balance between fixing the constraints and seizing the possibilities.<p>



    
    </div>




    
    <div class="unit" id="unit6">
      <h1>Evaluation of the Final Project (Unit 11) vs. Initial Project Proposal (Unit 6)</h1>

<h3>Introduction:</h3>
<p>
ABC Electronics' first project proposal sought to improve decision-making, user access, and data management by designing and building a logical database. By creating a thorough and integrated database system that addressed both strategic goals and operational efficiency, the completed project brought this vision to life.

</p>

<table>
  <h3>Table 14: Comparison of Key Elements</h3>
  <tr>
    <th>Aspect</th>
    <th>Initial Project Proposal (Unit 6)</th>
    <th>Final Project (Unit 11)</th>
    <th>Evaluation</th>
  </tr>
  <tr>
    <td>Objective</td>
    <td>Develop a database system to improve data management and user access.</td>
    <td>Created a robust database system integrating real-time data synchronization, IoT sensors, and advanced analytical capabilities.</td>
    <td>The final project achieved and extended the initial objectives by incorporating innovative technologies (De et al., 2020; Bag et al., 2020).</td>
  </tr>
  <tr>
    <td>Database Model</td>
    <td>Proposed a relational database with defined entities and attributes.</td>
    <td>Implemented a well-structured relational database, adhering to normalization principles, and ensured efficient data storage and retrieval.</td>
    <td>The logical and physical database designs were meticulously implemented, ensuring data integrity and performance (Date, 2019; Rajendran & Priya, 2023).</td>
  </tr>
  <tr>
    <td>DBMS Choice</td>
    <td>Recommended MySQL for its scalability, performance, security, and cost-effectiveness.</td>
    <td>Used MySQL, highlighting its benefits and addressing its limitations in scalability and configuration complexity.</td>
    <td>The choice of MySQL was validated, though potential scalability issues were acknowledged and addressed with recommendations for future improvements (Sarstedt et al., 2020; Palanisamy & SuvinthaVani, 2020).</td>
  </tr>
  <tr>
    <td>Data Management Pipeline</td>
    <td>Suggested ETL procedures for data capture, cleaning, and integration.</td>
    <td>Detailed the use of ETL tools, data validation, and normalization techniques to ensure data quality and consistency.</td>
    <td>The final project effectively implemented the proposed methodologies, ensuring high data quality and reliability (Cuzzocrea et al., 2011; Panda & Patra, 2015).</td>
  </tr>
  <tr>
    <td>Compliance and Security</td>
    <td>Emphasized the need for data security and compliance with standards like GDPR.</td>
    <td>Integrated comprehensive compliance measures, including GDPR and PCI DSS, and established regular audits and security protocols.</td>
    <td>The final project exceeded initial compliance goals by implementing rigorous security measures and ongoing compliance strategies (Shastri, 2020; Zarsky, 2016).</td>
  </tr>
  <tr>
    <td>Challenges and Mitigation</td>
    <td>Identified potential risks like data migration issues and system outages.</td>
    <td>Proactively managed risks with staggered deployment, thorough testing, and real-time performance monitoring tools.</td>
    <td>The final project effectively mitigated identified risks, ensuring a smooth implementation and operational stability (Adkins et al., 2020; Lund et al., 2024).</td>
  </tr>
  <tr>
    <td>Advanced Features</td>
    <td>Focused on basic database functionalities and data management practices.</td>
    <td>Incorporated advanced features like IoT integration for automatic stock tracking and real-time e-commerce updates.</td>
    <td>The inclusion of advanced features significantly enhanced the system's capabilities and operational efficiencies (FernÃ¡ndez et al., 2021; Hannila et al., 2022).</td>
  </tr>
</table>

<p><strong>Conclusion:</strong></p>
<p>The goals of the initial project proposal were effectively met and exceeded by the final project. With the integration of cutting-edge technology and the resolution of scalability and compliance issues, ABC Electronics' final database system is poised for future expansion and innovation. The project demonstrated how crucial careful planning, stakeholder involvement, and ongoing development are to achieving a reliable and expandable data management system.<p>
<p><strong>Recommendations:</strong></p>
<ol>
    <li><strong>Scalability Enhancement:</strong> Implement partitioning and indexing; consider PostgreSQL or cloud-based solutions for future scalability (Pirozzi, 2018).</li>
    <li><strong>Enhanced Data Analytics:</strong> Integrate tools like Tableau or Power BI for detailed insights (Pala, 2017; GonÃ§alves et al., 2023).</li>
    <li><strong>Regular Compliance Audits:</strong> Schedule regular compliance audits for GDPR, PCI DSS adherence (Elluri et al., 2018).</li>
    <li><strong>Employee Training:</strong> Conduct training sessions on data entry, security, and reporting (Elnaga & Imran, 2013).</li>
    <li><strong>System Performance Monitoring:</strong> Use tools like Nagios or New Relic for real-time performance monitoring (Cardoso, 2018).</li>
    <li><strong>Enhanced User Interface (UI):</strong> Refine UI based on user feedback to improve usability.</li>
    <li><strong>Data Backup and Recovery Plan:</strong> Develop and test a comprehensive data backup and recovery plan (Cerullo & Cerullo, 2004).</li>
</ol>
<p>By prioritizing these recommendations, ABC Electronics can maintain and enhance its database system's effectiveness, scalability, and compliance, ensuring it meets current and future business needs.</p>

  <a href="assets/pdf/team-project-report.pdf" target="_blank" class="btn btn-primary btn-custom">Initial Project - Unit 6 </a>

 <a href="assets/pdf/big-data-executive-summary.pdf" target="_blank" class="btn btn-primary btn-custom">Final Project - Unit 11  </a>


    </div>


    
    
    <div class="unit" id="unit7">

<h1>Professional Skills Matrix and Action Plan (PDP)</h1>


<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Professional Skills Matrix and Action Plan (PDP)</title>
    <style>
        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: 20px;
        }
        th, td {
            border: 1px solid #000;
            padding: 8px;
            text-align: left;
        }
        th {
            background-color: #f2f2f2;
        }
    </style>
</head>
<body>

<p><strong>Table 15: Professional Skills Matrix and Action Plan</strong></p>
<table>
    <thead>
        <tr>
            <th>Skill Category</th>
            <th>Specific Skills</th>
            <th>Goal</th>
            <th>Action</th>
            <th>Timeline</th>
            <th>Resources</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td rowspan="3">Technical Skills</td>
            <td>Data Wrangling</td>
            <td>Master advanced data wrangling techniques and automation</td>
            <td>Enroll in advanced Python courses focusing on automation (e.g., Coursera, edX)</td>
            <td>6 months</td>
            <td>Online courses, industry workshops, Python documentation</td>
        </tr>
        <tr>
            <td>Database Design and Management</td>
            <td>Improve knowledge of NoSQL databases and scalability</td>
            <td>Study NoSQL databases, focusing on MongoDB, Cassandra, and application of relevant certifications</td>
            <td>9 months</td>
            <td>MongoDB University, online courses, technical books</td>
        </tr>
        <tr>
            <td>Data Analysis and Visualization</td>
            <td>Learn advanced visualization tools like Tableau and Power BI</td>
            <td>Take specialized courses in Tableau and Power BI</td>
            <td>4 months</td>
            <td>Udemy, LinkedIn Learning, Tableau and Power BI websites</td>
        </tr>
        <tr>
            <td rowspan="2">Project Management</td>
            <td>Risk Management</td>
            <td>Develop advanced risk mitigation strategies</td>
            <td>Attend project management workshops focusing on risk mitigation</td>
            <td>8 months</td>
            <td>PMI workshops, project management courses</td>
        </tr>
        <tr>
            <td>Compliance and Security</td>
            <td>Gain in-depth understanding of GDPR and PCI DSS</td>
            <td>Study compliance frameworks and obtain certifications (e.g., CIPP/E)</td>
            <td>6 months</td>
            <td>IAPP courses, GDPR and PCI DSS training programs</td>
        </tr>
        <tr>
            <td rowspan="2">Interpersonal Skills</td>
            <td>Team Collaboration and Leadership</td>
            <td>Enhance leadership skills and conflict resolution abilities</td>
            <td>Participate in leadership training programs and team-building workshops</td>
            <td>Ongoing</td>
            <td>Leadership seminars, team-building workshops</td>
        </tr>
        <tr>
            <td>Communication</td>
            <td>Improve public speaking and persuasive communication</td>
            <td>Join a public speaking club (e.g., Toastmasters) and communication workshops</td>
            <td>Ongoing</td>
            <td>Toastmasters, communication courses</td>
        </tr>
        <tr>
            <td>Analytical Skills</td>
            <td>Problem-Solving</td>
            <td>Develop innovative problem-solving techniques</td>
            <td>Enroll in problem-solving workshops and case study analysis</td>
            <td>6 months</td>
            <td>Problem-solving courses, industry case studies</td>
        </tr>
        <tr>
            <td rowspan="2">Personal Development</td>
            <td>Time Management</td>
            <td>Master advanced time management strategies</td>
            <td>Take time management courses and apply techniques in daily tasks</td>
            <td>3 months</td>
            <td>Time management books, online courses</td>
        </tr>
        <tr>
            <td>Continuous Learning</td>
            <td>Foster a habit of lifelong learning and professional development</td>
            <td>Regularly attend webinars, online certification programs</td>
            <td>Ongoing</td>
            <td>Industry events, online learning platforms, certification programs</td>
        </tr>
    </tbody>
</table>

</body>
</html>







 </div>








    
        <div class="unit" id="unit8">


<h1>SWOT Analysis</h1>

<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SWOT Analysis</title>
    <style>
        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: 20px;
        }
        th, td {
            border: 1px solid #000;
            padding: 8px;
            text-align: left;
        }
        th {
            background-color: #f2f2f2;
        }
    </style>
</head>
<body>

<p><strong>Table: SWOT Analysis</strong></p>
<table>
    <thead>
        <tr>
            <th>Category</th>
            <th>Strengths</th>
            <th>Weaknesses</th>
            <th>Opportunities</th>
            <th>Threats</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>Technical Skills</td>
            <td>Advanced proficiency in SQL and MySQL demonstrated through database design and implementation projects.</td>
            <td>Data wrangling and cleaning processes can be time-consuming and computationally intensive.</td>
            <td>Enhance skills in advanced data wrangling techniques, NoSQL databases, and data visualization tools.</td>
            <td>Rapid technological changes may render current skills and knowledge obsolete.</td>
        </tr>
        <tr>
            <td>Project Management</td>
            <td>Strong capabilities in risk management, compliance, and requirements analysis.</td>
            <td>Initial challenges in handling large datasets and ensuring efficient scalability.</td>
            <td>Access to online courses, industry workshops, and certification programs for continuous learning.</td>
            <td>Increasing threats of data breaches and cyber-attacks require robust security measures.</td>
        </tr>
        <tr>
            <td>Interpersonal Skills</td>
            <td>Effective team collaboration and leadership abilities.</td>
            <td>Intermediate understanding of data security and compliance frameworks; need for deeper knowledge.</td>
            <td>Opportunities to lead larger projects and mentor junior team members, fostering a collaborative work environment.</td>
            <td>Limited resources for advanced training, certifications, and implementing new technologies.</td>
        </tr>
        <tr>
            <td>Communication</td>
            <td>Excellent communication skills, both written and verbal, facilitating clear and persuasive presentations.</td>
            <td>Balancing time between current responsibilities and pursuing additional certifications and training.</td>
            <td>Participating in leadership training programs and team-building exercises to enhance managerial skills.</td>
            <td>Competitive job market requiring continuous skill enhancement to stay relevant.</td>
        </tr>
        <tr>
            <td>Analytical Skills</td>
            <td>High-level problem-solving and critical thinking skills applied in various tasks and projects.</td>
            <td>Potential difficulties in scaling database systems to accommodate growing data volumes and transaction loads.</td>
            <td>Exploring new tools and methodologies to improve data management and operational efficiency.</td>
            <td>Compliance with evolving data protection regulations can be challenging and resource-consuming.</td>
        </tr>
        <tr>
            <td>Personal Development</td>
            <td>Ability to analyze data and derive meaningful insights to inform decision-making.</td>
            <td>Current expertise primarily focused on relational databases; limited experience with NoSQL databases.</td>
            <td>Implementing advanced security measures to protect data and ensure regulatory adherence.</td>
            <td>Balancing project deadlines with the need for ongoing professional development.</td>
        </tr>
        <tr>
            <td>Overall</td>
            <td>Strong foundation in data management, project execution, and team collaboration.</td>
            <td>Initial setup and configuration of secure systems can be complex and resource-intensive.</td>
            <td>Gaining in-depth understanding of GDPR, PCI DSS, and other compliance frameworks.</td>
            <td>Pressure to deliver high-quality results within tight deadlines, impacting work-life balance.</td>
        </tr>
    </tbody>
</table>

</body>
</html>







  

        

     
    
    
    </div>
  </div>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0/dist/js/bootstrap.bundle.min.js"
    integrity="sha384-p34f1UUtsS3wqzfto5wAAmdvj+osOnFyQFpp4Ua3gs/ZVWx6oOypYoCJhGGScy+8"
    crossorigin="anonymous"></script>
  <script src="https://kit.fontawesome.com/4ad03fe072.js" crossorigin="anonymous"></script>
</body>

</html>
